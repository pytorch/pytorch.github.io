<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      The Path to Achieve Ultra-Low Inference Latency With LLaMA 65B on PyTorch/XLA | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="Background &amp; State of the Art

" />

  <meta property="og:image" content="https://pytorch.org/assets/images/social-share.jpg" />
  <meta name="twitter:image" content="https://pytorch.org/assets/images/social-share.jpg" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="The Path to Achieve Ultra-Low Inference Latency With LLaMA 65B on PyTorch/XLA" />
<meta property="og:description" content="Background &amp; State of the Art

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="The Path to Achieve Ultra-Low Inference Latency With LLaMA 65B on PyTorch/XLA" />
<meta name="twitter:description" content="Background &amp; State of the Art

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/ptc/2022">
            <span class="dropdown-title">PyTorch Conference - 2022</span>
            <p>See the posters presented at PyTorch Conference - 2022</p>
          </a>
          <a class="nav-dropdown-item" href="https://events.linuxfoundation.org/pytorch-conference/">
            <span class="dropdown-title">PyTorch Conference - 2023</span>
            <p>October 16-17 in San Francisco</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/contributor-awards-2023">
            <span class="dropdown-title">Contributor Awards - 2023</span>
            <p>Award winners announced at this year's PyTorch Conference</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item active">
      <a href="/blog">Blog</a>
    </li>

    <li class="main-menu-item">
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/docs">
            <span class="dropdown-title docs-title">PyTorch</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/audio">
            <span class="dropdown-title docs-title">torchaudio</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/text">
            <span class="dropdown-title docs-title">torchtext</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/vision">
            <span class="dropdown-title docs-title">torchvision</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/torcharrow">
            <span class="dropdown-title docs-title">torcharrow</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/data">
            <span class="dropdown-title docs-title">TorchData</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/torchrec">
            <span class="dropdown-title docs-title">TorchRec</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/serve">
            <span class="dropdown-title docs-title">TorchServe</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/xla/release/1.6/index.html">
            <span class="dropdown-title docs-title">PyTorch on XLA Devices</span>
            <p></p>
          </a>
        </div>
      </div>
    </li>

    

    <li class="main-menu-item ">

      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Resources
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/features">
            <span class=dropdown-title>About</span>
            <p>Learn about PyTorch’s features and capabilities</p>
          </a>
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class=dropdown-title>Community stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/hub">
            <span class=dropdown-title>Models (Beta)</span>
            <p>Discover, publish, and reuse pre-trained models</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">June 28, 2023</p>
            <h1>
                <a class="blog-title">The Path to Achieve Ultra-Low Inference Latency With LLaMA 65B on PyTorch/XLA</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Milad Mohammadi, Jiewen Tan, Liyang Lu, Siyuan Liu, Yeounoh Chung,  Wonjoo Lee, Manfei Bai, Steven Krawczyk, Shauheen Zahirazami, Alex Wertheim, Meghan Cowan, Jack Cao,  Joe Spisak
                      
                    </p>
                    <h2 id="background--state-of-the-art">Background &amp; State of the Art</h2>

<p>In the natural language processing (NLP) space, language models are designed to generate a token (e.g. word) using a sequence of past input tokens. Large Language Models (LLMs) are the latest deep learning innovation in this space built to generate text in a human-like fashion. These models generally use <a href="https://arxiv.org/pdf/1706.03762.pdf">transformers</a> to improve their attention over a large sequence of input tokens.</p>

<p><a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">LLaMA</a>, open sourced by <a href="https://ai.facebook.com/">Meta AI</a>, is a powerful foundation LLM trained on over 1T tokens. LLaMA is competitive with many best-in-class models such as <a href="https://openai.com/blog/gpt-3-apps">GPT-3</a>, <a href="https://arxiv.org/pdf/2203.15556.pdf">Chinchilla</a>, <a href="https://arxiv.org/pdf/2204.02311.pdf">PaLM</a>. <a href="https://arxiv.org/pdf/2302.13971.pdf">LLaMA (13B) outperforms GPT-3 (175B)</a> highlighting its ability to extract more compute from each model parameter.</p>

<p>In this blog post, we use LLaMA as an example model to demonstrate the capabilities of PyTorch/XLA for LLM inference. We discuss how the computation techniques and optimizations discussed here improve inference latency by 6.4x on 65B parameter LLaMA models powered by Google Cloud TPU v4 (v4-16).</p>

<h2 id="model-overview">Model Overview</h2>

<p>We demonstrate the performance capabilities of PyTorch/XLA on <a href="https://github.com/facebookresearch/llama">LLaMA</a>, the latest LLM from Meta. We showcase performance optimizations on a series of common LLaMA configurations. Notice the 175B parameter model configuration is absent in the public domain. For the 175B parameter model mentioned below, we apply <a href="https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/models/opt/modeling_opt.py#L804">OPT 175B model configuration</a> to the LLaMA code base. Unless stated otherwise, in all configurations, we use <code class="language-plaintext highlighter-rouge">max_seq_len=256</code> and <code class="language-plaintext highlighter-rouge">dtype=bfloat16</code> for weights and activations.</p>

<h4 id="table-1-model-configurations-explored-in-this-article">Table 1: Model Configurations Explored in this article</h4>

<table>
  <tr>
   <td><strong>LLaMA</strong>
   </td>
   <td colspan="4"><strong>Model Hyper Parameters</strong>
   </td>
  </tr>
  <tr>
   <td><strong># Parameters</strong>
   </td>
   <td><strong>Dimensions</strong>
   </td>
   <td><strong>N Heads</strong>
   </td>
   <td><strong>N Layers</strong>
   </td>
   <td><strong>Max Seq Len</strong>
   </td>
  </tr>
  <tr>
   <td><strong>7B</strong>
   </td>
   <td>4,096
   </td>
   <td>32
   </td>
   <td>32
   </td>
   <td>256
   </td>
  </tr>
  <tr>
   <td><strong>33B</strong>
   </td>
   <td>6,656
   </td>
   <td>52
   </td>
   <td>60
   </td>
   <td>256
   </td>
  </tr>
  <tr>
   <td><strong>65B</strong>
   </td>
   <td>8,192
   </td>
   <td>64
   </td>
   <td>80
   </td>
   <td>256
   </td>
  </tr>
  <tr>
   <td><strong>175B</strong>
   </td>
   <td>12,288
   </td>
   <td>96
   </td>
   <td>96
   </td>
   <td>256
   </td>
  </tr>
</table>

<h2 id="performance-challenges-of-llms">Performance Challenges of LLMs</h2>

<p>LLMs have a few properties that make them challenging for compiler optimizations. (a) LLMs use autoregressive decoding to generate the next token baked on the previous ones; this means prompt tensors and coaches have a dynamic shape. (b) LLMs must work with variable input prompt lengths without triggering recompilation due to input tensor shape changes; input tensors must be properly bucketized and padded to avoid recompilation. (c) LLMs often require more memory than a single TPU (or GPU) device can support. A model-sharding scheme is required to fit the model across a distributed compute architecture. For instance, a LLaMA model with 65B parameters can fit on a v4-16 Cloud TPU, which is comparable to 8 A100 GPUs. (d) running LLMs in production can be expensive; one way to improve performance per total cost of ownership (Perf/TCO) is via quantization; quantization can potentially reduce hardware requirements.</p>

<h2 id="inference-tech-stack-in-pytorchxla">Inference Tech Stack in PyTorch/XLA</h2>

<p>Our goal is to offer the AI community a high performance inference stack. PyTorch/XLA integrates with <a href="https://pytorch.org/docs/stable/torch.compiler">TorchDynamo</a>, <a href="https://pytorch.org/blog/pytorch-2.0-xla/#pjrt-runtime-beta">PjRt</a>, <a href="https://pytorch.org/blog/pytorch-2.0-xla-path-forward/">OpenXLA</a>, and various model parallelism schemes. TorchDynamo eliminates tracing overhead at runtime, PjRt enables efficient host-device communication; PyTorch/XLA traceable collectives enable model and data parallelism on LLaMA via <a href="https://pytorch.org/docs/stable/torch.compiler">TorchDynamo</a>. To try our results, please use our custom <a href="https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch-nightly+20230422-cp38-cp38-linux_x86_64.whl">torch</a>, <a href="https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch_xla-nightly+20230422-cp38-cp38-linux_x86_64.whl">torch-xla</a> wheels to reproduce our <a href="https://github.com/pytorch-tpu/llama/tree/blog">LLaMA inference solution</a>. PyTorch/XLA 2.1 will support the features discussed in this post by default.</p>

<h2 id="parallel-computing">Parallel Computing</h2>

<h3 id="fairscale-sharding"><a href="https://github.com/facebookresearch/fairscale">FairScale</a> Sharding</h3>

<p>LLaMA uses FairScale model sharding API (<a href="https://github.com/facebookresearch/llama/blob/main/llama/model.py#L13-L17">fairscale.nn.model_parallel.layers</a>). We built an equivalent representation of this API using PyTorch/XLA communication collective (CC) ops such as <code class="language-plaintext highlighter-rouge">all-reduce</code> to communicate program state (e.g. activations) between accelerators. TorchDynamo does not fully support capturing CC ops currently (a.k.a. <a href="https://github.com/pytorch/pytorch/issues/93173">traceable collectives</a>). Without this support, a TorchDynamo FX graph would be cut at every device communication, meaning at every model layer. Graph cuts lead to performance loss as the underlying XLA compiler loses full graph optimization opportunities. To resolve this, we offer PyTorch/XLA traceable collectives by integrating the dispatcher collectives into our existing CC APIs. The difference is we don’t need to insert <code class="language-plaintext highlighter-rouge">c10d.wait()</code> ops after collectives, given the lazy execution nature of PyTorch/XLA. With support for traceable collectives, PyTorch/XLA allows singular FX graph generation in TorchDynamo.</p>

<h2 id="autoregressive-decoding-on-pytorchxla">Autoregressive Decoding on PyTorch/XLA</h2>

<p>LLMs need autoregressive decoding to feed the previous word as a prompt to predict the next token. Autoregressive decoding leads to unbounded dynamic shape problems, which in turn causes recompilation of every prompt. We optimized the LLaMA autoregressive decoder to operate with fixed shapes that in-place updates the KV-cache, output sequences, and attention masks during every token generation. With a combination of padding, masking, and index ops, we avoided excessive graph recompilation, thereby achieving efficient autoregressive decoding.</p>

<h3 id="kv-cache-optimization">KV-Cache Optimization</h3>

<p>LLaMA implements autoregressive decoding with KV-cache. For every generated token, the KV-cache stores the attention key/value activations of each Transformer layer. Thus, upon decoding a new token, the key/values of prior tokens no longer need recomputation.</p>

<p>In LLaMA, the KV-cache tensor slices are updated in-place; this leads to recompilation events every time a token is generated. To address this issue, we use index tensors and <code class="language-plaintext highlighter-rouge">tensor.index_copy()</code> ops to replace the in-place slice updates. Attention masks and output sequences also benefit from the same optimization.</p>

<h2 id="input-prompt-optimization">Input Prompt Optimization</h2>

<p>Variable length input prompts are common in LLM applications. This property causes input tensor shape dynamism and in turn recompilation events. When processing a prompt to fill the KV-cache, we either (a) process the input prompt token-by-token, or (b) process the whole prompt in one iteration. The pros and cons of each method are:</p>

<ol>
  <li>Pre-compile 1 graph and process a prompt token-by-token
    <ul>
      <li>Practical: 1 graph is compiled during warm-up</li>
      <li>Slow: <em>O(L)</em> to process an input prompt length <em>L</em> - a disadvantage for long prompts</li>
    </ul>
  </li>
  <li>Pre-compile all graphs with input lengths ranging from 1 to max_seq_len (e.g. 2,048)
    <ul>
      <li>Impractical: pre-compile and cache <em>max_seq_len</em> graphs during warm-up time</li>
      <li>Fast: 1 graph execution to process the full prompt</li>
    </ul>
  </li>
</ol>

<p>We introduce prompt length bucketization, an optimization to strike a balance between the two alternatives. We define a set of ascending bucket sizes, <em>(b<sub>0</sub>,b<sub>1</sub>,b<sub>2</sub>,…,b<sub>B-1</sub>)</em>, and then pre-compile program graphs with input sizes according to these bucket values, <em>(G<sub>0</sub>,G<sub>1</sub>,G<sub>2</sub>,…,G<sub>B-1</sub>)</em>; <em>B</em> is the number of buckets. For a given input prompt, we round up the prompt length to the closest bucket value <em>b<sub>n</sub></em>, pad the sequence, and use <em>G<sub>n</sub></em> to process the prompt in one iteration. The computation on the padding tokens is discarded. For prompts larger than the largest bucket size, we process them section-by-section.</p>

<p>The optimal bucket sizes should be determined by prompt length distribution in a target application. Here, we adopt bucket lengths: 128, 256, 384, 512. Any input prompt with up to 2,047 tokens requires up to 4 graph executions. For example, a 1,500 input prompt with generation length of 256 requires 260 graph executions - 4 to process the input, and 256 to generate the output.</p>

<h2 id="quantization">Quantization</h2>

<p>Quantization reduces the number of bits necessary to represent a value; it reduces the bandwidth to communicate data across multiple accelerator nodes (via collectives) and lowers the hardware requirements to serve a specific model size.</p>

<p>Normally, with <code class="language-plaintext highlighter-rouge">BF16</code> weights, a 175B parameter model would consume about 351GB of memory, and therefore require a v4-32 instance to accommodate the model. By quantizing the weights to <code class="language-plaintext highlighter-rouge">INT8</code>, we reduced the model size by roughly 50%, allowing it to run on a smaller v4-16 instance. Because LLaMA shards model activations, quantization offers negligible communication gain.</p>

<p>In our experiments, we quantized the linear layer. Since LLaMA model checkpoints are unavailable publicly, and our goal is to evaluate performance, the quantized model is initialized with random weights.Recent literature such as <a href="https://arxiv.org/pdf/2306.00978.pdf">AWQ</a> and <a href="https://arxiv.org/pdf/2305.12356.pdf">Integer or Floating Point?</a> offer insights into performance properties of LLaMA under various low-bit quantization schemes.</p>

<h3 id="effect-of-batch-size-on-quantization-performance">Effect of Batch Size on Quantization Performance</h3>

<p><a href="https://arxiv.org/pdf/2304.01433.pdf">TPU v4</a> is programmed to run <code class="language-plaintext highlighter-rouge">matmul</code> on the Matrix Multiply Unit (MXU) when the model batch size (BS) &gt; 1. For BS = 1, <code class="language-plaintext highlighter-rouge">matmul</code> runs on the Vector Processor Unit (VPU). Since MXU is more efficient than VPU, <code class="language-plaintext highlighter-rouge">INT8</code> quantization gains performance at BS&gt;1. See <a href="#heading=h.4xqv3t16rl42">Performance Analysis</a> section for details.</p>

<h2 id="op-support">Op Support</h2>

<p>Occasionally, new models introduce new mathematical operations that require PyTorch/XLA to extend its supported op set for compilation. For LLaMA, we supported: <a href="https://github.com/pytorch/xla/issues/4839">multinomial</a>.</p>

<h2 id="methodology">Methodology</h2>

<p>LLaMA works on PyTorch/XLA out of the box on LazyTensorCore. We use this configuration as a baseline for our follow up analysis. All experiments assume 256-long input prompts. In the absence of a publicly available model checkpoint, we used random tensor initialization for this inference stack optimization effort. A model checkpoint is not expected to change latency results discussed here.</p>

<h3 id="model-sizing">Model Sizing</h3>

<p>Assuming <code class="language-plaintext highlighter-rouge">N</code> is the number of parameters, <code class="language-plaintext highlighter-rouge">dimensions</code> is the hidden size, <code class="language-plaintext highlighter-rouge">n_layers</code> is the number of layers, <code class="language-plaintext highlighter-rouge">n_heads</code> is the number of attention heads, the equation below can be used to approximate the model size. See the <a href="#heading=h.tehlvi942ssk">Model Overview</a> section for details.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>N = (dimensions)^2 * n_layers * 12
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">n_heads</code> doesn’t affect <code class="language-plaintext highlighter-rouge">N</code>, but the following equation holds for the open sourced model configs.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dim = 128 * n_heads
</code></pre></div></div>

<h4 id="cache-sizing">Cache Sizing</h4>

<p>Both model parameters and the cache layers in the Attention block contribute to memory consumption. Since the default LLaMA model uses <code class="language-plaintext highlighter-rouge">BF16</code> weights, the memory consumption calculation in this section is based on <code class="language-plaintext highlighter-rouge">BF16</code> weights.</p>

<p>The size of the cache layer is calculated by <code class="language-plaintext highlighter-rouge">cache_size = max_batch_size * max_seq_len * dimensions</code>. <code class="language-plaintext highlighter-rouge">max_batch_size = 1</code> and <code class="language-plaintext highlighter-rouge">max_seq_len = 256 </code>are used as an example configuration in the following calculations. There are 2 cache layers in each Attention block. So, the total LLaMA cache size (in Bytes) is <code class="language-plaintext highlighter-rouge">total_cache_size = n_layers * 2 * cache_size * (2 bytes)</code>.</p>

<h4 id="tpu-v4-hardware-sizing">TPU v4 Hardware Sizing</h4>

<p>Each TPU v4 chip has 32GB of available High-Bandwidth Memory (HBM). Table 2 has the details on memory consumption and the number of required TPU chips to hold a LLaMA model.</p>

<h4 id="table-2-llama-tpu-v4-hbm-requirements-ie-tpu-v4-chip-requirements">Table 2: LLaMA TPU v4 HBM requirements (i.e. TPU v4 chip requirements)</h4>

<table>
  <tr>
   <td><strong># Parameters</strong>
   </td>
   <td><strong>Parameter (MB)</strong>
   </td>
   <td><strong>Cache (MB)</strong>
   </td>
   <td><strong>Total (GB)</strong>
   </td>
   <td><strong>Min # of TPU v4 Chips</strong>
   </td>
  </tr>
  <tr>
   <td><strong>7B</strong>
   </td>
   <td>14,000
   </td>
   <td>134
   </td>
   <td>14.128
   </td>
   <td>1
   </td>
  </tr>
  <tr>
   <td><strong>33B</strong>
   </td>
   <td>66,000
   </td>
   <td>408
   </td>
   <td>66.41
   </td>
   <td>3
   </td>
  </tr>
  <tr>
   <td><strong>65B</strong>
   </td>
   <td>130,000
   </td>
   <td>671
   </td>
   <td>130.67
   </td>
   <td>5
   </td>
  </tr>
  <tr>
   <td><strong>175B</strong>
   </td>
   <td>350,000
   </td>
   <td>1,208
   </td>
   <td>351.21
   </td>
   <td>11
   </td>
  </tr>
</table>

<h3 id="metrics">Metrics</h3>

<p>Below are useful metrics to measure inference speed. Assuming <code class="language-plaintext highlighter-rouge">T</code> is the total time, <code class="language-plaintext highlighter-rouge">B</code> is the batch size, <code class="language-plaintext highlighter-rouge">L</code> is the decoded sequence length.</p>

<h4 id="latency-definition">Latency Definition</h4>

<p>Latency is the time it takes to get the decoded result at target length <code class="language-plaintext highlighter-rouge">L</code>, regardless of the batch size <code class="language-plaintext highlighter-rouge">B</code>. Latency represents how long the user should wait to get the response from the generation model.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Latency = T (s)
</code></pre></div></div>

<h4 id="per-token-latency">Per-token latency</h4>

<p>One step of autoregressive decoding generates a token for each sample in the batch. Per-token latency is the average time for that one step.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Per-token latency = T / L (s/token)
</code></pre></div></div>

<h4 id="throughput">Throughput</h4>

<p>Throughput measures how many tokens are generated per unit time. While it’s not a useful metric for evaluating online serving it is useful to measure the speed of batch processing.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Throughput = B * L / T (tokens/s)
</code></pre></div></div>

<p>To minimize confusion and misinterpretation, it’s better to avoid metrics like <code class="language-plaintext highlighter-rouge">T / (B * L)</code>, which mixes latency and throughput.</p>

<h2 id="results">Results</h2>

<p>Figure 1 shows latency / token results for LLaMA 7B to 175B models. In each case, the model is run on a range of TPU v4 configurations. For instance, LLaMA 7B shows 4.7ms/token and 3.8ms/token on v4-8 and v4-16 respectively. For more comparison, visit the HuggingFace <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">LLM performance leaderboard</a>.</p>

<p>In the absence of the features discussed in this blog post, the LLaMA 65B running on v4-32 delivers 120ms/token instead of 14.5ms/token obtained here, leading to <strong>8.3x</strong> speedup. As discussed earlier, developers are encouraged to try our custom <a href="https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch-nightly+20230422-cp38-cp38-linux_x86_64.whl">torch</a>, <a href="https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch_xla-nightly+20230422-cp38-cp38-linux_x86_64.whl">torch-xla</a> wheels that unlock the repro of <a href="https://github.com/pytorch-tpu/llama/tree/blog">LLaMA inference</a> results shared here.</p>

<p><img src="/assets/images/low-latency/im1.svg" alt="Figure 1: LLaMA Inference Performance on TPU v4 hardware" style="max-height:800px; width:100%" /></p>

<p><small style="line-height: 1.1"><em><strong>Figure 1</strong>: LLaMA Inference Performance on TPU v4 hardware</em></small></p>

<p>PyTorch/XLA:GPU performance is better than PyTorch:GPU eager and similar to PyTorch Inductor. PyTorch/XLA:TPU performance is superior to PyTorch/XLA:GPU. In the near future, XLA:GPU will deliver optimizations that bring parity with XLA:TPU. The single A100 configuration only fits LLaMA 7B, and the 8-A100 doesn’t fit LLaMA 175B.</p>

<p><img src="/assets/images/low-latency/im2.svg" alt="Figure 2: LLaMA Inference Performance on GPU A100 hardware" style="max-height:800px; width:100%" /></p>

<p><small style="line-height: 1.1"><em><strong>Figure 2</strong>: LLaMA Inference Performance on GPU A100 hardware</em></small></p>

<p>As the batch size increases, we observe a sublinear increase in per-token latency highlighting the tradeoff between hardware utilization and latency.</p>

<p><img src="/assets/images/low-latency/im3.svg" alt="Figure 3: LLaMA Inference Performance across different batch sizes" style="max-height:800px; width:100%" /></p>

<p><small style="line-height: 1.1"><em><strong>Figure 3</strong>: LLaMA Inference Performance across different batch sizes</em></small></p>

<p>Our studies suggest the impact of maximum sequence input length (<code class="language-plaintext highlighter-rouge">max_seq_len</code>) on inference latency is relatively minimal. We attribute this to the sequential and iterative nature of token generation. The small difference in performance can be due to KV cache access latency changes as the storage size increases.</p>

<p><img src="/assets/images/low-latency/im4.svg" alt="Figure 4: LLaMA Inference Performance across different prompt lengths" style="max-height:800px; width:100%" /></p>

<p><small style="line-height: 1.1"><em><strong>Figure 4</strong>: LLaMA Inference Performance across different prompt lengths</em></small></p>

<p>LLMs are often memory bound applications; thus, by quantizing model parameters we enable loading and executing a larger tensor on MXUs per unit time (i.e. HBM ⇒ CMEM and CMEM ⇒ MXU data moevment). Figure 5 shows <code class="language-plaintext highlighter-rouge">INT8</code> weight-only quantization offers 1.6x-1.9x speedup allowing running a larger model on a given hardware.</p>

<p>When BS=1, INT8 tensors are dispatched to VPU which is smaller than MXU (see the <a href="https://arxiv.org/pdf/2304.01433.pdf">TPU v4 paper</a>); otherwise, MXU is used. As a result, when BS=1, quantization memory bandwidth gains are offset by lack of MXU utilization. When BS&gt;1, however, memory gains deliver superior latency on the quantized model. For example, in the case of 175B parameters LLaMA, v4-16 with quantiztion and v4-32 without quantiztion deliver similar performance. Note we do not provied <code class="language-plaintext highlighter-rouge">FP8</code> comparisons because PyTorch is yet to offer this data type.</p>

<p><img src="/assets/images/low-latency/im5.svg" alt="Figure 5: LLaMA Inference Performance vs. weight-only quantization. The missing blue bars suggest the model size doesn’t fit in the specified TPU hardware." style="max-height:800px; width:100%" /></p>

<p><small style="line-height: 1.1"><em><strong>Figure 5</strong>: LLaMA Inference Performance vs. weight-only quantization. The missing blue bars suggest the model size doesn’t fit in the specified TPU hardware.</em></small></p>

<p>Figure 6 demonstrates the steady performance advantage of PyTorch/XLA as the input prompt length grows from 10 tokens to 1,500 tokens. This strong scaling capability suggests minimal PyTorch/XLA recompilation events enabling a wide range of real-world applications. In this experiment, the maximum length is 2,048 and maximum generation length is 256.</p>

<p><img src="/assets/images/low-latency/im6.svg" alt="Figure 6: LLaMA Inference Performance vs. Input Prompt Length" style="max-height:800px; width:100%" /></p>

<p><small style="line-height: 1.1"><em><strong>Figure 6</strong>: LLaMA Inference Performance vs. Input Prompt Length</em></small></p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>We are ecstatic about what’s ahead for PyTorch/XLA and invite the community to join us. PyTorch/XLA is developed fully in open source. So, please file issues, submit pull requests, and send RFCs to <a href="https://github.com/pytorch/xla">GitHub</a> so that we can openly collaborate. You can also <a href="https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb">try out</a> PyTorch/XLA for yourself on various XLA devices including TPUs and GPUs.</p>

<p>Cheers,<br />
The PyTorch/XLA Team at Google<br />
#PoweredByPyTorch</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          <li><a href="https://github.com/pytorch/pytorch/security/policy" target="_blank">Security</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>Stay up to date</p></li>
          <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
          <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
          <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
          <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          <li><a href="https://social.lfx.dev/@pytorch" rel="me" target="_blank">Mastodon</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>PyTorch Podcasts</p></li>
          <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
          <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
          <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
          <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
        </ul>
      </div>
    </div>

    <div class="privacy-policy">
      <ul>
        <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
        <li class="privacy-policy-links">|</li>
        <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
      </ul>
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/ecosystem/ptc/2022">PyTorch Conference - 2022</a>
          </li>
          <li>
            <a href="https://events.linuxfoundation.org/pytorch-conference/">PyTorch Conference - 2023</a>
          </li>
          <li>
            <a href="/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
        </ul>

        <li class="active">
          <a href="/blog">Blog</a>
        </li>

        <li>
          <a href="https://pytorch.org/tutorials">Tutorials</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/docs">Docs</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/docs">PyTorch</a>
          </li>

          <li class="">
            <a href="/audio">torchaudio</a>
          </li>

          <li class="">
            <a href="/text">torchtext</a>
          </li>

          <li class="">
            <a href="/vision">torchvision</a>
          </li>

          <li class="">
            <a href="/torcharrow">torcharrow</a>
          </li>

          <li class="">
            <a href="/data">TorchData</a>
          </li> 

          <li class="">
            <a href="/torchrec">TorchRec</a>
          </li>

          <li class="">
            <a href="/serve">TorchServe</a>
          </li>

          <li class="">
            <a href="/xla/release/1.6/index.html">PyTorch on XLA Devices</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          Resources
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/features">About</a>
          </li>

          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          
          <li>
            <a href="/#community-module">Community</a>
          </li>
          
          <li class="">
            <a href="/community-stories">Community stories</a>
          </li>

          <li class="">
            <a href="/resources">Developer Resources</a>
          </li>

          <li>
            <a href="/events">Events</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.org">Forum</a>
          </li>

          <li class="">
            <a href="/hub">Models (Beta)</a>
          </li>

        </ul>

        <li id="github-mobile-menu-link">
          <a href="https://github.com/pytorch/pytorch">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

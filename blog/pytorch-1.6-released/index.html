<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-52DXT37');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      PyTorch 1.6 released w/ Native AMP Support, Microsoft joins as maintainers for Windows | PyTorch
    
  </title>
  
  <meta property="og:title" content="PyTorch" />
  <meta
    name="description"
    property="og:description"
    content="An open source machine learning framework that accelerates the path from research prototyping to production deployment."
  />
  <meta
  property="og:image"
  content="https://pytorch.org/assets/images/pytorch-logo.png"
  />
  <meta property="og:url" content="https://www.pytorch.org" />


<meta property="og:type" content="website" />
<meta name="robots" content="index, follow" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-52DXT37"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow" href="/ecosystem">
          Ecosystem
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/pted/2021">
            <span class="dropdown-title">Ecosystem Day - 2021</span>
            <p>See the posters presented at ecosystem day 2021</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/ptdd/2021">
            <span class="dropdown-title">Developer Day - 2021</span>
            <p>See the posters presented at developer day 2021</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item ">
      <a href="/mobile">Mobile</a>
    </li>

    <li class="main-menu-item active">
      <a href="/blog">Blog</a>
    </li>

    <li class="main-menu-item">
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="doc-option with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/docs">
            <span class="dropdown-title docs-title">PyTorch</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/audio">
            <span class="dropdown-title docs-title">torchaudio</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/text">
            <span class="dropdown-title docs-title">torchtext</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/vision">
            <span class="dropdown-title docs-title">torchvision</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/serve">
            <span class="dropdown-title docs-title">TorchServe</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/xla/release/1.6/index.html">
            <span class="dropdown-title docs-title">PyTorch on XLA Devices</span>
            <p></p>
          </a>
        </div>
      </div>
    </li>

    

    <li class="main-menu-item ">

      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Resources
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/features">
            <span class=dropdown-title>About</span>
            <p>Learn about PyTorch’s features and capabilities</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/hub">
            <span class=dropdown-title>Models (Beta)</span>
            <p>Discover, publish, and reuse pre-trained models</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">July 28, 2020</p>
            <h1>
                <a class="blog-title">PyTorch 1.6 released w/ Native AMP Support, Microsoft joins as maintainers for Windows</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Team PyTorch
                      
                    </p>
                    <p>Today, we’re announcing the availability of PyTorch 1.6, along with updated domain libraries. We are also excited to announce the team at <a href="https://pytorch.org/blog/microsoft-becomes-maintainer-of-the-windows-version-of-pytorch">Microsoft is now maintaining Windows builds and binaries</a> and will also be supporting the community on GitHub as well as the PyTorch Windows discussion forums.</p>

<p>The PyTorch 1.6 release includes a number of new APIs, tools for performance improvement and profiling, as well as major updates to both distributed data parallel (DDP) and remote procedure call (RPC) based distributed training. 
A few of the highlights include:</p>

<ol>
  <li>Automatic mixed precision (AMP) training is now natively supported and a stable feature (See <a href="https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/">here</a> for more details) - thanks for NVIDIA’s contributions;</li>
  <li>Native TensorPipe support now added for tensor-aware, point-to-point communication primitives built specifically for machine learning;</li>
  <li>Added support for complex tensors to the frontend API surface;</li>
  <li>New profiling tools providing tensor-level memory consumption information;</li>
  <li>Numerous improvements and new features for both distributed data parallel (DDP) training and the remote procedural call (RPC) packages.</li>
</ol>

<p>Additionally, from this release onward, features will be classified as Stable, Beta and Prototype. Prototype features are not included as part of the binary distribution and are instead available through either building from source, using nightlies or via compiler flag. You can learn more about what this change means in the post <a href="https://pytorch.org/blog/pytorch-feature-classification-changes/">here</a>. You can also find the full release notes <a href="https://github.com/pytorch/pytorch/releases">here</a>.</p>

<h1 id="performance--profiling">Performance &amp; Profiling</h1>

<h2 id="stable-automatic-mixed-precision-amp-training">[Stable] Automatic Mixed Precision (AMP) Training</h2>

<p>AMP allows users to easily enable automatic mixed precision training enabling higher performance and memory savings of up to 50% on Tensor Core GPUs. Using the natively supported <code class="language-plaintext highlighter-rouge">torch.cuda.amp</code> API, AMP provides convenience methods for mixed precision, where some operations use the <code class="language-plaintext highlighter-rouge">torch.float32 (float)</code> datatype and other operations use <code class="language-plaintext highlighter-rouge">torch.float16 (half)</code>. Some ops, like linear layers and convolutions, are much faster in <code class="language-plaintext highlighter-rouge">float16</code>. Other ops, like reductions, often require the dynamic range of <code class="language-plaintext highlighter-rouge">float32</code>. Mixed precision tries to match each op to its appropriate datatype.</p>

<ul>
  <li>Design doc (<a href="https://github.com/pytorch/pytorch/issues/25081">Link</a>)</li>
  <li>Documentation (<a href="https://pytorch.org/docs/stable/amp.html">Link</a>)</li>
  <li>Usage examples (<a href="https://pytorch.org/docs/stable/notes/amp_examples.html">Link</a>)</li>
</ul>

<h2 id="beta-forkjoin-parallelism">[Beta] Fork/Join Parallelism</h2>

<p>This release adds support for a language-level construct as well as runtime support for coarse-grained parallelism in TorchScript code. This support is useful for situations such as running models in an ensemble in parallel, or running bidirectional components of recurrent nets in parallel, and allows the ability to unlock the computational power of parallel architectures (e.g. many-core CPUs) for task level parallelism.</p>

<p>Parallel execution of TorchScript programs is enabled through two primitives: <code class="language-plaintext highlighter-rouge">torch.jit.fork</code> and <code class="language-plaintext highlighter-rouge">torch.jit.wait</code>. In the below example, we parallelize execution of <code class="language-plaintext highlighter-rouge">foo</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>

<span class="k">def</span> <span class="nf">foo</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">neg</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="o">@</span><span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">script</span>
<span class="k">def</span> <span class="nf">example</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">futures</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">fork</span><span class="p">(</span><span class="n">foo</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)]</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">wait</span><span class="p">(</span><span class="n">future</span><span class="p">)</span> <span class="k">for</span> <span class="n">future</span> <span class="ow">in</span> <span class="n">futures</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">results</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="n">example</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">([])))</span>
</code></pre></div></div>

<ul>
  <li>Documentation (<a href="https://pytorch.org/docs/stable/jit.html">Link</a>)</li>
</ul>

<h2 id="beta-memory-profiler">[Beta] Memory Profiler</h2>

<p>The <code class="language-plaintext highlighter-rouge">torch.autograd.profiler</code> API now includes a memory profiler that lets you inspect the tensor memory cost of different operators inside your CPU and GPU models.</p>

<p>Here is an example usage of the API:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision.models</span> <span class="k">as</span> <span class="n">models</span>
<span class="kn">import</span> <span class="nn">torch.autograd.profiler</span> <span class="k">as</span> <span class="n">profiler</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">resnet18</span><span class="p">()</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="k">with</span> <span class="n">profiler</span><span class="p">.</span><span class="n">profile</span><span class="p">(</span><span class="n">profile_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">record_shapes</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

<span class="c1"># NOTE: some columns were removed for brevity
</span><span class="k">print</span><span class="p">(</span><span class="n">prof</span><span class="p">.</span><span class="n">key_averages</span><span class="p">().</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s">"self_cpu_memory_usage"</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
<span class="c1"># ---------------------------  ---------------  ---------------  ---------------
# Name                         CPU Mem          Self CPU Mem     Number of Calls
# ---------------------------  ---------------  ---------------  ---------------
# empty                        94.79 Mb         94.79 Mb         123
# resize_                      11.48 Mb         11.48 Mb         2
# addmm                        19.53 Kb         19.53 Kb         1
# empty_strided                4 b              4 b              1
# conv2d                       47.37 Mb         0 b              20
# ---------------------------  ---------------  ---------------  ---------------
</span></code></pre></div></div>

<ul>
  <li>PR (<a href="https://github.com/pytorch/pytorch/pull/37775">Link</a>)</li>
  <li>Documentation (<a href="https://pytorch.org/docs/stable/autograd.html#profiler">Link</a>)</li>
</ul>

<h1 id="distributed-training--rpc">Distributed Training &amp; RPC</h1>

<h2 id="beta-tensorpipe-backend-for-rpc">[Beta] TensorPipe backend for RPC</h2>

<p>PyTorch 1.6 introduces a new backend for the RPC module which leverages the TensorPipe library, a tensor-aware point-to-point communication primitive targeted at machine learning, intended to complement the current primitives for distributed training in PyTorch (Gloo, MPI, …) which are collective and blocking. The pairwise and asynchronous nature of TensorPipe lends itself to new networking paradigms that go beyond data parallel: client-server approaches (e.g., parameter server for embeddings, actor-learner separation in Impala-style RL, …) and model and pipeline parallel training (think GPipe), gossip SGD, etc.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># One-line change needed to opt in
</span><span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">rpc</span><span class="p">.</span><span class="n">init_rpc</span><span class="p">(</span>
    <span class="p">...</span>
    <span class="n">backend</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">rpc</span><span class="p">.</span><span class="n">BackendType</span><span class="p">.</span><span class="n">TENSORPIPE</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># No changes to the rest of the RPC API
</span><span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">rpc</span><span class="p">.</span><span class="n">rpc_sync</span><span class="p">(...)</span>
</code></pre></div></div>

<ul>
  <li>Design doc (<a href="https://github.com/pytorch/pytorch/issues/35251">Link</a>)</li>
  <li>Documentation (<a href="https://pytorch.org/docs/stable/rpc/index.html">Link</a>)</li>
</ul>

<h2 id="beta-ddprpc">[Beta] DDP+RPC</h2>

<p>PyTorch Distributed supports two powerful paradigms: DDP for full sync data parallel training of models and the RPC framework which allows for distributed model parallelism. Previously, these two features worked independently and users couldn’t mix and match these to try out hybrid parallelism paradigms.</p>

<p>Starting in PyTorch 1.6, we’ve enabled DDP and RPC to work together seamlessly so that users can combine these two techniques to achieve both data parallelism and model parallelism. An example is where users would like to place large embedding tables on parameter servers and use the RPC framework for embedding lookups, but store smaller dense parameters on trainers and use DDP to synchronize the dense parameters. Below is a simple code snippet.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">//</span> <span class="n">On</span> <span class="n">each</span> <span class="n">trainer</span>

<span class="n">remote_emb</span> <span class="o">=</span> <span class="n">create_emb</span><span class="p">(</span><span class="n">on</span><span class="o">=</span><span class="s">"ps"</span><span class="p">,</span> <span class="p">...)</span>
<span class="n">ddp_model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">dense_model</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
   <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">context</span><span class="p">():</span>
      <span class="n">res</span> <span class="o">=</span> <span class="n">remote_emb</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">ddp_model</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
      <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">backward</span><span class="p">([</span><span class="n">loss</span><span class="p">])</span>
</code></pre></div></div>

<ul>
  <li>DDP+RPC Tutorial (<a href="https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html">Link</a>)</li>
  <li>Documentation (<a href="https://pytorch.org/docs/stable/rpc/index.html">Link</a>)</li>
  <li>Usage Examples (<a href="https://github.com/pytorch/examples/pull/800">Link</a>)</li>
</ul>

<h2 id="beta-rpc---asynchronous-user-functions">[Beta] RPC - Asynchronous User Functions</h2>

<p>RPC Asynchronous User Functions supports the ability to yield and resume on the server side when executing a user-defined function. Prior to this feature, when a callee processes a request, one RPC thread waits until the user function returns. If the user function contains IO (e.g., nested RPC) or signaling (e.g., waiting for another request to unblock), the corresponding RPC thread would sit idle waiting for these events. As a result, some applications have to use a very large number of threads and send additional RPC requests, which can potentially lead to performance degradation. To make a user function yield on such events, applications need to: 1) Decorate the function with the <code class="language-plaintext highlighter-rouge">@rpc.functions.async_execution</code> decorator; and 2) Let the function return a <code class="language-plaintext highlighter-rouge">torch.futures.Future</code> and install the resume logic as callbacks on the <code class="language-plaintext highlighter-rouge">Future</code> object. See below for an example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">rpc</span><span class="p">.</span><span class="n">functions</span><span class="p">.</span><span class="n">async_execution</span>
<span class="k">def</span> <span class="nf">async_add_chained</span><span class="p">(</span><span class="n">to</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">rpc</span><span class="p">.</span><span class="n">rpc_async</span><span class="p">(</span><span class="n">to</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)).</span><span class="n">then</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">fut</span><span class="p">:</span> <span class="n">fut</span><span class="p">.</span><span class="n">wait</span><span class="p">()</span> <span class="o">+</span> <span class="n">z</span>
    <span class="p">)</span>

<span class="n">ret</span> <span class="o">=</span> <span class="n">rpc</span><span class="p">.</span><span class="n">rpc_sync</span><span class="p">(</span>
    <span class="s">"worker1"</span><span class="p">,</span> 
    <span class="n">async_add_chained</span><span class="p">,</span> 
    <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="s">"worker2"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>
        
<span class="k">print</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>  <span class="c1"># prints tensor([3., 3.])
</span></code></pre></div></div>

<ul>
  <li>Tutorial for performant batch RPC using Asynchronous User Functions (<a href="https://github.com/pytorch/tutorials/blob/release/1.6/intermediate_source/rpc_async_execution.rst">Link</a>)</li>
  <li>Documentation (<a href="https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.functions.async_execution">Link</a>)</li>
  <li>Usage examples (<a href="https://github.com/pytorch/examples/tree/master/distributed/rpc/batch">Link</a>)</li>
</ul>

<h1 id="frontend-api-updates">Frontend API Updates</h1>

<h2 id="beta-complex-numbers">[Beta] Complex Numbers</h2>

<p>The PyTorch 1.6 release brings beta level support for complex tensors including torch.complex64 and torch.complex128 dtypes. A complex number is a number that can be expressed in the form a + bj, where a and b are real numbers, and j is a solution of the equation x^2 = −1. Complex numbers frequently occur in mathematics and engineering, especially in signal processing and the area of complex neural networks is an active area of research. The beta release of complex tensors will support common PyTorch and complex tensor functionality, plus functions needed by Torchaudio, ESPnet and others. While this is an early version of this feature, and we expect it to improve over time, the overall goal is provide a NumPy compatible user experience that leverages PyTorch’s ability to run on accelerators and work with autograd to better support the scientific community.</p>

<h1 id="mobile-updates">Mobile Updates</h1>

<p>PyTorch 1.6 brings increased performance and general stability for mobile on-device inference. We squashed a few bugs, continued maintenance and added few new features while improving fp32 and int8 performance on a large variety of ML model inference on CPU backend.</p>

<h2 id="beta-mobile-features-and-performance">[Beta] Mobile Features and Performance</h2>

<ul>
  <li>Stateless and stateful XNNPACK Conv and Linear operators</li>
  <li>Stateless MaxPool2d + JIT optimization passes</li>
  <li>JIT pass optimizations: Conv + BatchNorm fusion, graph rewrite to replace conv2d/linear with xnnpack ops, relu/hardtanh fusion, dropout removal</li>
  <li>QNNPACK integration removes requantization scale constraint</li>
  <li>Per-channel quantization for conv, linear and dynamic linear</li>
  <li>Disable tracing for mobile client to save ~600 KB on full-jit builds</li>
</ul>

<h1 id="updated-domain-libraries">Updated Domain Libraries</h1>

<h2 id="torchvision-07">torchvision 0.7</h2>

<p>torchvision 0.7 introduces two new pretrained semantic segmentation models, <a href="https://arxiv.org/abs/1411.4038">FCN ResNet50</a> and <a href="https://arxiv.org/abs/1706.05587">DeepLabV3 ResNet50</a>, both trained on COCO and using smaller memory footprints than the ResNet101 backbone. We also introduced support for AMP (Automatic Mixed Precision) autocasting for torchvision models and operators, which automatically selects the floating point precision for different GPU operations to improve performance while maintaining accuracy.</p>

<ul>
  <li>Release notes (<a href="https://github.com/pytorch/vision/releases">Link</a>)</li>
</ul>

<h2 id="torchaudio-06">torchaudio 0.6</h2>

<p>torchaudio now officially supports Windows. This release also introduces a new model module (with wav2letter included), new functionals (contrast, cvm, dcshift, overdrive, vad, phaser, flanger, biquad), datasets (GTZAN, CMU), and a new optional sox backend with support for TorchScript.</p>

<ul>
  <li>Release notes (<a href="https://github.com/pytorch/audio/releases">Link</a>)</li>
</ul>

<h1 id="additional-updates">Additional updates</h1>

<h2 id="hackathon">HACKATHON</h2>

<p>The Global PyTorch Summer Hackathon is back! This year, teams can compete in three categories virtually:</p>

<ol>
  <li><strong>PyTorch Developer Tools:</strong> Tools or libraries designed to improve productivity and efficiency of PyTorch for researchers and developers</li>
  <li><strong>Web/Mobile Applications powered by PyTorch:</strong> Applications with web/mobile interfaces and/or embedded devices powered by PyTorch</li>
  <li><strong>PyTorch Responsible AI Development Tools:</strong> Tools, libraries, or web/mobile apps for responsible AI development</li>
</ol>

<p>This is a great opportunity to connect with the community and practice your machine learning skills.</p>

<ul>
  <li><a href="http://pytorch2020.devpost.com/">Join the hackathon</a></li>
  <li><a href="https://www.youtube.com/pytorch">Watch educational videos</a></li>
</ul>

<h2 id="lpcv-challenge">LPCV Challenge</h2>

<p>The <a href="https://lpcv.ai/2020CVPR/video-track">2020 CVPR Low-Power Vision Challenge (LPCV) - Online Track for UAV video</a> submission deadline is coming up shortly. You have until July 31, 2020 to build a system that can discover and recognize characters in video captured by an unmanned aerial vehicle (UAV) accurately using PyTorch and Raspberry Pi 3B+.</p>

<h2 id="prototype-features">Prototype Features</h2>

<p>To reiterate, Prototype features in PyTorch are early features that we are looking to gather feedback on, gauge the usefulness of and improve ahead of graduating them to Beta or Stable. The following features are not part of the PyTorch 1.6 release and instead are available in nightlies with separate docs/tutorials to help facilitate early usage and feedback.</p>

<h4 id="distributed-rpcprofiler">Distributed RPC/Profiler</h4>
<p>Allow users to profile training jobs that use <code class="language-plaintext highlighter-rouge">torch.distributed.rpc</code> using the autograd profiler, and remotely invoke the profiler in order to collect profiling information across different nodes. The RFC can be found <a href="https://github.com/pytorch/pytorch/issues/39675">here</a> and a short recipe on how to use this feature can be found <a href="https://github.com/pytorch/tutorials/tree/master/prototype_source">here</a>.</p>

<h4 id="torchscript-module-freezing">TorchScript Module Freezing</h4>
<p>Module Freezing is the process of inlining module parameters and attributes values into the TorchScript internal representation. Parameter and attribute values are treated as final value and they cannot be modified in the frozen module. The PR for this feature can be found <a href="https://github.com/pytorch/pytorch/pull/32178">here</a> and a short tutorial on how to use this feature can be found <a href="https://github.com/pytorch/tutorials/tree/master/prototype_source">here</a>.</p>

<h4 id="graph-mode-quantization">Graph Mode Quantization</h4>
<p>Eager mode quantization requires users to make changes to their model, including explicitly quantizing activations, module fusion, rewriting use of torch ops with Functional Modules and quantization of functionals are not supported. If we can trace or script the model, then the quantization can be done automatically with graph mode quantization without any of the complexities in eager mode, and it is configurable through a <code class="language-plaintext highlighter-rouge">qconfig_dict</code>. A tutorial on how to use this feature can be found <a href="https://github.com/pytorch/tutorials/tree/master/prototype_source">here</a>.</p>

<h4 id="quantization-numerical-suite">Quantization Numerical Suite</h4>
<p>Quantization is good when it works, but it’s difficult to know what’s wrong when it doesn’t satisfy the expected accuracy. A prototype is now available for a Numerical Suite that measures comparison statistics between quantized modules and float modules. This is available to test using eager mode and on CPU only with more support coming. A tutorial on how to use this feature can be found <a href="https://github.com/pytorch/tutorials/tree/master/prototype_source">here</a>.</p>

<p>Cheers!</p>

<p>Team PyTorch</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>Stay up to date</p></li>
          <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
          <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
          <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
          <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>PyTorch Podcasts</p></li>
          <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
          <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
          <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
          <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
        </ul>
      </div>
    </div>

    <div class="privacy-policy">
      <ul>
        <li class="privacy-policy-links"><a href="/assets/tos-oss-privacy-policy/fb-tos-privacy-policy.pdf" target="_blank">Terms</a></li>
        <li class="privacy-policy-links">|</li>
        <li class="privacy-policy-links"><a href="/assets/tos-oss-privacy-policy/fb-oss-privacy-policy.pdf" target="_blank">Privacy</a></li>
      </ul>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/ecosystem">Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/ecosystem/pted/2021">Ecosystem Day 2021</a>
          </li>
          <li>
            <a href="/ecosystem/ptdd/2021">Developer Day 2021</a>
          </li>
        </ul>

        <li class="">
          <a href="/mobile">Mobile</a>
        </li>

        <li class="active">
          <a href="/blog">Blog</a>
        </li>

        <li>
          <a href="https://pytorch.org/tutorials">Tutorials</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/docs">Docs</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/docs">PyTorch</a>
          </li>

          <li class="">
            <a href="/audio">torchaudio</a>
          </li>

          <li class="">
            <a href="/text">torchtext</a>
          </li>

          <li class="">
            <a href="/docs/stable/torchvision">torchvision</a>
          </li>

          <li class="">
            <a href="/elastic">TorchElastic</a>
          </li>

          <li class="">
            <a href="/serve">TorchServe</a>
          </li>

          <li class="">
            <a href="/xla/release/1.6/index.html">PyTorch on XLA Devices</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          Resources
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/features">About</a>
          </li>

          <li>
            <a href="/#community-module">Community</a>
          </li>

          <li>
            <a href="/events">Events</a>
          </li>

          <li class="">
            <a href="/resources">Developer Resources</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.org">Forum</a>
          </li>

          <li class="">
            <a href="/hub">Models (Beta)</a>
          </li>

        </ul>

        <li id="github-mobile-menu-link">
          <a href="https://github.com/pytorch/pytorch">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      Stochastic Weight Averaging in PyTorch | PyTorch
    
  </title>
  <meta property="og:title" content="PyTorch"/>
<meta property="og:description" content="An open source deep learning platform that provides a seamless path from research prototyping to production deployment."/>
<meta property="og:url" content="https://www.pytorch.org"/>
<meta property="og:type" content="website"/>
<meta property="og:image" content="https://pytorch.org/assets/images/pytorch-logo.png"/>

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('create', 'UA-117752657-2', 'auto', 'newCampaignTracker');
  ga('send', 'pageview');

</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>

  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');

</script>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
</head>


<body class="blog">
    <div class="main-background blog-background blog-detail-background"></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="main-menu-item ">

      <div class="ecosystem-dropdown">
        <a id="dropdownMenuButton" data-toggle="ecosystem-dropdown">
          Ecosystem
        </a>
        <div class="ecosystem-dropdown-menu">
          <a class="nav-dropdown-item" href="/hub">
            <span class=dropdown-title>Models (Beta)</span>
            <p>Discover, publish, and reuse pre-trained models</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class=dropdown-title>Tools & Libraries</span>
            <p>Explore the ecosystem of tools and libraries</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item ">
      <a href="/mobile">Mobile</a>
    </li>

    <li class="main-menu-item active">
      <a href="/blog">Blog</a>
    </li>

    <li class="main-menu-item">
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li class="main-menu-item">
      <a href="/docs">Docs</a>
    </li>

    <li class="main-menu-item ">

      <div class="resources-dropdown">
        <a id="resourcesDropdownButton" data-toggle="resources-dropdown">
          Resources
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/features">
            <span class=dropdown-title>About</span>
            <p>Learn about PyTorch’s features and capabilities</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">April 29, 2019</p>
            <h1>
                <a class="blog-title">Stochastic Weight Averaging in PyTorch</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Pavel Izmailov and Andrew Gordon Wilson
                      
                    </p>
                    <p>In this blogpost we describe the recently proposed Stochastic Weight Averaging (SWA) technique [1, 2], and its new implementation in <a href="https://github.com/pytorch/contrib"><code class="highlighter-rouge">torchcontrib</code></a>.  SWA is a simple procedure that improves generalization in deep learning over Stochastic Gradient Descent (SGD) at no additional cost, and can be used as a drop-in replacement for any other optimizer in PyTorch. SWA has a wide range of applications and features:</p>

<ol>
  <li>SWA has been shown to significantly improve generalization in computer vision tasks, including VGG, ResNets, Wide ResNets and DenseNets on ImageNet and CIFAR benchmarks [1, 2].</li>
  <li>SWA provides state-of-the-art performance on key benchmarks in semi-supervised learning and domain adaptation [2].</li>
  <li>SWA is shown to improve the stability of training as well as the final average rewards of policy-gradient methods in deep reinforcement learning [3].</li>
  <li>An extension of SWA can obtain efficient Bayesian model averaging, as well as high quality uncertainty estimates and calibration in deep learning [4].</li>
  <li>SWA for low precision training, SWALP, can match the performance of full-precision SGD even with all numbers quantized down to 8 bits, including gradient accumulators [5].</li>
</ol>

<p>In short, SWA performs an equal average of the weights traversed by SGD with a modified learning rate schedule (see the left panel of Figure 1.). SWA solutions end up in the center of a wide flat region of loss, while SGD tends to converge to the boundary of the low-loss region, making it susceptible to the shift between train and test error surfaces (see the middle and right panels of Figure 1).</p>

<div class="text-center">
  <img src="https://pytorch.org/assets/images/swa/Figure1.png" width="100%" />
</div>

<p><strong>Figure 1.</strong> Illustrations of SWA and SGD with a Preactivation ResNet-164 on CIFAR-100 [1]. <strong>Left:</strong> test error surface for three FGE samples and the corresponding SWA solution (averaging in weight space). <strong>Middle</strong> and <strong>Right:</strong> test error and train loss surfaces showing the weights proposed by SGD (at convergence) and SWA, starting from the same initialization of SGD after 125 training epochs. Please see [1] for details on how these figures were constructed.</p>

<p><strong>With our new implementation in <a href="https://github.com/pytorch/contrib">torchcontrib</a> using SWA is as easy as using any other optimizer in PyTorch:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchcontrib.optim</span> <span class="kn">import</span> <span class="n">SWA</span>

<span class="o">...</span>
<span class="o">...</span>

<span class="c"># training loop</span>
<span class="n">base_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torchcontrib</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SWA</span><span class="p">(</span><span class="n">base_opt</span><span class="p">,</span> <span class="n">swa_start</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">swa_freq</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">swa_lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
     <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
     <span class="n">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
     <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="n">opt</span><span class="o">.</span><span class="n">swap_swa_sgd</span><span class="p">()</span>
</code></pre></div></div>

<p>You can wrap any optimizer from <code class="highlighter-rouge">torch.optim</code> using the <code class="highlighter-rouge">SWA</code> class, and then train your model as usual. When training is complete you simply call <code class="highlighter-rouge">swap_swa_sgd()</code> to set the weights of your model to their SWA averages. Below we explain the SWA procedure and the parameters of the <code class="highlighter-rouge">SWA</code> class in detail. We emphasize that SWA can be combined with <em>any</em> optimization procedure, such as Adam, in the same way that it can be combined with SGD.</p>

<h2 id="is-this-just-averaged-sgd">Is this just Averaged SGD?</h2>

<p>At a high level, averaging SGD iterates dates back several decades in convex optimization [6, 7], where it is sometimes referred to as Polyak-Ruppert averaging, or <em>averaged</em> SGD. <strong>But the details matter</strong>. <em>Averaged SGD</em> is often employed in conjunction with a decaying learning rate, and an exponentially moving average, typically for convex optimization. In convex optimization, the focus has been on improved rates of convergence. In deep learning, this form of averaged SGD smooths the trajectory of SGD iterates, but does not perform very differently.</p>

<p>By contrast, SWA is focused on an <strong>equal average</strong> of SGD iterates with a modified <strong>cyclical or high constant learning rate</strong>, and exploits the flatness of training objectives [8] specific to <strong>deep learning</strong> for <strong>improved generalization</strong>.</p>

<h2 id="stochastic-weight-averaging">Stochastic Weight Averaging</h2>

<p>There are two important ingredients that make SWA work. First, SWA uses a modified learning rate schedule so that SGD continues to explore the set of high-performing networks instead of simply converging to a single solution. For example, we can use the standard decaying learning rate strategy for the first 75% of training time, and then set the learning rate to a reasonably high constant value for the remaining 25% of the time (see the Figure 2 below). The second ingredient is to average the weights of the networks traversed by SGD. For example, we can maintain a running average of the weights obtained in the end of every epoch within the last 25% of training time (see Figure 2).</p>
<div class="text-center">
  <img src="https://pytorch.org/assets/images/swa/figure2-highres.png" width="70%" />
</div>

<p><strong>Figure 2.</strong> Illustration of the learning rate schedule adopted by SWA. Standard decaying schedule is used for the first 75% of the training and then a high constant value is used for the remaining 25%. The SWA averages are formed during the last 25% of training.</p>

<p>In our implementation the auto mode of the <code class="highlighter-rouge">SWA</code> optimizer allows us to run the procedure described above. To run SWA in auto mode you just need to wrap your optimizer <code class="highlighter-rouge">base_opt</code> of choice (can be SGD, Adam, or any other <code class="highlighter-rouge">torch.optim.Optimizer</code>) with <code class="highlighter-rouge">SWA(base_opt, swa_start, swa_freq, swa_lr)</code>. After <code class="highlighter-rouge">swa_start</code> optimization steps the learning rate will be switched to a constant value <code class="highlighter-rouge">swa_lr</code>, and in the end of every <code class="highlighter-rouge">swa_freq</code> optimization steps a snapshot of the weights will be added to the SWA running average. Once you run <code class="highlighter-rouge">opt.swap_swa_sgd()</code>, the weights of your model are replaced with their SWA running averages.</p>

<h2 id="batch-normalization">Batch Normalization</h2>

<p>One important detail to keep in mind is batch normalization. Batch normalization layers compute running statistics of activations during training. Note that the SWA averages of the weights are never used to make predictions during training, and so the batch normalization layers do not have the activation statistics computed after you reset the weights of your model with <code class="highlighter-rouge">opt.swap_swa_sgd()</code>. To compute the activation statistics you can just make a forward pass on your training data using the SWA model once the training is finished. In the <code class="highlighter-rouge">SWA</code> class we provide a helper function <code class="highlighter-rouge">opt.bn_update(train_loader, model)</code>. It updates the activation statistics for every batch normalization layer in the model by making a forward pass on the <code class="highlighter-rouge">train_loader</code> data loader. You only need to call this function once in the end of training.</p>

<h2 id="advanced-learning-rate-schedules">Advanced Learning-Rate Schedules</h2>

<p>SWA can be used with any learning rate schedule that encourages exploration of the flat region of solutions. For example, you can use cyclical learning rates in the last 25% of the training time instead of a constant value, and average the weights of the networks corresponding to the lowest values of the learning rate within each cycle (see Figure 3).</p>

<div class="text-center">
  <img src="https://pytorch.org/assets/images/swa/figure3-highres.png" width="70%" />
</div>

<p><strong>Figure 3.</strong> Illustration of SWA with an alternative learning rate schedule. Cyclical learning rates are adopted in the last 25% of training, and models for averaging are collected in the end of each cycle.</p>

<p>In our implementation you can implement custom learning rate and weight averaging strategies by using <code class="highlighter-rouge">SWA</code> in the manual mode. The following code is equivalent to the auto mode code presented in the beginning of this blogpost.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">opt</span> <span class="o">=</span> <span class="n">torchcontrib</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SWA</span><span class="p">(</span><span class="n">base_opt</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">10</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">update_swa</span><span class="p">()</span>
<span class="n">opt</span><span class="o">.</span><span class="n">swap_swa_sgd</span><span class="p">()</span>
</code></pre></div></div>

<p>In manual mode you don’t specify <code class="highlighter-rouge">swa_start</code>, <code class="highlighter-rouge">swa_lr</code> and <code class="highlighter-rouge">swa_freq</code>, and just call <code class="highlighter-rouge">opt.update_swa()</code> whenever you want to update the SWA running averages (for example in the end of each learning rate cycle). In manual mode <code class="highlighter-rouge">SWA</code> doesn’t change the learning rate, so you can use any schedule you want as you would normally do with any other <code class="highlighter-rouge">torch.optim.Optimizer</code>.</p>

<h2 id="why-does-it-work">Why does it work?</h2>

<p>SGD converges to a solution within a wide flat region of loss. The weight space is extremely high-dimensional, and most of the volume of the flat region is concentrated near the boundary, so SGD solutions will always be found near the boundary of the flat region of the loss. SWA on the other hand averages multiple SGD solutions, which allows it to move towards the center of the flat region.</p>

<p>We expect solutions that are centered in the flat region of the loss to generalize better than those near the boundary. Indeed, train and test error surfaces are not perfectly aligned in the weight space. Solutions that are centered in the flat region are not as susceptible to the shifts between train and test error surfaces as those near the boundary. In Figure 4 below we show the train loss and test error surfaces along the direction connecting the SWA and SGD solutions. As you can see, while SWA solution has a higher train loss compared to the SGD solution, it is centered in the region of low loss, and has a substantially better test error.</p>

<div class="text-center">
  <img src="https://pytorch.org/assets/images/swa/Figure4.png" width="90%" />
</div>

<p><strong>Figure 4.</strong> Train loss and test error along the line connecting the SWA solution (circle) and SGD solution (square). SWA solution is centered in a wide region of low train loss while the SGD solution lies near the boundary. Because of the shift between train loss and test error surfaces, SWA solution leads to much better generalization.</p>

<h2 id="examples-and-results">Examples and Results</h2>

<p>We released a GitHub repo <a href="https://github.com/izmailovpavel/contrib_swa_examples">here</a> with examples of using the <code class="highlighter-rouge">torchcontrib</code> implementation of SWA for training DNNs. For example, these examples can be used to achieve the following results on CIFAR-100:</p>

<table>
  <thead>
    <tr>
      <th>DNN (Budget)</th>
      <th style="text-align: center">SGD</th>
      <th style="text-align: center">SWA 1 Budget</th>
      <th style="text-align: center">SWA 1.25 Budgets</th>
      <th style="text-align: center">SWA 1.5 Budgets</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>VGG16 (200)</td>
      <td style="text-align: center">72.55 ± 0.10</td>
      <td style="text-align: center">73.91 ± 0.12</td>
      <td style="text-align: center">74.17 ± 0.15</td>
      <td style="text-align: center">74.27 ± 0.25</td>
    </tr>
    <tr>
      <td>PreResNet110 (150)</td>
      <td style="text-align: center">76.77 ± 0.38</td>
      <td style="text-align: center">78.75 ± 0.16</td>
      <td style="text-align: center">78.91 ± 0.29</td>
      <td style="text-align: center">79.10 ± 0.21</td>
    </tr>
    <tr>
      <td>PreResNet164 (150)</td>
      <td style="text-align: center">78.49 ± 0.36</td>
      <td style="text-align: center">79.77 ± 0.17</td>
      <td style="text-align: center">80.18 ± 0.23</td>
      <td style="text-align: center">80.35 ± 0.16</td>
    </tr>
    <tr>
      <td>WideResNet28x10 (200)</td>
      <td style="text-align: center">80.82 ± 0.23</td>
      <td style="text-align: center">81.46 ± 0.23</td>
      <td style="text-align: center">81.91 ± 0.27</td>
      <td style="text-align: center">82.15 ± 0.27</td>
    </tr>
  </tbody>
</table>

<h2 id="semi-supervised-learning">Semi-Supervised Learning</h2>

<p>In a follow-up <a href="https://arxiv.org/abs/1806.05594">paper</a> SWA was applied to semi-supervised learning, where it illustrated  improvements beyond the best reported results in multiple settings. For example, with SWA you can get 95% accuracy on CIFAR-10 if you only have the training labels for 4k training data points (the previous best reported result on this problem was 93.7%). This paper also explores averaging multiple times within epochs, which can accelerate convergence and find still flatter solutions in a given time.</p>
<div class="text-center">
<img src="https://pytorch.org/assets/images/swa/Figure5.png" width="90%" />
</div>

<p><strong>Figure 5.</strong> Performance of fast-SWA on semi-supervised learning with CIFAR-10. fast-SWA achieves record results in every setting considered.</p>

<h2 id="calibration-and-uncertainty-estimates">Calibration and Uncertainty Estimates</h2>
<p><a href="https://arxiv.org/abs/1902.02476">SWA-Gaussian</a> (SWAG) is a simple, scalable and convenient approach to uncertainty estimation and calibration in Bayesian deep learning. Similarly to SWA, which maintains a running average of SGD iterates, SWAG estimates the first and second moments of the iterates to construct a Gaussian distribution over weights. SWAG distribution approximates the shape of the true posterior: Figure 6 below shows the SWAG distribution on top of the posterior log-density for PreResNet-164 on CIFAR-100.</p>
<div class="text-center">
<img src="https://pytorch.org/assets/images/swa/Figure6.png" width="90%" />
</div>
<p><strong>Figure 6.</strong> SWAG distribution on top of posterior log-density for PreResNet-164 on CIFAR-100. The shape of SWAG distribution is aligned with the posterior.</p>

<p>Empirically, SWAG performs on par or better than popular alternatives including MC dropout, KFAC Laplace, and temperature scaling on uncertainty quantification, out-of-distribution detection, calibration and transfer learning in computer vision tasks. Code for SWAG is available <a href="https://github.com/wjmaddox/swa_gaussian">here</a>.</p>

<h2 id="reinforcement-learning">Reinforcement Learning</h2>

<p>In another follow-up <a href="http://www.gatsby.ucl.ac.uk/~balaji/udl-camera-ready/UDL-24.pdf">paper</a> SWA was shown to improve the performance of policy gradient methods A2C and DDPG on several Atari games and MuJoCo environments.</p>

<table>
  <thead>
    <tr>
      <th>Environment</th>
      <th style="text-align: center">A2C</th>
      <th style="text-align: center">A2C + SWA</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Breakout</td>
      <td style="text-align: center">522 ± 34</td>
      <td style="text-align: center">703 ± 60</td>
    </tr>
    <tr>
      <td>Qbert</td>
      <td style="text-align: center">18777 ± 778</td>
      <td style="text-align: center">21272 ± 655</td>
    </tr>
    <tr>
      <td>SpaceInvaders</td>
      <td style="text-align: center">7727 ± 1121</td>
      <td style="text-align: center">21676 ± 8897</td>
    </tr>
    <tr>
      <td>Seaquest</td>
      <td style="text-align: center">1779 ± 4</td>
      <td style="text-align: center">1795 ± 4</td>
    </tr>
    <tr>
      <td>CrazyClimber</td>
      <td style="text-align: center">147030 ± 10239</td>
      <td style="text-align: center">139752 ± 11618</td>
    </tr>
    <tr>
      <td>BeamRider</td>
      <td style="text-align: center">9999 ± 402</td>
      <td style="text-align: center">11321 ± 1065</td>
    </tr>
  </tbody>
</table>

<h2 id="low-precision-training">Low Precision Training</h2>
<p>We can filter through quantization noise by combining weights that have been rounded down with weights that have been rounded up. Moreover, by averaging weights to find a flat region of the loss surface, large perturbations of the weights will not affect the quality of the solution (Figures 7 and 8). Recent work shows that by adapting SWA to the low precision setting, in a method called SWALP, one can <em>match the performance of full-precision SGD even with all training in 8 bits</em> [5]. This is quite a practically important result, given that (1) SGD training in 8 bits performs notably worse than full precision SGD, and (2) low precision training is significantly harder than predictions in low precision after training (the usual setting). For example, a ResNet-164 trained on CIFAR-100 with float (16-bit) SGD achieves 22.2% error, while 8-bit SGD achieves 24.0% error. By contrast, SWALP with 8 bit training achieves 21.8% error.</p>
<div class="text-center">
<img src="https://pytorch.org/assets/images/swa/Figure7.png" width="90%" />
</div>

<p><strong>Figure 7.</strong> Quantizing in a flat region can still provide solutions with low loss.</p>

<div class="text-center">
<img src="https://pytorch.org/assets/images/swa/Figure8.png" width="90%" />
</div>

<p><strong>Figure 8.</strong> Low precision SGD training (with a modified learning rate schedule) and SWALP.</p>

<h2 id="conclusion">Conclusion</h2>

<p>One of the greatest open questions in deep learning is why SGD manages to find good solutions, given that the training objectives are highly multimodal, and there are in principle many settings of parameters that achieve no training loss but poor generalization. By understanding geometric features such as flatness, which relate to generalization, we can begin to resolve these questions and build optimizers that provide even better generalization, and many other useful features, such as uncertainty representation. We have presented SWA, a simple drop-in replacement for standard SGD, which can in principle benefit anyone training a deep neural network. SWA has been demonstrated to have strong performance in a number of areas, including computer vision, semi-supervised learning, reinforcement learning, uncertainty representation, calibration, Bayesian model averaging, and low precision training.</p>

<p>We encourage you try out SWA! Using SWA is now as easy as using any other optimizer in PyTorch. And even if you have already trained your model with SGD (or any other optimizer), it’s very easy to realize the benefits of SWA by running SWA for a small number of epochs starting with a pre-trained model.</p>

<ul>
  <li>[1] Averaging Weights Leads to Wider Optima and Better Generalization; Pavel Izmailov, Dmitry Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson; Uncertainty in Artificial Intelligence (UAI), 2018</li>
  <li>[2] There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average; Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, Andrew Gordon Wilson; International Conference on Learning Representations (ICLR), 2019</li>
  <li>[3] Improving Stability in Deep Reinforcement Learning with Weight Averaging; Evgenii Nikishin, Pavel Izmailov, Ben Athiwaratkun, Dmitrii Podoprikhin, Timur Garipov, Pavel Shvechikov, Dmitry Vetrov, Andrew Gordon Wilson, UAI 2018 Workshop: Uncertainty in Deep Learning, 2018</li>
  <li>[4]  A Simple Baseline for Bayesian Uncertainty in Deep Learning, Wesley Maddox, Timur Garipov, Pavel Izmailov, Andrew Gordon Wilson, arXiv pre-print, 2019: <a href="https://arxiv.org/abs/1902.02476">https://arxiv.org/abs/1902.02476</a></li>
  <li>[5] SWALP : Stochastic Weight Averaging in Low Precision Training, Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew Gordon Wilson, Christopher De Sa, To appear at the International Conference on Machine Learning  (ICML), 2019.</li>
  <li>[6] David Ruppert. Efficient estimations from a slowly convergent Robbins-Monro process. Technical report, Cornell University Operations Research and Industrial Engineering, 1988.</li>
  <li>[7] Acceleration of stochastic approximation by averaging. Boris T Polyak and Anatoli B Juditsky. SIAM Journal on Control and Optimization, 30(4):838–855, 1992.</li>
  <li>[8] Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs, Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, Andrew Gordon Wilson. Neural Information Processing Systems (NeurIPS), 2018</li>
</ul>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
        </ul>
      </div>

      <div class="footer-links-col follow-us-col">
        <ul>
          <li class="list-title">Stay Connected</li>
          <li>
            
            <div id="mc_embed_signup">
  <form
    action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
    method="post"
    id="mc-embedded-subscribe-form"
    name="mc-embedded-subscribe-form"
    class="email-subscribe-form validate"
    target="_blank"
    novalidate>
    <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
      <div class="mc-field-group">
        <label for="mce-EMAIL" style="display:none;">Email Address</label>
        <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
      </div>

      <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
      </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

      <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

      <div class="clear">
        <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
      </div>
    </div>
  </form>
</div>

          </li>
        </ul>

        <div class="footer-social-icons">
          <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
          <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
        </div>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <li class="">
          <a href="/features">Features</a>
        </li>

        <li class="">
          <a href="/ecosystem">Ecosystem</a>
        </li>

        <li class="">
          <a href="/mobile">Mobile</a>
        </li>

        <li class="">
          <a href="/hub">PyTorch Hub</a>
        </li>

        <li class="active">
          <a href="/blog">Blog</a>
        </li>

        <li>
          <a href="https://pytorch.org/tutorials">Tutorials</a>
        </li>

        <li>
          <a href="/docs">Docs</a>
        </li>

        <li class="">
          <a href="/resources">Resources</a>
        </li>

        <li id="github-mobile-menu-link">
          <a href="https://github.com/pytorch/pytorch">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top === 0) {
        $(".header-holder").css({"backgroundColor": "transparent"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>



</body>

</html>

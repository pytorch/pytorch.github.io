<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      How to Train State-Of-The-Art Models Using TorchVision’s Latest Primitives | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="

" />

  <meta property="og:image" content="https://pytorch.org/assets/images/fx-image2.png" />
  <meta name="twitter:image" content="https://pytorch.org/assets/images/fx-image2.png" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="How to Train State-Of-The-Art Models Using TorchVision’s Latest Primitives" />
<meta property="og:description" content="

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="How to Train State-Of-The-Art Models Using TorchVision’s Latest Primitives" />
<meta name="twitter:description" content="

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>
    <div class="hello-bar">
        <div class="container">
          Join us in Silicon Valley September 18-19 at the 2024 PyTorch Conference. <a target="_blank" href="https://events.linuxfoundation.org/pytorch-conference/">Learn more</a>.
        </div>
      </div>
      
    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Learn
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/get-started">
            <span class=dropdown-title>Get Started</span>
            <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/">
            <span class="dropdown-title">Tutorials</span>
            <p>Whats new in PyTorch tutorials</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
            <span class="dropdown-title">Learn the Basics</span>
            <p>Familiarize yourself with PyTorch concepts and modules</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
            <span class="dropdown-title">PyTorch Recipes</span>
            <p>Bite-size, ready-to-deploy PyTorch code examples</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
            <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
            <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/contributor-awards-2024">
            <span class="dropdown-title">Contributor Awards - 2024</span>
            <p>Award winners announced at this year's PyTorch Conference</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
            <p>Build innovative and privacy-aware AI experiences for edge devices</p>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
            <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
          </a>
          <a class="nav-dropdown-item" target="_blank" href="https://pytorch.org/executorch/stable/index.html">
            <span class="dropdown-title">ExecuTorch Documentation</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://pytorch.org/docs">
            <span class="dropdown-title">PyTorch</span>
            <p>Explore the documentation for comprehensive guidance on how to use PyTorch.</p>
          </a>
          <a class="nav-dropdown-item" href="/pytorch-domains">
            <span class="dropdown-title">PyTorch Domains</span>
            <p> Read the PyTorch Domains documentation to learn more about domain-specific libraries.</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Blog & News
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/blog">
            <span class="dropdown-title">PyTorch Blog</span>
            <p>Catch up on the latest technical news and happenings</p>
          </a>
          <a class="nav-dropdown-item" href="/community-blog">
            <span class="dropdown-title">Community Blog</span>
            <p>Stories from the PyTorch ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/videos">
            <span class="dropdown-title">Videos</span>
            <p>Learn about the latest PyTorch tutorials, new, and more </p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class="dropdown-title">Community Stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
          <a class="nav-dropdown-item" href="/newsletter">
            <span class=dropdown-title>Newsletter</span>
            <p>Stay up-to-date with the latest updates</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          About
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/governing-board">
            <span class=dropdown-title>Governing Board</span>
          </a>
          <a class="nav-dropdown-item" href="/credits">
            <span class=dropdown-title>Cloud Credit Program</span>
          </a>
          <a class="nav-dropdown-item" href="/tac">
            <span class=dropdown-title>Technical Advisory Council</span>
          </a>
          <a class="nav-dropdown-item" href="/staff">
            <span class=dropdown-title>Staff</span>
          </a>
          <a class="nav-dropdown-item" href="/contact-us">
            <span class=dropdown-title>Contact Us</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <a href="/join" data-cta="join">
        Become a Member
      </a>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub">
        <div id="topnav-gh-icon"></div>
      </a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">November 18, 2021</p>
            <h1>
                <a class="blog-title">How to Train State-Of-The-Art Models Using TorchVision’s Latest Primitives</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Vasilis Vryniotis
                      
                    </p>
                    <style type="text/css">
article.pytorch-article table tr th, article.pytorch-article table td {line-height: 1.5rem}
</style>

<p>A few weeks ago, TorchVision v0.11 was released packed with numerous new primitives, models and training recipe improvements which allowed achieving state-of-the-art (SOTA) results. The project was dubbed “<a href="https://github.com/pytorch/vision/issues/3911">TorchVision with Batteries Included</a>” and aimed to modernize our library. We wanted to enable researchers to reproduce papers and conduct research more easily by using common building blocks. Moreover, we aspired to provide the necessary tools to Applied ML practitioners to train their models on their own data using the same SOTA techniques as in research. Finally, we wanted to refresh our pre-trained weights and offer better off-the-shelf models to our users, hoping that they would build better applications.</p>

<p>Though there is still much work to be done, we wanted to share with you some exciting results from the above work. We will showcase how one can use the new tools included in TorchVision to achieve state-of-the-art results on a highly competitive and well-studied architecture such as ResNet50 <a href="https://arxiv.org/abs/1512.03385">[1]</a>. We will share the exact recipe used to improve our baseline by over 4.7 accuracy points to reach a final top-1 accuracy of 80.9% and share the journey for deriving the new training process. Moreover, we will show that this recipe generalizes well to other model variants and families. We hope that the above will influence future research for developing stronger generalizable training methodologies and will inspire the community to adopt and contribute to our efforts.</p>

<h2 id="the-results">The Results</h2>

<p>Using our new training recipe found on ResNet50, we’ve refreshed the pre-trained weights of the following models:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th style="text-align: center">Accuracy@1</th>
      <th style="text-align: center">Accuracy@5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ResNet50</td>
      <td style="text-align: center">80.858</td>
      <td style="text-align: center">95.434</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>ResNet101</td>
      <td style="text-align: center">81.886</td>
      <td style="text-align: center">95.780</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>ResNet152</td>
      <td style="text-align: center">82.284</td>
      <td style="text-align: center">96.002</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>ResNeXt50-32x4d</td>
      <td style="text-align: center">81.198</td>
      <td style="text-align: center">95.340</td>
    </tr>
  </tbody>
</table>

<p>Note that the accuracy of all models except RetNet50 can be further improved by adjusting their training parameters slightly, but our focus was to have a single robust recipe which performs well for all.</p>

<p><strong>UPDATE:</strong> We have refreshed the majority of popular classification models of TorchVision, you can find the details on this <a href="https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/">blog post</a>.</p>

<p>There are currently two ways to use the latest weights of the model.</p>

<h2 id="using-the-multi-pretrained-weight-api">Using the Multi-pretrained weight API</h2>

<p>We are currently working on a new prototype mechanism which will extend the model builder methods of TorchVision to <a href="https://github.com/pytorch/vision/issues/4611">support multiple weights</a>. Along with the weights, we store useful <a href="https://github.com/pytorch/vision/blob/c5fb79f8fad60511c89957c4970cc2a5cfc8432e/torchvision/prototype/models/resnet.py#L94-L103">meta-data</a> (such as the labels, the accuracy, links to recipe etc) and the preprocessing transforms necessary for using the models. Example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
  <span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">prototype</span> <span class="k">as</span> <span class="n">P</span>
  <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">"test/assets/encode_jpeg/grace_hopper_517x606.jpg"</span><span class="p">)</span>
  <span class="err"> </span>
  <span class="c1"># Initialize model
</span>  <span class="n">weights</span> <span class="o">=</span> <span class="n">P</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">ResNet50_Weights</span><span class="p">.</span><span class="n">IMAGENET1K_V2</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">P</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
  <span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>

  <span class="c1"># Initialize inference transforms
</span>  <span class="n">preprocess</span> <span class="o">=</span> <span class="n">weights</span><span class="p">.</span><span class="n">transforms</span><span class="p">()</span>
  <span class="err"> </span>
  <span class="c1"># Apply inference preprocessing transforms
</span>  <span class="n">batch</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">).</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">softmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
  <span class="err"> </span>
  <span class="c1"># Make predictions
</span>  <span class="n">label</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">.</span><span class="n">argmax</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
  <span class="n">score</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="n">label</span><span class="p">].</span><span class="n">item</span><span class="p">()</span>
  <span class="err"> </span>
  <span class="c1"># Use meta to get the labels
</span>  <span class="n">category_name</span> <span class="o">=</span> <span class="n">weights</span><span class="p">.</span><span class="n">meta</span><span class="p">[</span><span class="s">'categories'</span><span class="p">][</span><span class="n">label</span><span class="p">]</span>
  <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">category_name</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">score</span><span class="si">}</span><span class="s">%"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="using-the-legacy-api">Using the legacy API</h2>

<p>Those who don’t want to use a prototype API have the option of accessing the new weights via the legacy API using the following approach:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="n">resnet</span>
  <span class="err"> </span>
  <span class="c1"># Overwrite the URL of the previous weights
</span>  <span class="n">resnet</span><span class="p">.</span><span class="n">model_urls</span><span class="p">[</span><span class="s">"resnet50"</span><span class="p">]</span> <span class="o">=</span> <span class="s">"https://download.pytorch.org/models/resnet50-11ad3fa6.pth"</span>
  <span class="err"> </span>
  <span class="c1"># Initialize the model using the legacy API
</span>  <span class="n">model</span> <span class="o">=</span> <span class="n">resnet</span><span class="p">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="err"> </span>
  <span class="c1"># TODO: Apply preprocessing + call the model
</span>  <span class="c1"># ...
</span></code></pre></div></div>

<h2 id="the-training-recipe">The Training Recipe</h2>

<p>Our goal was to use the newly introduced primitives of TorchVision to derive a new strong training recipe which achieves state-of-the-art results for the vanilla ResNet50 architecture when trained from scratch on ImageNet with no additional external data. Though by using architecture specific tricks <a href="https://arxiv.org/abs/1812.01187">[2]</a> one could further improve the accuracy, we’ve decided not to include them so that the recipe can be used in other architectures. Our recipe heavily focuses on simplicity and builds upon work by FAIR <a href="https://arxiv.org/abs/2103.06877">[3]</a>, <a href="https://arxiv.org/abs/2106.14881">[4]</a>, <a href="https://arxiv.org/abs/1906.06423">[5]</a>, <a href="https://arxiv.org/abs/2012.12877">[6]</a>, <a href="https://arxiv.org/abs/2110.00476">[7]</a>. Our findings align with the parallel study of Wightman et al. <a href="https://arxiv.org/abs/2110.00476">[7]</a>, who also report major accuracy improvements by focusing on the training recipes.</p>

<p>Without further ado, here are the main parameters of our recipe:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># Optimizer &amp; LR scheme
</span>  <span class="n">ngpus</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
  <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="err"> </span> <span class="c1"># per GPU
</span>
  <span class="n">epochs</span><span class="o">=</span><span class="mi">600</span><span class="p">,</span> 
  <span class="n">opt</span><span class="o">=</span><span class="s">'sgd'</span><span class="p">,</span> <span class="err"> </span>
  <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>

  <span class="n">lr</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> 
  <span class="n">lr_scheduler</span><span class="o">=</span><span class="s">'cosineannealinglr'</span><span class="p">,</span> 
  <span class="n">lr_warmup_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
  <span class="n">lr_warmup_method</span><span class="o">=</span><span class="s">'linear'</span><span class="p">,</span> 
  <span class="n">lr_warmup_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> 


  <span class="c1"># Regularization and Augmentation
</span>  <span class="n">weight_decay</span><span class="o">=</span><span class="mf">2e-05</span><span class="p">,</span> 
  <span class="n">norm_weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>

  <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
  <span class="n">mixup_alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> 
  <span class="n">cutmix_alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
  <span class="n">auto_augment</span><span class="o">=</span><span class="s">'ta_wide'</span><span class="p">,</span> 
  <span class="n">random_erase</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
  
  <span class="n">ra_sampler</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
  <span class="n">ra_reps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>


  <span class="c1"># EMA configuration
</span>  <span class="n">model_ema</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
  <span class="n">model_ema_steps</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> 
  <span class="n">model_ema_decay</span><span class="o">=</span><span class="mf">0.99998</span><span class="p">,</span> 


  <span class="c1"># Resizing
</span>  <span class="n">interpolation</span><span class="o">=</span><span class="s">'bilinear'</span><span class="p">,</span> 
  <span class="n">val_resize_size</span><span class="o">=</span><span class="mi">232</span><span class="p">,</span> 
  <span class="n">val_crop_size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> 
  <span class="n">train_crop_size</span><span class="o">=</span><span class="mi">176</span><span class="p">,</span>
</code></pre></div></div>

<p>Using our standard <a href="https://github.com/pytorch/vision/tree/main/references/classification">training reference script</a>, we can train a ResNet50 using the following command:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torchrun --nproc_per_node=8 train.py --model resnet50 --batch-size 128 --lr 0.5 \
--lr-scheduler cosineannealinglr --lr-warmup-epochs 5 --lr-warmup-method linear \
--auto-augment ta_wide --epochs 600 --random-erase 0.1 --weight-decay 0.00002 \
--norm-weight-decay 0.0 --label-smoothing 0.1 --mixup-alpha 0.2 --cutmix-alpha 1.0 \
--train-crop-size 176 --model-ema --val-resize-size 232 --ra-sampler --ra-reps 4
</code></pre></div></div>

<h2 id="methodology">Methodology</h2>

<p>There are a few principles we kept in mind during our explorations:</p>

<ol>
  <li>Training is a stochastic process and the validation metric we try to optimize is a random variable. This is due to the random weight initialization scheme employed and the existence of random effects during the training process. This means that we can’t do a single run to assess the effect of a recipe change. The standard practice is doing multiple runs (usually 3 to 5) and studying the summarization stats (such as mean, std, median, max, etc).</li>
  <li>There is usually a significant interaction between different parameters, especially for techniques that focus on Regularization and reducing overfitting. Thus changing the value of one can have effects on the optimal configurations of others. To account for that one can either adopt a greedy search approach (which often leads to suboptimal results but tractable experiments) or apply grid search (which leads to better results but is computationally expensive). In this work, we used a mixture of both.</li>
  <li>Techniques that are non-deterministic or introduce noise usually require longer training cycles to improve model performance. To keep things tractable, we initially used short training cycles (small number of epochs) to decide which paths can be eliminated early and which should be explored using longer training.</li>
  <li>There is a risk of overfitting the validation dataset <a href="https://arxiv.org/abs/1902.10811">[8]</a> because of the repeated experiments. To mitigate some of the risk, we apply only training optimizations that provide a significant accuracy improvements and use K-fold cross validation to verify optimizations done on the validation set. Moreover we confirm that our recipe ingredients generalize well on other models for which we didn’t optimize the hyper-parameters.</li>
</ol>

<h2 id="break-down-of-key-accuracy-improvements">Break down of key accuracy improvements</h2>

<p>As discussed in <a href="https://pytorch.org/blog/torchvision-ssdlite-implementation/#break-down-of-key-accuracy-improvements">earlier blogposts</a>, training models is not a journey of monotonically increasing accuracies and the process involves a lot of backtracking. To quantify the effect of each optimization, below we attempt to show-case an idealized linear journey of deriving the final recipe starting from the original recipe of TorchVision. We would like to clarify that this is an oversimplification of the actual path we followed and thus it should be taken with a grain of salt. </p>

<p align="center">
<img src="/assets/images/sota/Cumulative Accuracy Improvements for ResNet50.png" alt="Cumulative Accuracy Improvements for ResNet50" width="100%" />
</p>

<p>In the table below, we provide a summary of the performance of stacked incremental improvements on top of Baseline. Unless denoted otherwise, we report the model with best Acc@1 out of 3 runs:</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">Accuracy@1</th>
      <th style="text-align: center">Accuracy@5</th>
      <th style="text-align: left">Incremental Diff</th>
      <th style="text-align: center">Absolute Diff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ResNet50 Baseline</td>
      <td style="text-align: center">76.130</td>
      <td style="text-align: center">92.862</td>
      <td style="text-align: left">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>+ LR optimizations</td>
      <td style="text-align: center">76.494</td>
      <td style="text-align: center">93.198</td>
      <td style="text-align: left">0.364</td>
      <td style="text-align: center">0.364</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>+ TrivialAugment</td>
      <td style="text-align: center">76.806</td>
      <td style="text-align: center">93.272</td>
      <td style="text-align: left">0.312</td>
      <td style="text-align: center">0.676</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>+ Long Training</td>
      <td style="text-align: center">78.606</td>
      <td style="text-align: center">94.052</td>
      <td style="text-align: left">1.800</td>
      <td style="text-align: center">2.476</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>+ Random Erasing</td>
      <td style="text-align: center">78.796</td>
      <td style="text-align: center">94.094</td>
      <td style="text-align: left">0.190</td>
      <td style="text-align: center">2.666</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>+ Label Smoothing</td>
      <td style="text-align: center">79.114</td>
      <td style="text-align: center">94.374</td>
      <td style="text-align: left">0.318</td>
      <td style="text-align: center">2.984</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>+ Mixup</td>
      <td style="text-align: center">79.232</td>
      <td style="text-align: center">94.536</td>
      <td style="text-align: left">0.118</td>
      <td style="text-align: center">3.102</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>+ Cutmix</td>
      <td style="text-align: center">79.510</td>
      <td style="text-align: center">94.642</td>
      <td style="text-align: left">0.278</td>
      <td style="text-align: center">3.380</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>+ Weight Decay tuning</td>
      <td style="text-align: center">80.036</td>
      <td style="text-align: center">94.746</td>
      <td style="text-align: left">0.526</td>
      <td style="text-align: center">3.906</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>+ FixRes mitigations</td>
      <td style="text-align: center">80.196</td>
      <td style="text-align: center">94.672</td>
      <td style="text-align: left">0.160</td>
      <td style="text-align: center">4.066</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>+ EMA</td>
      <td style="text-align: center">80.450</td>
      <td style="text-align: center">94.908</td>
      <td style="text-align: left">0.254</td>
      <td style="text-align: center">4.320</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>+ Inference Resize tuning *</td>
      <td style="text-align: center">80.674</td>
      <td style="text-align: center">95.166</td>
      <td style="text-align: left">0.224</td>
      <td style="text-align: center">4.544</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>+ Repeated Augmentation **</td>
      <td style="text-align: center">80.858</td>
      <td style="text-align: center">95.434</td>
      <td style="text-align: left">0.184</td>
      <td style="text-align: center">4.728</td>
    </tr>
  </tbody>
</table>

<p>*The tuning of the inference size was done on top of the last model. See below for details.</p>

<p>** Community contribution done after the release of the article. See below for details.</p>

<h2 id="baseline">Baseline</h2>

<p>Our baseline is the previously released ResNet50 model of TorchVision. It was trained with the following recipe:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># Optimizer &amp; LR scheme
</span>  <span class="n">ngpus</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
  <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="err"> </span> <span class="c1"># per GPU
</span>
  <span class="n">epochs</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> 
  <span class="n">opt</span><span class="o">=</span><span class="s">'sgd'</span><span class="p">,</span> <span class="err"> </span>
  <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>

  <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
  <span class="n">lr_scheduler</span><span class="o">=</span><span class="s">'steplr'</span><span class="p">,</span> 
  <span class="n">lr_step_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> 
  <span class="n">lr_gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 


  <span class="c1"># Regularization
</span>  <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>


  <span class="c1"># Resizing
</span>  <span class="n">interpolation</span><span class="o">=</span><span class="s">'bilinear'</span><span class="p">,</span> 
  <span class="n">val_resize_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> 
  <span class="n">val_crop_size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> 
  <span class="n">train_crop_size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span>
</code></pre></div></div>

<p>Most of the above parameters are the defaults on our <a href="https://github.com/pytorch/vision/tree/main/references/classification">training scripts</a>. We will start building on top of this baseline by introducing optimizations until we gradually arrive at the final recipe.</p>

<h2 id="lr-optimizations">LR optimizations</h2>

<p>There are a few parameter updates we can apply to improve both the accuracy and the speed of our training. This can be achieved by increasing the batch size and tuning the LR. Another common method is to apply warmup and gradually increase our learning rate. This is beneficial especially when we use very high learning rates and helps with the stability of the training in the early epochs. Finally, another optimization is to apply Cosine Schedule to adjust our LR during the epochs. A big advantage of cosine is that there are no hyper-parameters to optimize, which cuts down our search space.</p>

<p>Here are the additional optimizations applied on top of the baseline recipe. Note that we’ve run multiple experiments to determine the optimal configuration of the parameters:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="err"> </span> <span class="c1"># per GPU
</span>
  <span class="n">lr</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> 
  <span class="n">lr_scheduler</span><span class="o">=</span><span class="s">'cosineannealinglr'</span><span class="p">,</span> 
  <span class="n">lr_warmup_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
  <span class="n">lr_warmup_method</span><span class="o">=</span><span class="s">'linear'</span><span class="p">,</span> 
  <span class="n">lr_warmup_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
</code></pre></div></div>

<p>The above optimizations increase our top-1 Accuracy by 0.364 points comparing to the baseline. Note that in order to combine the different LR strategies we use the newly introduced <a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR">SequentialLR</a> scheduler.</p>

<h2 id="trivialaugment">TrivialAugment</h2>

<p>The original model was trained using basic augmentation transforms such as Random resized crops and horizontal flips. An easy way to improve our accuracy is to apply more complex “Automatic-Augmentation” techniques. The one that performed best for us is TrivialAugment <a href="https://arxiv.org/abs/2103.10158">[9]</a>, which is extremely simple and can be considered “parameter free”, which means it can help us cut down our search space further.</p>

<p>Here is the update applied on top of the previous step:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>auto_augment='ta_wide',
</code></pre></div></div>

<p>The use of TrivialAugment increased our top-1 Accuracy by 0.312 points compared to the previous step.</p>

<h2 id="long-training">Long Training</h2>

<p>Longer training cycles are beneficial when our recipe contains ingredients that behave randomly. More specifically as we start adding more and more techniques that introduce noise, increasing the number of epochs becomes crucial. Note that at early stages of our exploration, we used relatively short cycles of roughly 200 epochs which was later increased to 400 as we started narrowing down most of the parameters and finally increased to 600 epochs at the final versions of the recipe.</p>

<p>Below we see the update applied on top of the earlier steps:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>epochs=600,
</code></pre></div></div>

<p>This further increases our top-1 Accuracy by 1.8 points on top of the previous step. This is the biggest increase we will observe in this iterative process. It’s worth noting that the effect of this single optimization is overstated and somehow misleading. Just increasing the number of epochs on top of the old baseline won’t yield such significant improvements. Nevertheless the combination of the LR optimizations with strong Augmentation strategies helps the model benefit from longer cycles. It’s also worth mentioning that the reason we introduce the lengthy training cycles so early in the process is because in the next steps we will introduce techniques that require significantly more epochs to provide good results.</p>

<h2 id="random-erasing">Random Erasing</h2>

<p>Another data augmentation technique known to help the classification accuracy is Random Erasing <a href="https://arxiv.org/abs/1708.04896">[10]</a>, <a href="https://arxiv.org/abs/1708.04552">[11]</a>. Often paired with Automatic Augmentation methods, it usually yields additional improvements in accuracy due to its regularization effect. In our experiments we tuned only the probability of applying the method via a grid search and found that it’s beneficial to keep its probability at low levels, typically around 10%. </p>

<p>Here is the extra parameter introduced on top of the previous:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>random_erase=0.1,
</code></pre></div></div>

<p>Applying Random Erasing increases our Acc@1 by further 0.190 points.</p>

<h2 id="label-smoothing">Label Smoothing</h2>

<p>A good technique to reduce overfitting is to stop the model from becoming overconfident. This can be achieved by softening the ground truth using Label Smoothing <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf">[12]</a>. There is a single parameter which controls the degree of smoothing (the higher the stronger) that we need to specify. Though optimizing it via grid search is possible, we found that values around 0.05-0.15 yield similar results, so to avoid overfitting it we used the same value as on the paper that introduced it.</p>

<p>Below we can find the extra config added on this step:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>label_smoothing=0.1,
</code></pre></div></div>

<p>We use PyTorch’s newly introduced <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=label_smoothing">CrossEntropyLoss</a> label_smoothing parameter and that increases our accuracy by an additional 0.318 points.</p>

<h2 id="mixup-and-cutmix">Mixup and Cutmix</h2>

<p>Two data augmentation techniques often used to produce SOTA results are Mixup and Cutmix <a href="https://arxiv.org/abs/1710.09412">[13]</a>, <a href="https://arxiv.org/abs/1905.04899">[14]</a>. They both provide strong regularization effects by softening not only the labels but also the images. In our setup we found it beneficial to apply one of them randomly with equal probability. Each is parameterized with a hyperparameter alpha, which controls the shape of the Beta distribution from which the smoothing probability is sampled. We did a very limited grid search, focusing primarily on common values proposed on the papers. </p>

<p>Below you will find the optimal values for the alpha parameters of the two techniques:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mixup_alpha=0.2, 
cutmix_alpha=1.0,
</code></pre></div></div>

<p>Applying mixup increases our accuracy by 0.118 points and combining it with cutmix improves it by additional 0.278 points.</p>

<h2 id="weight-decay-tuning">Weight Decay tuning</h2>

<p>Our standard recipe uses L2 regularization to reduce overfitting. The Weight Decay parameter controls the degree of the regularization (the larger the stronger) and is applied universally to all learned parameters of the model by default. In this recipe, we apply two optimizations to the standard approach. First we perform grid search to tune the parameter of weight decay and second we disable weight decay for the parameters of the normalization layers. </p>

<p>Below you can find the optimal configuration of weight decay for our recipe:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>weight_decay=2e-05, 
norm_weight_decay=0.0,
</code></pre></div></div>

<p>The above update improves our accuracy by a further 0.526 points, providing additional experimental evidence for a known fact that tuning weight decay has significant effects on the performance of the model. Our approach for separating the Normalization parameters from the rest was inspired by <a href="https://github.com/facebookresearch/ClassyVision">ClassyVision’s</a> approach.</p>

<h2 id="fixres-mitigations">FixRes mitigations</h2>

<p>An important property identified early in our experiments is the fact that the models performed significantly better if the resolution used during validation was increased from the 224x224 of training. This effect is studied in detail on the FixRes paper <a href="https://arxiv.org/abs/1906.06423">[5]</a> and two mitigations are proposed: a) one could try to reduce the training resolution so that the accuracy on the validation resolution is maximized or b) one could fine-tune the model on a two-phase training so that it adjusts on the target resolution. Since we didn’t want to introduce a 2-phase training, we went for option a). This means that we reduced the train crop size from 224 and used grid search to find the one that maximizes the validation on resolution of 224x224.</p>

<p>Below you can see the optimal value used on our recipe:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val_crop_size=224, 
train_crop_size=176,
</code></pre></div></div>

<p>The above optimization improved our accuracy by an additional 0.160 points and sped up our training by 10%. </p>

<p>It’s worth noting that the FixRes effect still persists, meaning that the model continues to perform better on validation when we increase the resolution. Moreover, further reducing the training crop-size actually hurts the accuracy. This intuitively makes sense because one can only reduce the resolution so much before critical details start disappearing from the picture. Finally, we should note that the above FixRes mitigation seems to benefit models with similar depth to ResNet50. Deeper variants with larger receptive fields seem to be slightly negatively affected (typically by 0.1-0.2 points). Hence we consider this part of the recipe optional. Below we visualize the performance of the best available checkpoints (with the full recipe) for models trained with 176 and 224 resolution:</p>

<div style="display: flex">
<img src="/assets/images/sota/Best ResNet50 trained with 176 Resolution.png" alt="Best ResNet50 trained with 176 Resolution" width="50%" />
<img src="/assets/images/sota/Best ResNet50 trained with 224 Resolution.png" alt="Best ResNet50 trained with 224 Resolution" width="50%" />
</div>

<h2 id="exponential-moving-average-ema">Exponential Moving Average (EMA)</h2>

<p>EMA is a technique that allows one to push the accuracy of a model without increasing its complexity or inference time. It performs an exponential moving average on the model weights and this leads to increased accuracy and more stable models. The averaging happens every few iterations and its decay parameter was tuned via grid search. </p>

<p>Below you can see the optimal values for our recipe:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model_ema=True, 
model_ema_steps=32, 
model_ema_decay=0.99998,
</code></pre></div></div>

<p>The use of EMA increases our accuracy by 0.254 points comparing to the previous step. Note that TorchVision’s <a href="https://github.com/pytorch/vision/pull/4406">EMA implementation</a> is build on top of PyTorch’s <a href="https://pytorch.org/docs/stable/optim.html#stochastic-weight-averaging">AveragedModel</a> class with the key difference being that it averages not only the model parameters but also its buffers. Moreover, we have adopted tricks from <a href="https://github.com/facebookresearch/pycls/tree/main/pycls">Pycls</a> which allow us to parameterize the decay in a way that doesn’t depend on the number of epochs.</p>

<h2 id="inference-resize-tuning">Inference Resize tuning</h2>

<p>Unlike all other steps of the process which involved training models with different parameters, this optimization was done on top of the final model. During inference, the image is resized to a specific resolution and then a central 224x224 crop is taken from it. The original recipe used a resize size of 256, which caused a similar discrepancy as the one described on the FixRes paper <a href="https://arxiv.org/abs/1906.06423">[5]</a>. By bringing this resize value closer to the target inference resolution, one can improve the accuracy. To select the value we run a short grid search between interval [224, 256] with step of 8. To avoid overfitting, the value was selected using half of the validation set and confirmed using the other half.</p>

<p>Below you can see the optimal value used on our recipe:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val_resize_size=232,
</code></pre></div></div>

<p>The above is an optimization which improved our accuracy by 0.224 points. It’s worth noting that the optimal value for ResNet50 works also best for ResNet101, ResNet152 and ResNeXt50, which hints that it generalizes across models:</p>

<div style="display: flex">
<img src="/assets/images/sota/ResNet50 Inference Resize.png" alt="ResNet50 Inference Resize" width="30%" />
<img src="/assets/images/sota/ResNet101 Inference Resize.png" alt="ResNet101 Inference Resize" width="30%" />
<img src="/assets/images/sota/ResNet152 Inference Resize.png" alt="Best ResNet50 trained with 224 Resolution" width="30%" />
</div>

<h2 id="update-repeated-augmentation">[UPDATE] Repeated Augmentation</h2>

<p>Repeated Augmentation <a href="https://arxiv.org/abs/1901.09335">[15]</a>, <a href="https://arxiv.org/abs/1902.05509">[16]</a> is another technique which can improve the overall accuracy and has been used by other strong recipes such as those at <a href="https://arxiv.org/abs/2012.12877">[6]</a>, <a href="https://arxiv.org/abs/2110.00476">[7]</a>. Tal Ben-Nun, a community contributor, has <a href="https://github.com/pytorch/vision/pull/5201">further improved</a> upon our original recipe by proposing training the model with 4 repetitions. His contribution came after the release of this article.</p>

<p>Below you can see the optimal value used on our recipe:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ra_sampler=True,
ra_reps=4,
</code></pre></div></div>

<p>The above is the final optimization which improved our accuracy by 0.184 points. </p>

<h2 id="optimizations-that-were-tested-but-not-adopted">Optimizations that were tested but not adopted</h2>

<p>During the early stages of our research, we experimented with additional techniques, configurations and optimizations. Since our target was to keep our recipe as simple as possible, we decided not to include anything that didn’t provide a significant improvement. Here are a few approaches that we took but didn’t make it to our final recipe:</p>

<ul>
  <li><strong>Optimizers:</strong> Using more complex optimizers such as Adam, RMSProp or SGD with Nesterov momentum didn’t provide significantly better results than vanilla SGD with momentum.</li>
  <li><strong>LR Schedulers:</strong> We tried different LR Scheduler schemes such as StepLR and Exponential. Though the latter tends to work better with EMA, it often requires additional hyper-parameters such as defining the minimum LR to work well. Instead, we just use cosine annealing decaying the LR up to zero and choose the checkpoint with the highest accuracy.</li>
  <li><strong>Automatic Augmentations:</strong> We’ve tried different augmentation strategies such as AutoAugment and RandAugment. None of these outperformed the simpler parameter-free TrivialAugment.</li>
  <li><strong>Interpolation:</strong> Using bicubic or nearest interpolation didn’t provide significantly better results than bilinear.</li>
  <li><strong>Normalization layers:</strong> Using Sync Batch Norm didn’t yield significantly better results than using the regular Batch Norm.</li>
</ul>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>We would like to thank Piotr Dollar, Mannat Singh and Hugo Touvron for providing their insights and feedback during the development of the recipe and for their previous research work on which our recipe is based on. Their support was invaluable for achieving the above result. Moreover, we would like to thank Prabhat Roy, Kai Zhang, Yiwen Song, Joel Schlosser, Ilqar Ramazanli, Francisco Massa, Mannat Singh, Xiaoliang Dai, Samuel Gabriel, Allen Goodman and Tal Ben-Nun for their contributions to the Batteries Included project.</p>

<h2 id="references">References</h2>

<ol>
  <li>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. “Deep Residual Learning for Image Recognition”.</li>
  <li>Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, Mu Li. “Bag of Tricks for Image Classification with Convolutional Neural Networks”</li>
  <li>Piotr Dollár, Mannat Singh, Ross Girshick. “Fast and Accurate Model Scaling”</li>
  <li>Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollár, Ross Girshick. “Early Convolutions Help Transformers See Better”</li>
  <li>Hugo Touvron, Andrea Vedaldi, Matthijs Douze, Hervé Jégou. “Fixing the train-test resolution discrepancy</li>
  <li>Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou. “Training data-efficient image transformers &amp; distillation through attention”</li>
  <li>Ross Wightman, Hugo Touvron, Hervé Jégou. “ResNet strikes back: An improved training procedure in timm”</li>
  <li>Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar. “Do ImageNet Classifiers Generalize to ImageNet?”</li>
  <li>Samuel G. Müller, Frank Hutter. “TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation”</li>
  <li>Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, Yi Yang. “Random Erasing Data Augmentation”</li>
  <li>Terrance DeVries, Graham W. Taylor. “Improved Regularization of Convolutional Neural Networks with Cutout”</li>
  <li>Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna. “Rethinking the Inception Architecture for Computer Vision”</li>
  <li>Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz. “mixup: Beyond Empirical Risk Minimization”</li>
  <li>Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, Youngjoon Yoo. “CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features”</li>
  <li>Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, Daniel Soudry. “Augment your batch: better training with larger batches”</li>
  <li>Maxim Berman, Hervé Jégou, Andrea Vedaldi, Iasonas Kokkinos, Matthijs Douze. “Multigrain: a unified image embedding for classes and instances”</li>
</ol>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p
        class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and the latest news</p>
    
    
        <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
        <script>
          hbspt.forms.create({
            region: "na1",
            portalId: "8112310",
            formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
          });
        </script>
        
    
      <p
        class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its projects regarding their events, training, research, developments, and related announcements. I understand that I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
        
    </div>
    


    <div class="lf-grid">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org" class="footer-logo">
          <img src="/assets/images/logo-icon.svg" alt="PyTorch logo" width="40">
        </a>
      </div>

      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook"><path fill="currentColor" d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z"/></svg>	
        </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X"><path fill="currentColor" d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66"/></svg>
        </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube"><path fill="currentColor" d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z"/></svg>	
        </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn"><rect width="512" height="512" rx="0" fill="currentColor"/><circle fill="#000" cx="142" cy="138" r="37"/><path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198"/><path fill="#000" d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
        </a></li>
        <li><a href="https://join.slack.com/t/pytorch/shared_invite/zt-2j2la612p-miUinTTaxXczKOJw48poHA" target="_blank" title="PyTorch Slack">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack"><path fill="currentColor" d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z"></path></svg>
        </a></li>
        <li><a href="/wechat" title="PyTorch on WeChat">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat"><path fill="currentColor" d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z"></path><path fill="currentColor" d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z"></path></svg>
        </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/legal/policies/">Linux Foundation Policies</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">LF Projects, LLC Policies</a>. <a href="https://www.linuxfoundation.org/privacy">Privacy Policy</a> and <a href="https://www.linuxfoundation.org/terms">Terms of Use</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Learn</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/get-started">Get Started</a> 
          </li>
          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/#community-module">Community</a>
          </li>
          <li>
            <a href="https://discuss.pytorch.org">Forums</a>
          </li>
          <li>
            <a href="/resources">Developer Resources</a>
          </li>
          <li>
            <a href="/ecosystem/contributor-awards-2024">Contributor Awards - 2024</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
          <li>
            <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Docs</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/docs">PyTorch</a>
          </li>
          <li>
            <a href="/pytorch-domains">PyTorch Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/blog">PyTorch Blog</a>
          </li>
          <li>
            <a href="/community-blog">Community Blog</a>
          </li>
          <li>
            <a href="/videos">Videos</a>
          </li>
          <li>
            <a href="/community-stories">Community Stories</a>
          </li>
          <li>
            <a href="/events">Events</a>
          </li>
          <li>
            <a href="/newsletter">Newsletter</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="/credits">Cloud Credit Program</a>
          </li>
          <li>          
            <a href="/tac">Technical Advisory Council</a>
          </li>
          <li>
            <a href="/staff">Staff</a>
          </li>
          <li>
            <a href="/contact-us">Contact Us</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a href="/join">Become a Member</a>
        </li>
        <li class="resources-mobile-menu-title">
          <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub"><div id="topnav-gh-icon"></div></a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header, .announcement-header, .comm-stories-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

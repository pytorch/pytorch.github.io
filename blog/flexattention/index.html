<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      FlexAttention: The Flexibility of PyTorch with the Performance of FlashAttention | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="

" />

  <meta property="og:image" content="https://pytorch.org/assets/images/social-share.jpg" />
  <meta name="twitter:image" content="https://pytorch.org/assets/images/social-share.jpg" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="FlexAttention: The Flexibility of PyTorch with the Performance of FlashAttention" />
<meta property="og:description" content="

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="FlexAttention: The Flexibility of PyTorch with the Performance of FlashAttention" />
<meta name="twitter:description" content="

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>
    <div class="hello-bar">
        <div class="container">
          Join us in Silicon Valley September 18-19 at the 2024 PyTorch Conference. <a target="_blank" href="https://events.linuxfoundation.org/pytorch-conference/">Learn more</a>.
        </div>
      </div>
      
    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Learn
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/get-started">
            <span class=dropdown-title>Get Started</span>
            <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/">
            <span class="dropdown-title">Tutorials</span>
            <p>Whats new in PyTorch tutorials</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
            <span class="dropdown-title">Learn the Basics</span>
            <p>Familiarize yourself with PyTorch concepts and modules</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
            <span class="dropdown-title">PyTorch Recipes</span>
            <p>Bite-size, ready-to-deploy PyTorch code examples</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
            <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
            <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="https://github.com/pytorch-fdn/ecosystem" target="_blank">
            <span class="dropdown-title">Join the Ecosystem</span>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/contributor-awards-2024">
            <span class="dropdown-title">Contributor Awards - 2024</span>
            <p>Award winners announced at this year's PyTorch Conference</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
            <p>Build innovative and privacy-aware AI experiences for edge devices</p>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
            <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
          </a>
          <a class="nav-dropdown-item" target="_blank" href="https://pytorch.org/executorch/stable/index.html">
            <span class="dropdown-title">ExecuTorch Documentation</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://pytorch.org/docs">
            <span class="dropdown-title">PyTorch</span>
            <p>Explore the documentation for comprehensive guidance on how to use PyTorch.</p>
          </a>
          <a class="nav-dropdown-item" href="/pytorch-domains">
            <span class="dropdown-title">PyTorch Domains</span>
            <p> Read the PyTorch Domains documentation to learn more about domain-specific libraries.</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Blog & News
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/blog">
            <span class="dropdown-title">PyTorch Blog</span>
            <p>Catch up on the latest technical news and happenings</p>
          </a>
          <a class="nav-dropdown-item" href="/community-blog">
            <span class="dropdown-title">Community Blog</span>
            <p>Stories from the PyTorch ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/videos">
            <span class="dropdown-title">Videos</span>
            <p>Learn about the latest PyTorch tutorials, new, and more </p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class="dropdown-title">Community Stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
          <a class="nav-dropdown-item" href="/newsletter">
            <span class=dropdown-title>Newsletter</span>
            <p>Stay up-to-date with the latest updates</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          About
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/governing-board">
            <span class=dropdown-title>Governing Board</span>
          </a>
          <a class="nav-dropdown-item" href="/credits">
            <span class=dropdown-title>Cloud Credit Program</span>
          </a>
          <a class="nav-dropdown-item" href="/tac">
            <span class=dropdown-title>Technical Advisory Council</span>
          </a>
          <a class="nav-dropdown-item" href="/staff">
            <span class=dropdown-title>Staff</span>
          </a>
          <a class="nav-dropdown-item" href="/contact-us">
            <span class=dropdown-title>Contact Us</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <a href="/join" data-cta="join">
        Become a Member
      </a>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub">
        <div id="topnav-gh-icon"></div>
      </a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">August 07, 2024</p>
            <h1>
                <a class="blog-title">FlexAttention: The Flexibility of PyTorch with the Performance of FlashAttention</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Team PyTorch: Driss Guessous, Yanbo Liang, Joy Dong, Horace He
                      
                    </p>
                    <p><img src="/assets/images/flexattention/fg1.jpg" alt="a cartoon chart flexing his muscles" style="width:100%" /></p>

<p>In theory, Attention is All You Need. In practice, however, we also need optimized attention implementations like FlashAttention.</p>

<p>Although these fused attention implementations have substantially improved performance and enabled long contexts, this efficiency has come with a loss of flexibility. You can no longer try out a new attention variant by writing a few PyTorch operators - you often need to write a new custom kernel! This operates as a sort of “software lottery” for ML researchers - if your attention variant doesn’t fit into one of the existing optimized kernels, you’re doomed to slow runtime and CUDA OOMs.</p>

<p>For some examples of attention variants, we have Causal, <a href="https://paperswithcode.com/method/relative-position-encodings">Relative Positional Embeddings</a>, <a href="https://paperswithcode.com/method/alibi">Alibi</a>, <a href="https://mistral.ai/news/announcing-mistral-7b/">Sliding Window Attention</a>, <a href="https://twitter.com/andersonbcdefg/status/1800907703688339569">PrefixLM</a>,  <a href="https://github.com/pytorch/torchtune/pull/875">Document Masking/Sample Packing/Jagged Tensors</a>, <a href="https://twitter.com/LysandreJik/status/1807779471891538199">Tanh Soft-Capping</a>, <a href="https://arxiv.org/abs/2309.06180">PagedAttention</a>, etc. Even worse, folks often want combinations of these! Sliding Window Attention + Document Masking + Causal + Context Parallelism? Or what about PagedAttention + Sliding Window + Tanh Soft-Capping?</p>

<p>The left picture below represents the state of the world today - some combinations of masking + biases + setting have existing kernels implemented. But the various options lead to an exponential number of settings, and so overall we end up with fairly spotty support. Even worse, new attention variants researchers come up with will have <em>zero</em> support.</p>

<p><img src="/assets/images/flexattention/fg2.jpg" alt="Attention variant support diagram" style="max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%" /></p>

<p>To solve this hypercube problem once and for all, we introduce <strong>FlexAttention</strong>, a new PyTorch API.</p>

<ol>
  <li>We provide a flexible API that allows implementing many attention variants (including all the ones mentioned in the blog post so far) in a few lines of idiomatic PyTorch code.</li>
  <li>We lower this into a fused FlashAttention kernel through <code class="language-plaintext highlighter-rouge">torch.compile</code>, generating a FlashAttention kernel that doesn’t materialize any extra memory and has performance competitive with handwritten ones.</li>
  <li>We also automatically generate the backwards pass, leveraging PyTorch’s autograd machinery.</li>
  <li>Finally, we can also take advantage of sparsity in the attention mask, resulting in significant improvements over standard attention implementations.</li>
</ol>

<p>With FlexAttention, we hope that trying new attention variants will only be limited by your imagination.</p>

<p>You can find many FlexAttention examples at the Attention Gym: <a href="https://github.com/pytorch-labs/attention-gym">https://github.com/pytorch-labs/attention-gym</a>. If you have any cool applications, feel free to submit an example!</p>

<p>PS: We also find this API very exciting since it leverages a lot of existing PyTorch infra in a fun way - more on that in the end.</p>

<h2 id="flexattention">FlexAttention</h2>

<p>Here is the classic attention equation:</p>

<p><img src="/assets/images/flexattention/fg3.png" alt="math equation" style="max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%" /></p>

<p>In code form:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">]</span>
<span class="n">score</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q</span> <span class="o">@</span> <span class="n">K</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">head_dim</span><span class="p">)</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">output</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">probabilities</span> <span class="o">@</span> <span class="n">V</span>
</code></pre></div></div>

<p>FlexAttention allows for an user-defined function <code class="language-plaintext highlighter-rouge">score_mod:</code></p>

<p><img src="/assets/images/flexattention/fg4.png" alt="math equation" style="width:100%" /></p>

<p>In code form:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">]</span>
<span class="n">score</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q</span> <span class="o">@</span> <span class="n">K</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">head_dim</span><span class="p">)</span>
<span class="n">modified_scores</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">score_mod</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">modified_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">output</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">probabilities</span> <span class="o">@</span> <span class="n">V</span>
</code></pre></div></div>

<p>This function allows you to <em>modify</em> the attention scores prior to softmax. Surprisingly, this ends up being sufficient for the vast majority of attention variants (examples below)!</p>

<p>Concretely, the expected signature for <code class="language-plaintext highlighter-rouge">score_mod</code> is somewhat unique.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">score_mod</span><span class="p">(</span><span class="n">score</span><span class="p">:</span> <span class="n">f32</span><span class="p">[],</span> <span class="n">b</span><span class="p">:</span> <span class="n">i32</span><span class="p">[],</span> <span class="n">h</span><span class="p">:</span> <span class="n">i32</span><span class="p">[],</span> <span class="n">q_idx</span><span class="p">:</span> <span class="n">i32</span><span class="p">[],</span> <span class="n">kv_idx</span><span class="p">:</span> <span class="n">i32</span><span class="p">[])</span>
    <span class="k">return</span> <span class="n">score</span> <span class="c1"># noop - standard attention
</span></code></pre></div></div>

<p>In other words, <code class="language-plaintext highlighter-rouge">score</code> is a scalar pytorch tensor that represents the dot product of a query token and a key token. The rest of the arguments tell you <em>which</em> dot product you’re currently computing - <code class="language-plaintext highlighter-rouge">b</code> (current element in batch), <code class="language-plaintext highlighter-rouge">h</code> (current head), <code class="language-plaintext highlighter-rouge">q_idx</code> (position in query), <code class="language-plaintext highlighter-rouge">kv_idx</code> (position in key/value tensors).</p>

<p>To apply this function, we could implement it as</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">q_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">kv_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">):</span>
                <span class="n">modified_scores</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">score_mod</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">],</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">)</span>
</code></pre></div></div>

<p>Of course, this is not how FlexAttention is implemented under the hood. Leveraging <code class="language-plaintext highlighter-rouge">torch.compile</code>, we automatically lower your function into a single <em>fused</em> FlexAttention kernel - guaranteed or your money back!</p>

<p>This API ends up being surprisingly expressive. Let’s look at some examples.</p>

<h2 id="score-mod-examples">Score Mod Examples</h2>

<h3 id="full-attention">Full Attention</h3>

<p>Let’s first do “full attention”, or standard bidirectional attention. In this case, <code class="language-plaintext highlighter-rouge">score_mod</code> is a no-op - it takes as input the scores and then returns them as is..</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">noop</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">score</span>
</code></pre></div></div>

<p>And to use it end to end (including both forwards <em>and</em> backwards):</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch.nn.attention.flex_attention</span> <span class="kn">import</span> <span class="n">flex_attention</span>

<span class="n">flex_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">score_mod</span><span class="o">=</span><span class="n">noop</span><span class="p">).</span><span class="nb">sum</span><span class="p">().</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="relative-position-encodings">Relative Position Encodings</h3>

<p>One common attention variant is the <a href="https://paperswithcode.com/method/relative-position-encodings">“relative position encoding</a>”. Instead of encoding the absolute distance in the queries and keys, relative position encoding adjusts scores based on the “distance” between the queries and keys.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">relative_positional</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">score</span> <span class="o">+</span> <span class="p">(</span><span class="n">q_idx</span> <span class="o">-</span> <span class="n">kv_idx</span><span class="p">)</span>
</code></pre></div></div>

<p>Note that unlike typical implementations, this does <em>not</em> need to materialize a SxS tensor. Instead, FlexAttention computes the bias values “on the fly” within the kernel, leading to significant memory and performance improvements.</p>

<p><img src="/assets/images/flexattention/fg5.png" alt="relative position encoding" style="width:100%" /></p>

<h3 id="alibi-bias">ALiBi Bias</h3>

<p><img src="/assets/images/flexattention/fg6.png" alt="alibi bias" style="max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%" /></p>
<p style="text-align: center;"><em>Source: <a href="https://arxiv.org/abs/2108.12409">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</a></em></p>

<p>ALiBi was introduced in <a href="https://arxiv.org/abs/2108.12409">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</a>, and claims to have beneficial properties for length extrapolation at inference. Notably, MosaicML has pointed to <a href="https://twitter.com/jefrankle/status/1804567458092605736">“lack of kernel support”</a> as the main reason why they eventually switched from ALiBi to rotary embeddings.</p>

<p>Alibi is similar to relative positional encodings with one exception - it has a per-head factor that is typically precomputed.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">alibi_bias</span> <span class="o">=</span> <span class="n">generate_alibi_bias</span><span class="p">()</span> <span class="c1"># [num_heads]
</span>
<span class="k">def</span> <span class="nf">alibi</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">alibi_bias</span><span class="p">[</span><span class="n">h</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">kv_idx</span> <span class="o">-</span> <span class="n">q_idx</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">score</span> <span class="o">+</span> <span class="n">bias</span>
</code></pre></div></div>

<p>This demonstrates one interesting piece of flexibility <code class="language-plaintext highlighter-rouge">torch.compile</code> provides - we can load from <code class="language-plaintext highlighter-rouge">alibi_bias</code> even though it <em>wasn’t explicitly passed in as an input</em>! The generated Triton kernel will calculate the correct loads from the <code class="language-plaintext highlighter-rouge">alibi_bias</code> tensor and fuse it. Note that you could regenerate <code class="language-plaintext highlighter-rouge">alibi_bias</code> and we still wouldn’t need to recompile.</p>

<h3 id="soft-capping">Soft-capping</h3>

<p>Soft-capping is a technique used in <a href="https://huggingface.co/blog/gemma2#soft-capping-and-attention-implementations">Gemma2</a> and Grok-1 that prevents logits from growing excessively large. In FlexAttention, it looks like:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">softcap</span> <span class="o">=</span> <span class="mi">20</span>
<span class="k">def</span> <span class="nf">soft_cap</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">score</span> <span class="o">/</span> <span class="n">softcap</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">score</span> <span class="o">*</span> <span class="n">softcap</span>
    <span class="k">return</span> <span class="n">score</span>
</code></pre></div></div>

<p>Note that we also automatically generate the backwards pass from the forwards pass here. Also, although this implementation is semantically correct, we likely want to use a tanh approximation in this case for performance reasons. See <a href="https://github.com/pytorch-labs/attention-gym/blob/main/attn_gym/mods/softcapping.py">attention-gym</a> for more details.</p>

<h3 id="causal-mask">Causal Mask</h3>

<p>Although bidirectional attention is the simplest, the original <em>Attention is All You Need</em> paper and the vast majority of LLMs use attention in a decoder-only setting where each token can only attend to the tokens prior to it. Folks often think of this as a lower-triangular mask, but with the <code class="language-plaintext highlighter-rouge">score_mod</code> API it can be expressed as:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">causal_mask</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">q_idx</span> <span class="o">&gt;=</span> <span class="n">kv_idx</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s">"inf"</span><span class="p">))</span>
</code></pre></div></div>

<p>Basically, if the query token is “after” the key token, we keep the score. Otherwise, we mask it out by setting it to -inf, thus ensuring it won’t participate in the softmax calculation.</p>

<p>However, masking is special compared to other modifications - if something is masked out, we can completely skip its computation! In this case, a causal mask has about 50% sparsity, so not taking advantage of the sparsity would result in a 2x slowdown. Although this <code class="language-plaintext highlighter-rouge">score_mod</code> is sufficient to implement causal masking <em>correctly</em>, getting the performance benefits of sparsity requires another concept - <code class="language-plaintext highlighter-rouge">mask_mod</code>.</p>

<h2 id="mask-mods">Mask Mods</h2>

<p>To take advantage of sparsity from masking, we need to do some more work. Specifically, by passing a <code class="language-plaintext highlighter-rouge">mask_mod</code> to <a href="https://github.com/pytorch/pytorch/blob/e49c0acc396e89baf8c6450e1fa0571d4ce2d4ed/torch/nn/attention/flex_attention.py#L594"><code class="language-plaintext highlighter-rouge">create_block_mask</code></a>, we can create a <code class="language-plaintext highlighter-rouge">BlockMask</code>. FlexAttention can then use <code class="language-plaintext highlighter-rouge">BlockMask</code> to take advantage of the sparsity!</p>

<p>The signature of <code class="language-plaintext highlighter-rouge">mask_mod</code> is very similar to <code class="language-plaintext highlighter-rouge">score_mod</code> - just without the <code class="language-plaintext highlighter-rouge">score</code>. In particular</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># returns True if this position should participate in the computation
</span><span class="n">mask_mod</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="nb">bool</span>
</code></pre></div></div>

<p>Note that <code class="language-plaintext highlighter-rouge">score_mod</code> is strictly <em>more</em> expressive than <code class="language-plaintext highlighter-rouge">mask_mod</code>. However, for masking, it’s recommended to use <code class="language-plaintext highlighter-rouge">mask_mod</code> and <code class="language-plaintext highlighter-rouge">create_block_mask</code>, as it’s more performant. See the FAQ on why <code class="language-plaintext highlighter-rouge">score_mod</code> and <code class="language-plaintext highlighter-rouge">mask_mod</code> are separate.</p>

<p>Now, let’s take a look at how we might implement causal mask with <code class="language-plaintext highlighter-rouge">mask_mod</code>.</p>

<h3 id="causal-mask-1">Causal Mask</h3>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch.nn.attention.flex_attention</span> <span class="kn">import</span> <span class="n">create_block_mask</span>

<span class="k">def</span> <span class="nf">causal</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">q_idx</span> <span class="o">&gt;=</span> <span class="n">kv_idx</span>

<span class="c1"># Because the sparsity pattern is independent of batch and heads, we'll set them to None (which broadcasts them) 
</span><span class="n">block_mask</span> <span class="o">=</span> <span class="n">create_block_mask</span><span class="p">(</span><span class="n">causal</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">Q_LEN</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">KV_LEN</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
<span class="c1"># In this case, we don't need a score_mod, so we won't pass any in.
# However, score_mod can still be combined with block_mask if you need the additional flexibility.
</span><span class="n">flex_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">block_mask</span><span class="o">=</span><span class="n">block_mask</span><span class="p">)</span>
</code></pre></div></div>

<p>Note that <code class="language-plaintext highlighter-rouge">create_block_mask</code> is a <strong>relatively expensive operation!</strong> Although FlexAttention will not need to recompile when it changes, if you aren’t careful about caching it, it can lead to significant slowdowns (check out the FAQ for suggestions on best practices).</p>

<p><img src="/assets/images/flexattention/fg7.png" alt="flexattention performance charts" style="width:100%" /></p>

<p>While the TFlops are roughly the same, the execution time is 2x faster for the mask_mod version! This demonstrates that we can leverage the sparsity that BlockMask provides us <em>without</em> losing hardware efficiency.</p>

<h3 id="sliding-window--causal">Sliding Window + Causal</h3>

<p><img src="/assets/images/flexattention/fg8.png" alt="Sliding Window Causal diagrams" style="width:100%" /></p>
<p style="text-align: center;"><em>Source: <a href="https://arxiv.org/abs/2310.06825">Mistral 7B</a></em></p>

<p>Popularized by <a href="https://arxiv.org/abs/2310.06825">Mistral</a>, sliding window attention (also known as local attention) takes advantage of the intuition that the most recent tokens are the most useful. In particular, it allows the query token to only attend to, say, the 1024 most recent tokens. This is often used together with causal attention.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">SLIDING_WINDOW</span> <span class="o">=</span> <span class="mi">1024</span>

<span class="k">def</span> <span class="nf">sliding_window_causal</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
    <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">q_idx</span> <span class="o">&gt;=</span> <span class="n">kv_idx</span>
    <span class="n">window_mask</span> <span class="o">=</span> <span class="n">q_idx</span> <span class="o">-</span> <span class="n">kv_idx</span> <span class="o">&lt;=</span> <span class="n">SLIDING_WINDOW</span> 
    <span class="k">return</span> <span class="n">causal_mask</span> <span class="o">&amp;</span> <span class="n">window_mask</span>

<span class="c1"># If you want to be cute...
</span><span class="kn">from</span> <span class="nn">torch.nn.attention</span> <span class="kn">import</span> <span class="n">and_masks</span>

<span class="k">def</span> <span class="nf">sliding_window</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">q_idx</span> <span class="o">-</span> <span class="n">kv_idx</span> <span class="o">&lt;=</span> <span class="n">SLIDING_WINDOW</span>

<span class="n">sliding_window_causal</span> <span class="o">=</span> <span class="n">and_masks</span><span class="p">(</span><span class="n">causal_mask</span><span class="p">,</span> <span class="n">sliding_window</span><span class="p">)</span>
</code></pre></div></div>

<p>We benchmark it against <code class="language-plaintext highlighter-rouge">F.scaled_dot_product_attention</code> with a sliding window mask as well as FA2 with a causal mask (as a reference point for performance). Not only are we significantly faster than <code class="language-plaintext highlighter-rouge">F.scaled_dot_product_attention</code>, we’re <em>also</em> significantly faster than FA2 with a causal mask as this mask has significantly more sparsity.</p>

<p><img src="/assets/images/flexattention/fg9.png" alt="execution time charts" style="max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%" /></p>

<h3 id="prefixlm">PrefixLM</h3>

<p><img src="/assets/images/flexattention/fg10.png" alt="PrefixLM diagram" style="max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%" /></p>
<p style="text-align: center;"><em>Source: <a href="https://arxiv.org/abs/2407.07726">PaliGemma: A versatile 3B VLM for transfer</a></em></p>

<p>The T5 architecture, proposed in <a href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>, describes an attention variant that performs full bidirectional attention on a “prefix”, and causal attention on the rest. We again compose two mask functions to accomplish this, one for causal masking and one that is based off of the prefix length.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prefix_length</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">]</span>
<span class="k">def</span> <span class="nf">prefix_mask</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">kv_idx</span> <span class="o">&lt;=</span> <span class="n">prefix_length</span><span class="p">[</span><span class="n">b</span><span class="p">]</span>

<span class="n">prefix_lm_causal</span> <span class="o">=</span> <span class="n">or_masks</span><span class="p">(</span><span class="n">prefix_mask</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">)</span>
<span class="c1"># In this case, our mask is different per sequence so we set B equal to our batch size
</span><span class="n">block_mask</span> <span class="o">=</span> <span class="n">create_block_mask</span><span class="p">(</span><span class="n">prefix_lm_causal</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">S</span><span class="p">)</span>
</code></pre></div></div>

<p>Just like with <code class="language-plaintext highlighter-rouge">score_mod</code>, <code class="language-plaintext highlighter-rouge">mask_mod</code> allows us to refer to additional tensors that aren’t explicitly an input to the function! However, with prefixLM, the sparsity pattern changes <em>per</em> <em>input</em>. This means that for each new input batch, we’ll need to recompute the <code class="language-plaintext highlighter-rouge">BlockMask</code>. One common pattern is to call <code class="language-plaintext highlighter-rouge">create_block_mask</code> at the beginning of your model and reuse that <code class="language-plaintext highlighter-rouge">block_mask</code> for all attention calls in your model. See <em>Recomputing Block Masks vs. Recompilation.</em></p>

<p>However, in exchange for that, we’re not only able to have an efficient attention kernel for prefixLM, we’re <em>also</em> able to take advantage of however much sparsity exists in the input! FlexAttention will dynamically adjust its performance based off of the BlockMask data, <em>without</em> needing to recompile the kernel.</p>

<h3 id="document-maskingjagged-sequences">Document Masking/Jagged Sequences</h3>

<p>Another common attention variant is document masking/jagged sequences. Imagine that you have a number of sequences of varying length. You want to train on all of them together, but unfortunately, most operators only accept rectangular tensors.</p>

<p>Through <code class="language-plaintext highlighter-rouge">BlockMask</code>, we can support this efficiently in FlexAttention as well!</p>

<ol>
  <li>First, we flatten all sequences into a single sequence with sum(sequence lengths) tokens.</li>
  <li>Then, we compute the document_id that each token belongs to.</li>
  <li>Finally, in our <code class="language-plaintext highlighter-rouge">mask_mod</code>, we simply whether the query and kv token belong to the same document!</li>
</ol>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The document that each token belongs to.
# e.g. [0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2] corresponds to sequence lengths 3, 2, and 6.
</span><span class="n">document_id</span><span class="p">:</span> <span class="p">[</span><span class="n">SEQ_LEN</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">document_masking</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">document_id</span><span class="p">[</span><span class="n">q_idx</span><span class="p">]</span> <span class="o">==</span> <span class="n">document_id</span><span class="p">[</span><span class="n">kv_idx</span><span class="p">]</span>
</code></pre></div></div>

<p>And that’s it! In this case, we see that we end up with a blockdiagonal mask.</p>

<p><img src="/assets/images/flexattention/fg11.png" alt="blockdiagonal mask" style="max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%" /></p>

<p>One interesting aspect about document masking is that it’s easy to see how it might combine with an arbitrary combination of other masks . For example, we already defined <code class="language-plaintext highlighter-rouge">prefixlm_mask</code> in the previous section. Do we now need to define a <code class="language-plaintext highlighter-rouge">prefixlm_document_mask</code> function as well?</p>

<p>In these cases, one pattern we’ve found quite useful is what we call a “higher level modification”. In this case, we can take an existing <code class="language-plaintext highlighter-rouge">mask_mod</code> and automatically transform it into one that works with jagged sequences!</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_doc_mask_mod</span><span class="p">(</span><span class="n">mask_mod</span><span class="p">,</span> <span class="n">document_id</span><span class="p">):</span>
    <span class="c1"># Get unique document IDs and their counts
</span>    <span class="n">_</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">unique_consecutive</span><span class="p">(</span><span class="n">document_id</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="c1"># Create cumulative counts (offsets)
</span>    <span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">document_id</span><span class="p">.</span><span class="n">device</span><span class="p">),</span> <span class="n">counts</span><span class="p">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">0</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
    <span class="k">def</span> <span class="nf">doc_mask_wrapper</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
        <span class="n">same_doc</span> <span class="o">=</span> <span class="n">document_id</span><span class="p">[</span><span class="n">q_idx</span><span class="p">]</span> <span class="o">==</span> <span class="n">document_id</span><span class="p">[</span><span class="n">kv_idx</span><span class="p">]</span>
        <span class="n">q_logical</span> <span class="o">=</span> <span class="n">q_idx</span> <span class="o">-</span> <span class="n">offsets</span><span class="p">[</span><span class="n">document_id</span><span class="p">[</span><span class="n">q_idx</span><span class="p">]]</span>
        <span class="n">kv_logical</span> <span class="o">=</span> <span class="n">kv_idx</span> <span class="o">-</span> <span class="n">offsets</span><span class="p">[</span><span class="n">document_id</span><span class="p">[</span><span class="n">kv_idx</span><span class="p">]]</span>
        <span class="n">inner_mask</span> <span class="o">=</span> <span class="n">mask_mod</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_logical</span><span class="p">,</span> <span class="n">kv_logical</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">same_doc</span> <span class="o">&amp;</span> <span class="n">inner_mask</span>
    <span class="k">return</span> <span class="n">doc_mask_wrapper</span>
</code></pre></div></div>

<p>For example, given the <code class="language-plaintext highlighter-rouge">prefix_lm_causal</code> mask from above, we can transform it into one that works on on packed documents like so:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prefix_length</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">prefix_mask</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">kv_idx</span> <span class="o">&lt;</span> <span class="n">prefix_length</span>
<span class="n">prefix_lm_causal</span> <span class="o">=</span> <span class="n">or_masks</span><span class="p">(</span><span class="n">prefix_mask</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">)</span>
<span class="n">doc_prefix_lm_causal_mask</span> <span class="o">=</span> <span class="n">generate_doc_mask_mod</span><span class="p">(</span><span class="n">prefix_lm_causal</span><span class="p">,</span> <span class="n">document_id</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/images/flexattention/fg12.png" alt="blockdiagonal mask" style="max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%" /></p>

<p>Now, this mask is “block-prefixLM-diagonal” shaped. :)</p>

<p>That’s all of our examples! There are far more attention variants than we have space to list, so check out <a href="https://github.com/pytorch-labs/attention-gym">Attention Gym</a> for more examples. We hope that the community will contribute some of their favorite applications of FlexAttention as well.</p>

<h3 id="faq">FAQ</h3>

<h5 id="q-when-does-flexattention-need-to-recompile"><strong>Q: When does FlexAttention need to recompile?</strong></h5>

<p>As FlexAttention leverages <code class="language-plaintext highlighter-rouge">torch.compile</code> for graph capture, it can actually avoid recompilation in a broad spectrum of cases. Notably, it does <em>not</em> need to recompile even if captured tensors change values!</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">flex_attention</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">flex_attention</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">create_bias_mod</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">bias_mod</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">score</span> <span class="o">+</span> <span class="n">bias</span>
    <span class="k">return</span> <span class="n">bias_mod</span>
<span class="n">bias_mod1</span> <span class="o">=</span> <span class="n">create_bias_mod</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="n">flex_attention</span><span class="p">(...,</span> <span class="n">score_mod</span><span class="o">=</span><span class="n">bias_mod1</span><span class="p">)</span> <span class="c1"># Compiles the kernel here 
</span>
<span class="n">bias_mod2</span> <span class="o">=</span> <span class="n">create_bias_mod</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="n">flex_attention</span><span class="p">(...,</span> <span class="n">score_mod</span><span class="o">=</span><span class="n">bias_mod2</span><span class="p">)</span> <span class="c1"># Doesn't need to recompile! 
</span></code></pre></div></div>

<p>Even changing the block-sparsity doesn’t require a recompile. However, if the block-sparsity changes, we do need to <em>recompute</em> the BlockMask.</p>

<h5 id="q-when-should-we-recompute-the-blockmask"><strong>Q: When should we recompute the BlockMask?</strong></h5>

<p>We need to recompute the BlockMask whenever the block-sparsity changes. Although computing the BlockMask is much cheaper than recompilation (on the order of hundreds of microseconds as opposed to seconds), you should still take care to not excessively recompute the BlockMask.</p>

<p>Here are some common patterns and some recommendations on how you might approach them.</p>

<p><strong>Mask never changes (e.g. causal mask)</strong><br />
In this case, you can simply precompute the block mask and cache it globally, reusing it for all attention calls.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">block_mask</span> <span class="o">=</span> <span class="n">create_block_mask</span><span class="p">(</span><span class="n">causal_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span><span class="n">S</span><span class="p">)</span>
<span class="n">causal_attention</span> <span class="o">=</span> <span class="n">functools</span><span class="p">.</span><span class="n">partial</span><span class="p">(</span><span class="n">flex_attention</span><span class="p">,</span> <span class="n">block_mask</span><span class="o">=</span><span class="n">block_mask</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Mask changes every batch (e.g. document masking)</strong><br />
In this case, we would suggest computing the BlockMask at the beginning of the model and threading it through the model - reusing the BlockMask for all layers.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">doc_mask</span><span class="p">):</span>
    <span class="c1"># Compute block mask at beginning of forwards
</span>    <span class="n">block_mask</span> <span class="o">=</span> <span class="n">create_block_mask</span><span class="p">(</span><span class="n">doc_mask</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">S</span><span class="p">)</span>    
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">block_mask</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">block_mask</span><span class="p">)</span>
    <span class="p">...</span>
    <span class="c1"># amortize block mask construction cost across all layers
</span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">block_mask</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p><strong>Mask changes every layer (e.g. data-dependent sparsity)</strong><br />
This is the hardest setting, since we’re unable to amortize the block mask computation across multiple FlexAttention invocations. Although FlexAttention can certainly still benefit this case, the actual benefits from BlockMask depend on how sparse your attention mask is and how fast we can construct the BlockMask. That leads us to…</p>

<h5 id="q-how-can-we-compute-blockmask-quicker"><strong>Q: How can we compute BlockMask quicker?</strong></h5>

<p><code class="language-plaintext highlighter-rouge">create_block_mask</code> is unfortunately fairly expensive, both from a memory and compute perspective, as determining whether a block is completely sparse requires evaluating <code class="language-plaintext highlighter-rouge">mask_mod</code> at every single point in the block. There are a couple ways to address this:</p>

<ol>
  <li>If your mask is the same across batch size or heads, make sure that you’re broadcasting over those (i.e. set them to <code class="language-plaintext highlighter-rouge">None</code> in <code class="language-plaintext highlighter-rouge">create_block_mask</code>).</li>
  <li>Compile <code class="language-plaintext highlighter-rouge">create_block_mask</code>. Unfortunately, today, <code class="language-plaintext highlighter-rouge">torch.compile</code> does not work directly on <code class="language-plaintext highlighter-rouge">create_block_mask</code> due to some unfortunate limitations. However, you can set <code class="language-plaintext highlighter-rouge">_compile=True</code>, which will significantly reduce the peak memory and runtime (often an order of magnitude in our testing).</li>
  <li>
    <p>Write a custom constructor for BlockMask. The metadata for BlockMask is quite simple (see the <a href="https://pytorch.org/docs/main/nn.attention.flex_attention.html#blockmask">documentation</a>). It’s essentially two tensors.
a. <code class="language-plaintext highlighter-rouge">num_blocks</code>: The number of KV blocks computed for each query block.<br />
b. <code class="language-plaintext highlighter-rouge">indices</code>: The positions of the KV blocks computed for each query block.</p>

    <p>For example, here’s a custom BlockMask constructor for <code class="language-plaintext highlighter-rouge">causal_mask</code>.</p>
  </li>
</ol>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_causal_mask</span><span class="p">(</span><span class="n">S</span><span class="p">):</span>
    <span class="n">BLOCK_SIZE</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="c1"># The first query block computes one block, the second query block computes 2 blocks, etc.
</span>    <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">S</span> <span class="o">//</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="c1"># Since we're always computing from the left to the right,
</span>    <span class="c1"># we can use the indices [0, 1, 2, ...] for every query block.
</span>    <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">S</span> <span class="o">//</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">).</span><span class="n">expand</span><span class="p">(</span>
        <span class="n">S</span> <span class="o">//</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">S</span> <span class="o">//</span> <span class="n">BLOCK_SIZE</span>
    <span class="p">)</span>
    <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">num_blocks</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="k">return</span> <span class="n">BlockMask</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">mask_mod</span><span class="o">=</span><span class="n">causal_mask</span><span class="p">)</span>
</code></pre></div></div>

<h5 id="q-why-are-score_mod-and-mask_mod-different-isnt-mask_mod-just-a-special-case-of-score_mod"><strong>Q: Why are <code class="language-plaintext highlighter-rouge">score_mod</code> and <code class="language-plaintext highlighter-rouge">mask_mod</code> different? Isn’t <code class="language-plaintext highlighter-rouge">mask_mod</code> just a special case of <code class="language-plaintext highlighter-rouge">score_mod</code>?</strong></h5>

<p>Very astute question, hypothetical audience member! In fact, any <code class="language-plaintext highlighter-rouge">mask_mod</code> can be easily converted to a <code class="language-plaintext highlighter-rouge">score_mod</code> (we do not recommend using this function in practice!)</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mask_mod_as_score_mod</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask_mod</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">),</span> <span class="n">score</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s">"inf"</span><span class="p">))</span>
</code></pre></div></div>

<p>So, if <code class="language-plaintext highlighter-rouge">score_mod</code> can implement everything <code class="language-plaintext highlighter-rouge">mask_mod</code> can, what’s the point of having <code class="language-plaintext highlighter-rouge">mask_mod</code>?</p>

<p>One immediate challenge: a <code class="language-plaintext highlighter-rouge">score_mod</code> requires the actual <code class="language-plaintext highlighter-rouge">score</code> value as an input, but when we’re precomputing the BlockMask, we don’t have the actual <code class="language-plaintext highlighter-rouge">score</code> value. We can perhaps fake the values by passing in all zeros, and if the <code class="language-plaintext highlighter-rouge">score_mod</code> returns <code class="language-plaintext highlighter-rouge">-inf</code>, then we consider it to be masked (in fact, we originally did this!).</p>

<p>However, there are two issues. The first is that this is hacky - what if the user’s <code class="language-plaintext highlighter-rouge">score_mod</code> returned <code class="language-plaintext highlighter-rouge">-inf</code> when the input is 0? Or what if the user’s <code class="language-plaintext highlighter-rouge">score_mod</code> masked out with a large negative value instead of <code class="language-plaintext highlighter-rouge">-inf</code>? It seems we’re trying to cram a round peg into a square hole. However, there’s a more important reason to separate out <code class="language-plaintext highlighter-rouge">mask_mod</code> from <code class="language-plaintext highlighter-rouge">score_mod</code> - it’s fundamentally more efficient!.</p>

<p>As it turns out, applying masking to every single computed element is actually quite expensive - our benchmarks see about a 15-20% degradation in performance! So, although we can get significant speedups by skipping half the computation, we lose a meaningful part of that speedup from needing to mask out every element!</p>

<p>Luckily, if we visualize the causal mask, we notice that the vast majority of blocks do not require a “causal mask” at all - they’re fully computed! It is only the blocks on the diagonal, partially computed and partially masked, that require masking to be applied.</p>

<p><img src="/assets/images/flexattention/fg13.png" alt="blockdiagonal mask" style="width:100%" /></p>

<p>The BlockMask previously told us which blocks we need to compute and which blocks we can skip. Now, we further augment this data structure to also tell us which blocks are “fully computed” (i.e. masking can be skipped) vs. “partially computed” (i.e. a mask needs to be applied). Note, however, that although masks can be skipped on “fully computed” blocks, other <code class="language-plaintext highlighter-rouge">score_mod</code>s like relative positional embeddings still need to be applied.</p>

<p>Given just a <code class="language-plaintext highlighter-rouge">score_mod</code>, there’s no sound way for us to tell which parts of it are “masking”. Hence, the user must separate these out themselves into <code class="language-plaintext highlighter-rouge">mask_mod</code>.</p>

<h5 id="q-how-much-additional-memory-does-the-blockmask-need"><strong>Q: How much additional memory does the BlockMask need?</strong></h5>

<p>The BlockMask metadata is of size <code class="language-plaintext highlighter-rouge">[BATCH_SIZE, NUM_HEADS, QUERY_LEN//BLOCK_SIZE, KV_LEN//BLOCK_SIZE].</code> If the mask is the same across the batch or heads dimension it can be broadcasted over that dimension to save memory.</p>

<p>At the default <code class="language-plaintext highlighter-rouge">BLOCK_SIZE</code> of 128, we expect that the memory usage will be fairly negligible for most use cases. For example, for a sequence length of 1 million, the BlockMask would only use 60MB of additional memory. If this is a problem, you can increase the block size:  <code class="language-plaintext highlighter-rouge">create_block_mask(..., BLOCK_SIZE=1024).</code> For example, increasing <code class="language-plaintext highlighter-rouge">BLOCK_SIZE</code> to 1024 would result in this metadata dropping to under a megabyte.</p>

<h5 id="q-how-do-the-numerics-compare"><strong>Q: How do the numerics compare?</strong></h5>

<p>Although the results are not bitwise identical, we are confident that FlexAttention is as numerically accurate as FlashAttention. We generate the following distribution of differences comparing FlashAttention versus FlexAttention over a large range of inputs on both causal and non causal attention variants. The errors are nearly identical.</p>

<p><img src="/assets/images/flexattention/fg14.png" alt="distribution chart" style="width:100%" /></p>

<h3 id="performance">Performance</h3>

<p>Generally speaking, FlexAttention is nearly as performant as a handwritten Triton kernel, which is unsurprising, as we heavily leverage a handwritten Triton kernel. However, due to its generality, we do incur a small performance penalty. For example, we must incur some additional latency to determine which block to compute next. In some cases, we provide some kernel options that can affect the performance of the kernel while changing its behavior. They can be found here: <a href="https://github.com/pytorch/pytorch/blob/ee09d066d35d7e17cf7e9479c0b8bfc70cffc264/torch/_inductor/kernel/flex_attention.py#L146-L155">performance knobs</a></p>

<p>As a case study, let’s explore how the knobs affect the performance of causal attention. We will compare performance of the triton kernel versus FlashAttentionv2 on A100. The script can be found <a href="https://github.com/pytorch/pytorch/blob/main/benchmarks/transformer/score_mod.py">here</a>.</p>

<p>FlexAttention achieves 90% of FlashAttention2’s performance in the forward pass and 85% in the backward pass. FlexAttention is currently utilizing a deterministic algorithm that recomputes more intermediates than FAv2, but we have plans to improve FlexAttention’s backward algorithm and hope to close this gap!</p>

<p><img src="/assets/images/flexattention/fg15.png" alt="flexattention speed chart" style="width:100%" /></p>

<p><img src="/assets/images/flexattention/fg16.png" alt="flexattention speed chart" style="width:100%" /></p>

<h2 id="conclusion">Conclusion</h2>

<p>We hope you have as much fun using FlexAttention as we did developing it! While working on this, we ended up finding way more applications of this API than we could have expected. We’ve already seen it accelerate torchtune’s <a href="https://github.com/pytorch/torchtune/pull/1193">sample packing throughput by 71%</a>, replace the need for a researcher to spend over a week writing their own custom Triton kernel, and deliver competitive performance with custom handwritten attention variants.</p>

<p>One final thing that made implementing FlexAttention quite fun is that we were able to leverage a lot of existing PyTorch infra in an interesting way. For example, one of the unique aspects about TorchDynamo (torch.compile’s frontend) is that it does <em>not</em> require tensors used in the compiled function to be explicitly passed in as inputs. This allows us to compile mods like document masking, which require accessing <em>global</em> variables where the global variables need to change!</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">score_mod</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">score</span> <span class="o">+</span> <span class="n">bias</span><span class="p">[</span><span class="n">q_idx</span><span class="p">][</span><span class="n">kv_idx</span><span class="p">]</span> <span class="c1"># The bias tensor can change!
</span></code></pre></div></div>

<p>Furthermore, the fact that <code class="language-plaintext highlighter-rouge">torch.compile</code> is a generic graph-capture mechanism also allows it to support more “advanced” transformations, such as the higher order transform that transforms any <code class="language-plaintext highlighter-rouge">mask_mod</code> into one that works with jagged tensors.</p>

<p>We also leverage TorchInductor (torch.compile’s backend) infrastructure for Triton templates. Not only did this make it easy to support codegening FlexAttention - it also automatically gave us support for dynamic shapes as well as epilogue fusion (i.e. fusing an operator onto the end of attention)! In the future, we plan on extending this support to allow for quantized versions of attention or things like <a href="https://lmsys.org/blog/2024-01-17-sglang/">RadixAttention</a> as well.</p>

<p>In addition, we also leveraged higher order ops, PyTorch’s autograd to automatically generate the backwards pass, as well as vmap to automatically apply <code class="language-plaintext highlighter-rouge">score_mod</code> for creating the BlockMask.</p>

<p>And, of course, this project wouldn’t have been possible without Triton and TorchInductor’s ability to generate Triton code.</p>

<p>We look forward to leveraging the approach we used here to more applications in the future!</p>

<h3 id="limitations-and-future-work">Limitations and Future Work</h3>

<ul>
  <li>FlexAttention is currently available in PyTorch nightly releases, we plan to release it as a prototype feature in 2.5.0</li>
  <li>We did not cover how to use FlexAttention for inference here (or how to implement PagedAttention) - we will cover those in a later post.</li>
  <li>We are working to improve the performance of FlexAttention to match FlashAttention3 on H100 GPUs.</li>
  <li>FlexAttention requires that all sequence lengths be a multiple of 128 - this will be addressed soon.</li>
  <li>We plan on adding GQA support soon - for now, you can just replicate the kv heads.</li>
</ul>

<h3 id="acknowledgements">Acknowledgements</h3>

<p>We want to highlight some prior work (and people) that have inspired FlexAttention.</p>

<ul>
  <li>Tri Dao’s work on FlashAttention</li>
  <li>Francisco Massa and the Xformers team for BlockSparseAttention in Triton</li>
  <li>The Jax team’s work on SplashAttention</li>
  <li>Philippe Tillet and Keren Zhou for helping us with Triton</li>
  <li>Ali Hassani for discussions on neighborhood attention</li>
  <li>Everybody who’s complained about attention kernels not supporting their favorite attention variant :)</li>
</ul>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p
        class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and the latest news</p>
    
    
        <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
        <script>
          hbspt.forms.create({
            region: "na1",
            portalId: "8112310",
            formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
          });
        </script>
        
    
      <p
        class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its projects regarding their events, training, research, developments, and related announcements. I understand that I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
        
    </div>
    


    <div class="lf-grid">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org" class="footer-logo">
          <img src="/assets/images/logo-icon.svg" alt="PyTorch logo" width="40">
        </a>
      </div>

      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook"><path fill="currentColor" d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z"/></svg>	
        </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X"><path fill="currentColor" d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66"/></svg>
        </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube"><path fill="currentColor" d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z"/></svg>	
        </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn"><rect width="512" height="512" rx="0" fill="currentColor"/><circle fill="#000" cx="142" cy="138" r="37"/><path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198"/><path fill="#000" d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
        </a></li>
        <li><a href="https://join.slack.com/t/pytorch/shared_invite/zt-2j2la612p-miUinTTaxXczKOJw48poHA" target="_blank" title="PyTorch Slack">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack"><path fill="currentColor" d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z"></path></svg>
        </a></li>
        <li><a href="/wechat" title="PyTorch on WeChat">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat"><path fill="currentColor" d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z"></path><path fill="currentColor" d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z"></path></svg>
        </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/legal/policies/">Linux Foundation Policies</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">LF Projects, LLC Policies</a>. <a href="https://www.linuxfoundation.org/privacy">Privacy Policy</a> and <a href="https://www.linuxfoundation.org/terms">Terms of Use</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Learn</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/get-started">Get Started</a> 
          </li>
          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Tools</a>
          </li>
          <li>
            <a href="https://github.com/pytorch-fdn/ecosystem">Join the Ecosystem</a>
          </li>
          <li>
            <a href="/#community-module">Community</a>
          </li>
          <li>
            <a href="https://discuss.pytorch.org">Forums</a>
          </li>
          <li>
            <a href="/resources">Developer Resources</a>
          </li>
          <li>
            <a href="/ecosystem/contributor-awards-2024">Contributor Awards - 2024</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
          <li>
            <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Docs</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/docs">PyTorch</a>
          </li>
          <li>
            <a href="/pytorch-domains">PyTorch Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/blog">PyTorch Blog</a>
          </li>
          <li>
            <a href="/community-blog">Community Blog</a>
          </li>
          <li>
            <a href="/videos">Videos</a>
          </li>
          <li>
            <a href="/community-stories">Community Stories</a>
          </li>
          <li>
            <a href="/events">Events</a>
          </li>
          <li>
            <a href="/newsletter">Newsletter</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="/credits">Cloud Credit Program</a>
          </li>
          <li>          
            <a href="/tac">Technical Advisory Council</a>
          </li>
          <li>
            <a href="/staff">Staff</a>
          </li>
          <li>
            <a href="/contact-us">Contact Us</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a href="/join">Become a Member</a>
        </li>
        <li class="resources-mobile-menu-title">
          <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub"><div id="topnav-gh-icon"></div></a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header, .announcement-header, .comm-stories-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

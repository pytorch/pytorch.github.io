<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      Tensor Comprehensions in PyTorch | PyTorch
    
  </title>
  <meta property="og:title" content="PyTorch"/>
<meta property="og:description" content="An open source deep learning platform that provides a seamless path from research prototyping to production deployment."/>
<meta property="og:url" content="https://www.pytorch.org"/>
<meta property="og:type" content="website"/>
<meta property="og:image" content="https://pytorch.org/assets/images/pytorch-logo.png"/>

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA‌-90545585-1', 'auto');
  ga('send', 'pageview');

</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
  </script>
  <noscript>
    <img height="1" width="1"
    src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
    &noscript=1"/>
  </noscript>

  
</head>


<body class="blog">
    <div class="main-background "></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="">
      <a href="/features">Features</a>
    </li>

    <li class="">
      <a href="/ecosystem">Ecosystem</a>
    </li>

    <li class="active">
      <a href="/blog">Blog</a>
    </li>

    <li>
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li>
      <a href="/docs">Docs</a>
    </li>

    <li class="">
      <a href="/resources">Resources</a>
    </li>

    <li>
      <a href="https://github.com/pytorch/pytorch">Github</a>
    </li>
  </ul>
</div>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid">
        <div class="container blog-detail-container">
            <p class="blog-date">March 05, 2018</p>
            <h1>
                <a class="blog-title">Tensor Comprehensions in PyTorch</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper">
        <div class="main-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Priya Goyal (FAIR), Nicolas Vasilache (FAIR), Oleksandr Zinenko (Inria & DI ENS), Theodoros Theodoridis (ETH Zürich), Zachary DeVito (FAIR), William S. Moses (MIT CSAIL), Sven Verdoolaege (FAIR), Andrew Adams (FAIR), Albert Cohen (Inria & DI ENS & FAIR)
                      
                    </p>
                    <p>Tensor Comprehensions (TC) is a tool that lowers the barrier for writing high-performance code. It generates GPU code from a simple high-level language and autotunes the code for specific input sizes.</p>

<p><strong>We highly recommend reading the <a href="https://research.fb.com/announcing-tensor-comprehensions/">Tensor Comprehensions blogpost</a> first.</strong></p>

<p>If you ran into any of the following scenarios, TC is a useful tool for you.</p>

<ul>
  <li>
    <p>Your PyTorch layer is large and slow, and you contemplated writing a dedicated C++ or CUDA code for it. But you don’t know how to program in CUDA or write low-level code.</p>
  </li>
  <li>
    <p>You wrote a CUDA layer, but it took a week to write, debug, optimize for speed. You wished you could do this in an hour.</p>
  </li>
  <li>
    <p>You want to fuse multiple layers like Conv-ReLU-BatchNorm or Linear-ReLU-Linear-ReLU in your network for speed, but it was quite difficult to comprehend</p>
  </li>
  <li>
    <p>Your research involves weird Tensor shapes that CuDNN and MKL are not optimized for. For example, you do convolutions of 13 x 24 with an input image of 143 x 55. You tried running it with CuDNN and it was slower than you wished.</p>
  </li>
  <li>
    <p>Your code is slowed-down by transposing Tensors constantly to fit a particular memory layout. You wish it was easy to write custom code that operates efficiently on your input layout.</p>
  </li>
</ul>

<p>Tensor Comprehensions are seamless to use in PyTorch, interoperating with PyTorch Tensors and <code class="highlighter-rouge">nn</code> Variables.</p>

<p>Let us run through using TC with PyTorch.</p>

<h4 id="1-install-the-package">1. Install the package</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda install <span class="nt">-c</span> pytorch <span class="nt">-c</span> tensorcomp tensor_comprehensions
</code></pre></div></div>

<p>At this time we only provide Linux-64 binaries which have been tested on Ubuntu 16.04 and CentOS7.</p>

<p>TC depends on heavyweight C++ projects such as <a href="http://halide-lang.org/">Halide</a>, <a href="https://github.com/wsmoses/Tapir-LLVM">Tapir-LLVM</a> and <a href="http://isl.gforge.inria.fr/">ISL</a>. Hence, we rely on Anaconda to distribute these dependencies reliably. For the same reason, TC is not available via PyPI.</p>

<h4 id="2-import-the-python-package">2. Import the python package</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensor_comprehensions</span> <span class="k">as</span> <span class="n">tc</span>
</code></pre></div></div>

<h4 id="3-define-the-tc-expression-and-create-a-python-function">3. Define the TC expression and create a python function</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lang</span> <span class="o">=</span> <span class="s">"""
def fcrelu(float(B,M) I, float(N,M) W1, float(N) B1) -&gt; (O1) {
    O1(b, n) +=! I(b, m) * W1(n, m)
    O1(b, n) = O1(b, n) + B1(n)
    O1(b, n) = fmax(O1(b, n), 0)
}
"""</span>
<span class="n">fcrelu</span> <span class="o">=</span> <span class="n">tc</span><span class="o">.</span><span class="n">define</span><span class="p">(</span><span class="n">lang</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"fcrelu"</span><span class="p">)</span>
</code></pre></div></div>

<p>This <code class="highlighter-rouge">fcrelu</code> function takes PyTorch Tensors as input and returns a PyTorch Tensor. It takes input <code class="highlighter-rouge">I</code>, weight <code class="highlighter-rouge">W1</code>, bias <code class="highlighter-rouge">B1</code> and returns output <code class="highlighter-rouge">O1</code>.</p>

<h4 id="4-lets-create-some-dummy-input-tensors">4. Let’s create some dummy input tensors</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">B</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">100</span>
<span class="n">I</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">B1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="5-now-autotune-the-function-for-your-input-sizes">5. Now autotune the function for your input sizes</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fcrelu</span><span class="o">.</span><span class="n">autotune</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">B1</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="s">"fcrelu_100_128_100.tc"</span><span class="p">)</span>
</code></pre></div></div>

<p>The autotuner is your biggest friend. You generally do not want to use a <code class="highlighter-rouge">tc</code> function without autotuning it first.</p>

<p>When the autotuning is running, the current best performance is displayed. If you are satisfied with the current result or you are out of time, stop the tuning procedure by pressing <code class="highlighter-rouge">Ctrl+C</code>.</p>

<p><img src="https://pytorch.org/static/img/tc_autotuner.gif" alt="tc-autotuner" /></p>

<p><code class="highlighter-rouge">cache</code> saves the results of the autotuned kernel search and saves it to the file <code class="highlighter-rouge">fcrelu_100_128_100.tc</code>. The next time you call the same line of code, it loads the results of the autotuning without recomputing it.</p>

<p>The autotuner has a few hyperparameters (just like your ConvNet has learning rate, number of layers, etc.). We pick reasonable defaults, but you can read about using advanced options <a href="https://facebookresearch.github.io/TensorComprehensions/framework/pytorch_integration/writing_layers.html#specifying-mapping-options">here</a>.</p>

<h4 id="6-call-the-function-with-the-inputs-to-get-your-result">6. Call the function with the inputs, to get your result</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">out</span> <span class="o">=</span> <span class="n">fcrelu</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">B1</span><span class="p">)</span>
</code></pre></div></div>

<p>Now, let’s look at how to write TC expressions.</p>

<h2 id="a-quick-primer-on-the-tc-language">A quick primer on the TC language</h2>

<p>The TC notation focuses on the mathematical nature of the layer, leaving performance considerations to it’s backend code that uses Halide and polyhedral compilation techniques which accumulate decades of cutting edge Loop Nest Optimization (LNO) research.</p>

<p>TC is close to <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html">np.einsum</a>. We shall quickly learn TC by example</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lang</span> <span class="o">=</span> <span class="s">"""
def matmul(float(M,N) A, float(N,K) B) -&gt; (output) {
  output(i, j) +=! A(i, kk) * B(kk, j)
}
"""</span>
</code></pre></div></div>

<p>In this example, we define a function <code class="highlighter-rouge">matmul</code> which takes two input <code class="highlighter-rouge">A</code> and <code class="highlighter-rouge">B</code> of shapes <code class="highlighter-rouge">M x N</code> and <code class="highlighter-rouge">N x K</code> and returns a single <code class="highlighter-rouge">output</code>. The shape of <code class="highlighter-rouge">output</code> is automatically inferred by the TC language (discussed below).</p>

<p>Let’s look at this line:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">output</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="o">+=</span><span class="err">!</span> <span class="n">A</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">kk</span><span class="p">)</span> <span class="o">*</span> <span class="n">B</span><span class="p">(</span><span class="n">kk</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
</code></pre></div></div>

<p>It says:</p>

<ul>
  <li><code class="highlighter-rouge">output(i, j)</code> means output is 2D.</li>
  <li>for each location <code class="highlighter-rouge">output(i, j)</code>, we add (<code class="highlighter-rouge">+=</code>) <code class="highlighter-rouge">A(i, kk) * B(kk, j)</code>.</li>
  <li><code class="highlighter-rouge">i</code> is well-defined as all locations in <code class="highlighter-rouge">A</code> dim=0, i.e. <code class="highlighter-rouge">i in range(0, M)</code></li>
  <li><code class="highlighter-rouge">j</code> is well-defined as all locations in <code class="highlighter-rouge">B</code> dim=1, i.e. <code class="highlighter-rouge">j in range(0, K)</code></li>
  <li><code class="highlighter-rouge">kk</code> is inferred as all locations from <code class="highlighter-rouge">0</code> to <code class="highlighter-rouge">N</code></li>
</ul>

<p>The shape of output is inferred from the maximum values <code class="highlighter-rouge">i</code> and <code class="highlighter-rouge">j</code> can take, which is <code class="highlighter-rouge">M</code> and <code class="highlighter-rouge">K</code>, so output is of size <code class="highlighter-rouge">M x K</code>.</p>

<p>The <code class="highlighter-rouge">!</code> symbol initializes output with <code class="highlighter-rouge">0.0</code>. It is equivalent to:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">output</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">output</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="o">+=</span> <span class="n">A</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">kk</span><span class="p">)</span> <span class="o">*</span> <span class="n">B</span><span class="p">(</span><span class="n">kk</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Scalar inputs and range constraints: implement AvgPool2d</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""

def avgpool(float(B, C, H, W) input) -&gt; (output) {{
  output(b, c, h, w) += input(b, c, h * {sH} + kh, w * {sW} + kw) where kh in 0:{kH}, kw in 0:{kW}
}}

"""</span>
<span class="n">avgpool</span> <span class="o">=</span> <span class="n">tc</span><span class="o">.</span><span class="n">define</span><span class="p">(</span><span class="n">LANG</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"avgpool"</span><span class="p">,</span> <span class="n">constants</span><span class="o">=</span><span class="p">{</span><span class="s">"sH"</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="s">"sW"</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="s">"kH"</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="s">"kW"</span><span class="p">:</span><span class="mi">2</span><span class="p">})</span>
</code></pre></div></div>

<p>here the <code class="highlighter-rouge">where</code> keyword can take ranges of values to operate on. <code class="highlighter-rouge">0:{kH}</code> is equivalent <code class="highlighter-rouge">range(kH)</code> in Python.</p>

<p>Note: the syntax for passing in scalars is subject to change in the next release.</p>

<h2 id="torchnn-layers">torch.nn layers</h2>

<p>We added some sugar-coating around the basic PyTorch integration of TC to make it easy to integrate TC into larger <code class="highlighter-rouge">torch.nn</code> models by defining the forward and backward TC expressions and taking <code class="highlighter-rouge">Variable</code> inputs / outputs. Here is an <a href="https://github.com/facebookresearch/TensorComprehensions/blob/master/test_python/layers/test_convolution_train.py">example</a> of defining a convolution layer with TC.</p>

<h2 id="some-essentials-that-you-will-miss-were-working-on-them">Some essentials that you will miss (we’re working on them)</h2>

<h3 id="autotuning-for-variable-length-sequences">Autotuning for variable-length sequences</h3>

<p>The TC auto-tuner requires all input sizes to be specified before-hand. For example, if you have input <code class="highlighter-rouge">I1</code> which is an image batch, the autotuner wants to know the exact shape of <code class="highlighter-rouge">I1</code> to generate an optimized kernel. You cannot specify: <code class="highlighter-rouge">image with height between 200 and 300</code>. This is more essential in sequence data such as NLP, where each sentence can have a different length.</p>

<p>The reason why the autotuner is non-parametric is because it’s harder and harder to auto-tune parametric constraints, this is active research. Hence, for the first release, we made a conscious decision to give you the tool in a form where we know it works well.</p>

<p>As a work-around, if you know that you have a few specific shapes of interest, you can run the autotuner with these multiple shapes.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">relu</span> <span class="o">=</span> <span class="n">tc</span><span class="o">.</span><span class="n">define</span><span class="p">(</span><span class="n">LANG</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"relu"</span><span class="p">)</span>
<span class="n">batch</span><span class="p">,</span> <span class="n">channels</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span>
<span class="n">tc</span><span class="o">.</span><span class="n">autotune</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span> <span class="c"># image of size 32 x 32</span>
<span class="n">tc</span><span class="o">.</span><span class="n">autotune</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">48</span><span class="p">))</span> <span class="c"># image of size 48 x 48</span>
<span class="n">tc</span><span class="o">.</span><span class="n">autotune</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span> <span class="c"># image of size 64 x 64</span>
</code></pre></div></div>

<p>Now the autotuner is tuned for these three specific image sizes <code class="highlighter-rouge">32x32</code>, <code class="highlighter-rouge">48x48</code> and <code class="highlighter-rouge">64x64</code>.</p>

<h3 id="lack-of-loops">Lack of loops</h3>

<p>If you want to write an RNN, it’s easy to see it as a <code class="highlighter-rouge">for</code> loop over time. However, the TC language does not have loops yet. If you reallly want to write RNNs, you can write unrolled loops.</p>

<h3 id="strided-tensors">Strided-Tensors</h3>

<p>The TC backend does not support non-contiguous Tensors yet. If the inputs you give are not contiguous, they are made contiguous before passing to the TC backend.</p>

<h3 id="reshaping-tensors-within-a-tc-expression">Reshaping Tensors within a TC expression</h3>

<p>You cannot write this operation in TC: <code class="highlighter-rouge">torch.matmul(...).view(...).mean(...)</code>. Whenever there is need for a <code class="highlighter-rouge">view</code> to change the shape of an input, you have to get the output, <code class="highlighter-rouge">view</code> it at the PyTorch level.</p>

<h2 id="getting-started">Getting Started</h2>

<ul>
  <li><a href="https://facebookresearch.github.io/TensorComprehensions/framework/pytorch_integration/writing_layers.html">Walk through Tutorial</a> to quickly get started with understanding and using Tensor Comprehensions PyTorch package.</li>
  <li><a href="https://github.com/facebookresearch/TensorComprehensions/tree/master/test_python/layers">Over 20 examples</a> of various ML layers with TC, including <code class="highlighter-rouge">avgpool</code>, <code class="highlighter-rouge">maxpool</code>, <code class="highlighter-rouge">matmul</code>, matmul - give output buffers and <code class="highlighter-rouge">batch-matmul</code>, <code class="highlighter-rouge">convolution</code>, <code class="highlighter-rouge">strided-convolution</code>, <code class="highlighter-rouge">batchnorm</code>, <code class="highlighter-rouge">copy</code>, <code class="highlighter-rouge">cosine similarity</code>, <code class="highlighter-rouge">Linear</code>, <code class="highlighter-rouge">Linear + ReLU</code>, <code class="highlighter-rouge">group-convolutions</code>, strided <code class="highlighter-rouge">group-convolutions</code>, <code class="highlighter-rouge">indexing</code>, <code class="highlighter-rouge">Embedding</code> (lookup table), small-mobilenet, <code class="highlighter-rouge">softmax</code>, <code class="highlighter-rouge">tensordot</code>, <code class="highlighter-rouge">transpose</code></li>
  <li><a href="https://facebookresearch.github.io/TensorComprehensions/framework/pytorch_integration/getting_started.html">Detailed docs</a> on Tensor Comprehensions and integration with PyTorch.</li>
</ul>

<h2 id="communication">Communication</h2>

<ul>
  <li><a href="https://tensorcomprehensions.herokuapp.com/">Slack</a>: For discussion around framework integration, build support, collaboration, etc. join our slack channel.</li>
  <li>Email: tensorcomp@fb.com</li>
  <li><a href="https://github.com/facebookresearch/TensorComprehensions">GitHub</a>: bug reports, feature requests, install issues, RFCs, thoughts, etc.</li>
</ul>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>We would like to thank Soumith Chintala, <a href="https://github.com/ezyang">Edward Yang</a> and <a href="https://github.com/colesbury">Sam Gross</a> for their immense guidance and help in making the integration API nice and smooth. We would also like to thank rest of the PyTorch team and our pre-release users for their helpful feedback that guided us in making the integration better.</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
        </ul>
      </div>

      <div class="footer-links-col follow-us-col">
        <ul>
          <li class="list-title">Stay Connected</li>
          <li>
            
            <div id="mc_embed_signup">
  <form
    action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
    method="post"
    id="mc-embedded-subscribe-form"
    name="mc-embedded-subscribe-form"
    class="email-subscribe-form validate"
    target="_blank"
    novalidate>
    <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
      <div class="mc-field-group">
        <label for="mce-EMAIL" style="display:none;">Email Address</label>
        <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
      </div>

      <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
      </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

      <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

      <div class="clear">
        <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
      </div>
    </div>
  </form>
</div>

          </li>
        </ul>

        <div class="footer-social-icons">
          <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
          <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
        </div>
      </div>
    </div>
  </div>
</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
  <ul>
    <li class="">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="">
      <a href="/features">Features</a>
    </li>

    <li class="">
      <a href="/ecosystem">Ecosystem</a>
    </li>

    <li class="active">
      <a href="/blog">Blog</a>
    </li>

    <li>
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li>
      <a href="/docs">Docs</a>
    </li>

    <li class="">
      <a href="/resources">Resources</a>
    </li>

    <li>
      <a href="https://github.com/pytorch/pytorch">Github</a>
    </li>
  </ul>
</div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top === 0) {
        $(".header-holder").css({"backgroundColor": "transparent"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>



</body>

</html>

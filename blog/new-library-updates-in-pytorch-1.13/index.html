<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      New Library Updates in PyTorch 1.13 | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="Summary

" />

  <meta property="og:image" content="https://pytorch.org/assets/images/new-library-updates-in-pytorch-1.13-2.jpg" />
  <meta name="twitter:image" content="https://pytorch.org/assets/images/new-library-updates-in-pytorch-1.13-2.jpg" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="New Library Updates in PyTorch 1.13" />
<meta property="og:description" content="Summary

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="New Library Updates in PyTorch 1.13" />
<meta name="twitter:description" content="Summary

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>
    <div class="hello-bar">
        <div class="container">
          Join us in Silicon Valley September 18-19 at the 2024 PyTorch Conference. <a target="_blank" href="https://events.linuxfoundation.org/pytorch-conference/">Learn more</a>.
        </div>
      </div>
      
    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Learn
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/get-started">
            <span class=dropdown-title>Get Started</span>
            <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/">
            <span class="dropdown-title">Tutorials</span>
            <p>Whats new in PyTorch tutorials</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
            <span class="dropdown-title">Learn the Basics</span>
            <p>Familiarize yourself with PyTorch concepts and modules</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
            <span class="dropdown-title">PyTorch Recipes</span>
            <p>Bite-size, ready-to-deploy PyTorch code examples</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
            <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
            <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/contributor-awards-2023">
            <span class="dropdown-title">Contributor Awards - 2023</span>
            <p>Award winners announced at this year's PyTorch Conference</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
            <p>Build innovative and privacy-aware AI experiences for edge devices</p>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
            <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://pytorch.org/docs">
            <span class="dropdown-title">PyTorch</span>
            <p>Explore the documentation for comprehensive guidance on how to use PyTorch.</p>
          </a>
          <a class="nav-dropdown-item" href="/pytorch-domains">
            <span class="dropdown-title">PyTorch Domains</span>
            <p> Read the PyTorch Domains documentation to learn more about domain-specific libraries.</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Blog & News
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/blog">
            <span class="dropdown-title">PyTorch Blog</span>
            <p>Catch up on the latest technical news and happenings</p>
          </a>
          <a class="nav-dropdown-item" href="/community-blog">
            <span class="dropdown-title">Community Blog</span>
            <p>Stories from the PyTorch ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/videos">
            <span class="dropdown-title">Videos</span>
            <p>Learn about the latest PyTorch tutorials, new, and more </p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class="dropdown-title">Community Stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          About
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/governing-board">
            <span class=dropdown-title>Governing Board</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <a href="/join" data-cta="join">
        Become a Member
      </a>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub">
        <div id="topnav-gh-icon"></div>
      </a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">October 28, 2022</p>
            <h1>
                <a class="blog-title">New Library Updates in PyTorch 1.13</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Team PyTorch
                      
                    </p>
                    <h2 id="summary">Summary</h2>

<p>We are bringing a number of improvements to the current PyTorch libraries, alongside the PyTorch 1.13 <a href="https://github.com/pytorch/pytorch/releases">release</a>. These updates demonstrate our focus on developing common and extensible APIs across all domains to make it easier for our community to build ecosystem projects on PyTorch.</p>

<p>Along with <strong>1.13</strong>, we are releasing updates to the PyTorch Libraries, please find them below.</p>

<h3 id="torchaudio">TorchAudio</h3>

<h4 id="beta-hybrid-demucs-model-and-pipeline">(Beta) Hybrid Demucs Model and Pipeline</h4>

<p>Hybrid Demucs is a music source separation model that uses both spectrogram and time domain features. It has demonstrated state-of-the-art performance in the Sony<sup>®</sup> Music DeMixing Challenge. (citation: <a href="https://arxiv.org/abs/2111.03600">https://arxiv.org/abs/2111.03600</a>)</p>

<p>The TorchAudio v0.13 release includes the following features</p>

<ul>
  <li>MUSDB_HQ Dataset, which is used in Hybrid Demucs training (<a href="https://pytorch.org/audio/0.13.0/generated/torchaudio.datasets.MUSDB_HQ.html#torchaudio.datasets.MUSDB_HQ">docs</a>)</li>
  <li>Hybrid Demucs model architecture (<a href="https://pytorch.org/audio/0.13.0/generated/torchaudio.models.HDemucs.html#torchaudio.models.HDemucs">docs</a>)</li>
  <li>Three factory functions suitable for different sample rate ranges</li>
  <li>Pre-trained pipelines (<a href="https://pytorch.org/audio/0.13.0/pipelines.html#id46">docs</a>)</li>
  <li>SDR Results of pre-trained pipelines on MUSDB_HQ test set</li>
  <li>Tutorial that steps through music source separation using the pretrained pipeline (<a href="https://pytorch.org/audio/0.13.0/tutorials/hybrid_demucs_tutorial.html">docs</a>)</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Pipeline</th>
      <th>All</th>
      <th>Drums</th>
      <th>Bass</th>
      <th>Other</th>
      <th>Vocals</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>HDEMUCS_HIGH_MUSDB*</em></td>
      <td>6.42</td>
      <td>7.76</td>
      <td>6.51</td>
      <td>4.47</td>
      <td>6.93</td>
    </tr>
    <tr>
      <td><em>HDEMUCS_HIGH_MUSDB_PLUS**</em></td>
      <td>9.37</td>
      <td>11.38</td>
      <td>10.53</td>
      <td>7.24</td>
      <td>8.32</td>
    </tr>
  </tbody>
</table>

<p><small>* Trained on the training data of MUSDB-HQ dataset.<br />** Trained on both training and test sets of MUSDB-HQ and 150 extra songs from an internal database that were specifically produced for Meta.</small></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchaudio.pipelines</span> <span class="kn">import</span> <span class="n">HDEMUCS_HIGH_MUSDB_PLUS</span>

<span class="n">bundle</span> <span class="o">=</span> <span class="n">HDEMUCS_HIGH_MUSDB_PLUS</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">bundle</span><span class="p">.</span><span class="n">get_model</span><span class="p">()</span>
<span class="n">sources_list</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">sources</span>

<span class="n">mixture</span><span class="p">,</span> <span class="n">samplerate</span> <span class="o">=</span> <span class="n">torchaudio</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"song.wav"</span><span class="p">)</span>
<span class="n">sources</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">mixture</span><span class="p">)</span>
<span class="n">audios</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">sources_list</span><span class="p">,</span> <span class="n">sources</span><span class="p">)</span>
</code></pre></div></div>

<p>Special thanks to Alexandre Defossez for the guidance.</p>

<h4 id="beta-datasets-and-metadata-mode-for-superb-benchmark">(Beta) Datasets and Metadata Mode for SUPERB Benchmark</h4>

<p>TorchAudio adds support for various audio-related datasets used in downstream tasks for benchmarking self-supervised learning models. With the addition of several new datasets, there is now support for the downstream tasks in version 1 of the <a href="https://superbbenchmark.org/">SUPERB benchmark</a>, which can be found in the <a href="https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/docs/superb.md">s3prl repository</a>.</p>

<p>For these datasets, we also add metadata support through a <code class="language-plaintext highlighter-rouge">get_metadata</code> function, enabling faster dataset iteration or preprocessing without the need to load waveforms. The function returns the same features as <code class="language-plaintext highlighter-rouge">__getitem__</code>, except it returns the relative waveform path rather than the loaded waveform.</p>

<p>Datasets with metadata functionality</p>

<ul>
  <li>LIBRISPEECH (<a href="https://pytorch.org/audio/0.13.0/generated/torchaudio.datasets.LIBRISPEECH.html#torchaudio.datasets.LIBRISPEECH">docs</a>)</li>
  <li>LibriMix (<a href="https://pytorch.org/audio/0.13.0/generated/torchaudio.datasets.LibriMix.html#torchaudio.datasets.LibriMix">docs</a>)</li>
  <li>QUESST14 (<a href="https://pytorch.org/audio/0.13.0/generated/torchaudio.datasets.QUESST14.html#torchaudio.datasets.QUESST14">docs</a>)</li>
  <li>SPEECHCOMMANDS (<a href="https://pytorch.org/audio/0.13.0/generated/torchaudio.datasets.SPEECHCOMMANDS.html#torchaudio.datasets.SPEECHCOMMANDS">docs</a>)</li>
  <li>(new) FluentSpeechCommands (<a href="https://pytorch.org/audio/0.13.0/generated/torchaudio.datasets.FluentSpeechCommands.html#torchaudio.datasets.FluentSpeechCommands">docs</a>)</li>
  <li>(new) Snips (<a href="https://pytorch.org/audio/0.13.0/generated/torchaudio.datasets.Snips.html#torchaudio.datasets.Snips">docs</a>)</li>
  <li>(new) IEMOCAP (<a href="https://pytorch.org/audio/0.13.0/generated/torchaudio.datasets.IEMOCAP.html#torchaudio.datasets.IEMOCAP">docs</a>)</li>
  <li>(new) VoxCeleb1 (<a href="https://pytorch.org/audio/0.13.0/generated/torchaudio.datasets.VoxCeleb1Identification.html#torchaudio.datasets.VoxCeleb1Identification">Identification</a>, <a href="https://pytorch.org/audio/0.13.0/generated/torchaudio.datasets.VoxCeleb1Verification.html#torchaudio.datasets.VoxCeleb1Verification">Verification</a>)</li>
</ul>

<h4 id="beta-custom-language-model-support-in-ctc-beam-search-decoding">(Beta) Custom Language Model support in CTC Beam Search Decoding</h4>

<p>TorchAudio released a CTC beam search decoder in release 0.12, with KenLM language model support. This release, there is added functionality for creating custom Python language models that are compatible with the decoder, using the <code class="language-plaintext highlighter-rouge">torchaudio.models.decoder.CTCDecoderLM</code> wrapper.</p>

<p>For more information on using a custom language model, please refer to the <a href="https://pytorch.org/audio/0.13.0/generated/torchaudio.models.decoder.CTCDecoder.html#ctcdecoderlm">documentation</a> and <a href="https://pytorch.org/audio/0.13.0/tutorials/asr_inference_with_ctc_decoder_tutorial.html#custom-language-model">tutorial</a>.</p>

<h4 id="beta-streamwriter">(Beta) StreamWriter</h4>

<p>torchaudio.io.StreamWriter is a class for encoding media including audio and video. This can handle a wide variety of codecs, chunk-by-chunk encoding and GPU encoding.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">writer</span> <span class="o">=</span> <span class="n">StreamWriter</span><span class="p">(</span><span class="s">"example.mp4"</span><span class="p">)</span>
<span class="n">writer</span><span class="p">.</span><span class="n">add_audio_stream</span><span class="p">(</span>
    <span class="n">sample_rate</span><span class="o">=</span><span class="mi">16_000</span><span class="p">,</span>
    <span class="n">num_channels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">writer</span><span class="p">.</span><span class="n">add_video_stream</span><span class="p">(</span>
    <span class="n">frame_rate</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">96</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s">"rgb24"</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">writer</span><span class="p">.</span><span class="nb">open</span><span class="p">():</span>
    <span class="n">writer</span><span class="p">.</span><span class="n">write_audio_chunk</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">audio</span><span class="p">)</span>
    <span class="n">writer</span><span class="p">.</span><span class="n">write_video_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">video</span><span class="p">)</span>
</code></pre></div></div>

<p>For more information, refer to <a href="https://pytorch.org/audio/0.13.0/generated/torchaudio.io.StreamWriter.html">the documentation</a> and the following tutorials</p>
<ul>
  <li><a href="https://pytorch.org/audio/0.13.0/tutorials/streamwriter_basic_tutorial.html">StreamWriter Basic Usage</a></li>
  <li><a href="https://pytorch.org/audio/0.13.0/tutorials/streamwriter_advanced.html">StreamWriter Advanced Usage</a></li>
  <li><a href="https://pytorch.org/audio/0.13.0/hw_acceleration_tutorial.html">Hardware-Accelerated Video Decoding and Encoding</a></li>
</ul>

<h3 id="torchdata">TorchData</h3>

<p>For a complete list of changes and new features, please visit <a href="https://github.com/pytorch/data/releases">our repository’s 0.5.0 release note</a>.</p>

<h4 id="prototype-dataloader2">(Prototype) DataLoader2</h4>

<p><code class="language-plaintext highlighter-rouge">DataLoader2</code> was introduced in the last release to execute <code class="language-plaintext highlighter-rouge">DataPipe</code> graph, with support for dynamic sharding for multi-process/distributed data loading, multiple backend ReadingServices, and <code class="language-plaintext highlighter-rouge">DataPipe</code> graph in-place modification (e.g. shuffle control).</p>

<p>In this release, we further consolidated the API for <code class="language-plaintext highlighter-rouge">DataLoader2</code> and a <a href="https://pytorch.org/data/0.5/dataloader2.html">detailed documentation is now available here</a>. We continue to welcome early adopters and feedback, as well as potential contributors. If you are interested in trying it out, we encourage you to install the nightly version of TorchData.</p>

<h4 id="beta-data-loading-from-cloud-service-providers">(Beta) Data Loading from Cloud Service Providers</h4>

<p>We extended our support to load data from additional cloud storage providers via DataPipes, now covering AWS, Google Cloud Storage, and Azure. A <a href="https://pytorch.org/data/0.5/tutorial.html#working-with-cloud-storage-providers">tutorial is also available</a>. We are open to feedback and feature requests.</p>

<p>We also performed a simple benchmark, comparing the performance of data loading from AWS S3 and attached volume on an AWS EC2 instance.</p>

<h3 id="torchdeploy-beta">torch::deploy (Beta)</h3>

<p>torch::deploy is now in Beta! torch::deploy is a C++ library for Linux based operating systems that allows you to run multiple Python interpreters in a single process. You can run your existing eager PyTorch models without any changes for production inference use cases. Highlights include:</p>

<ul>
  <li>Existing models work out of the box–no need to modify your python code to support tracing.</li>
  <li>Full support for your existing Python environment including C extensions.</li>
  <li>No need to cross process boundaries to load balance in multi-GPU serving environments.</li>
  <li>Model weight can be shared between multiple Python interpreters.</li>
  <li>A vastly improved installation and setup process.</li>
</ul>

<pre><code class="language-Python">torch::deploy::InterpreterManager manager(4);

// access one of the 4 interpreters
auto I = manager.acquireOne();

// run infer from your_model.py
I.global("your_model", "infer")({at::randn({10, 240, 320})});
</code></pre>

<p>Learn more <a href="https://github.com/pytorch/multipy">here</a>.</p>

<h4 id="beta-cudarocmcpu-backends">(Beta) CUDA/ROCm/CPU Backends</h4>

<p>torch::deploy now links against standard PyTorch Python distributions so all accelerators that PyTorch core supports such as CUDA and AMD/HIP work out of the box.</p>

<ul>
  <li>Can install any device variant of PyTorch via pip/conda like normal.</li>
  <li><a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a></li>
</ul>

<h4 id="prototype-aarch64arm64-support">(Prototype) aarch64/arm64 support</h4>

<p>torch::deploy now has basic support for aarch64 Linux systems.</p>

<ul>
  <li>We’re looking to gather feedback on it and learn more about arm use cases for eager PyTorch models.</li>
  <li>Learn more / share your use case at <a href="https://github.com/pytorch/multipy/issues/64">https://github.com/pytorch/multipy/issues/64</a></li>
</ul>

<h3 id="torcheval">TorchEval</h3>

<h4 id="prototype-introducing-native-metrics-support-for-pytorch">(Prototype) Introducing Native Metrics Support for PyTorch</h4>

<p>TorchEval is a library built for users who want highly performant implementations of common metrics to evaluate machine learning models. It also provides an easy to use interface for building custom metrics with the same toolkit. Building your metrics with TorchEval makes running distributed training loops with <a href="https://pytorch.org/docs/stable/distributed.html">torch.distributed</a> a breeze.</p>

<p>Learn more with our <a href="https://pytorch.org/torcheval">docs</a>, see our <a href="https://pytorch.org/torcheval/stable/metric_example.html">examples</a>, or check out our <a href="http://github.com/pytorch/torcheval">GitHub repo</a>.</p>

<h3 id="torchmultimodal-release-beta">TorchMultimodal Release (Beta)</h3>

<p>Please watch for upcoming blogs in early November that will introduce TorchMultimodal, a PyTorch domain library for training SoTA multi-task multimodal models at scale, in more details; in the meantime, play around with the library and models through our <a href="https://pytorch.org/tutorials/beginner/flava_finetuning_tutorial.html">tutorial</a>.</p>

<h3 id="torchrec">TorchRec</h3>

<h4 id="prototype-simplified-optimizer-fusion-apis">(Prototype) Simplified Optimizer Fusion APIs</h4>

<p>We’ve provided a simplified and more intuitive API for setting fused optimizer settings via apply_optimizer_in_backward. This new approach enables the ability to specify optimizer settings on a per-parameter basis and sharded modules will configure <a href="https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/fbgemm_gpu/split_table_batched_embeddings_ops.py#L181">FBGEMM’s TableBatchedEmbedding modules accordingly</a>. Additionally, this now let’s TorchRec’s planner account for optimizer memory usage. This should alleviate reports of sharding jobs OOMing after using Adam using a plan generated from planner.</p>

<h4 id="prototype-simplified-sharding-apis">(Prototype) Simplified Sharding APIs</h4>

<p>We’re introducing the shard API, which now allows you to shard only the embedding modules within a model, and provides an alternative to the current main entry point - DistributedModelParallel. This lets you have a finer grained control over the rest of the model, which can be useful for customized parallelization logic, and inference use cases (which may not require any parallelization on the dense layers). We’re also introducing construct_module_sharding_plan, providing a simpler interface to the TorchRec sharder.</p>

<h4 id="beta-quantized-comms">(Beta) Quantized Comms</h4>

<p>Applying <a href="https://dlp-kdd.github.io/assets/pdf/a11-yang.pdf">quantization or mixed precision</a> to tensors in a collective call during model parallel training greatly improves training efficiency, with little to no effect on model quality. TorchRec now integrates with the <a href="https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/fbgemm_gpu/quantize_comm.py">quantized comms library provided by FBGEMM GPU</a> and provides an interface to construct encoders and decoders (codecs) that surround the all_to_all, and reduce_scatter collective calls in the output_dist of a sharded module. We also allow you to construct your own codecs to apply to your sharded module. The codces provided by FBGEMM allow FP16, BF16, FP8, and INT8 compressions, and you may use different quantizations for the forward pass and backward pass.</p>

<h3 id="torchsnapshot-beta">TorchSnapshot (Beta)</h3>

<p>Along with PyTorch 1.13, we are releasing the beta version of TorchSnapshot, which is a performant, memory-efficient checkpointing library for PyTorch applications, designed with large, complex distributed workloads in mind. Highlights include:</p>

<ul>
  <li>Performance: TorchSnapshot provides a fast checkpointing implementation employing various optimizations, including zero-copy serialization for most tensor types, overlapped device-to-host copy and storage I/O, parallelized storage I/O</li>
  <li>Memory Use: TorchSnapshot’s memory usage adapts to the host’s available resources, greatly reducing the chance of out-of-memory issues when saving and loading checkpoints</li>
  <li>Usability: Simple APIs that are consistent between distributed and non-distributed workloads</li>
</ul>

<p>Learn more with our <a href="https://pytorch.org/torchsnapshot/main/getting_started.html">tutorial</a>.</p>

<h3 id="torchvision">TorchVision</h3>

<p>We are happy to introduce torchvision v0.14 <a href="https://github.com/pytorch/vision/releases">(release note)</a>. This version introduces a new <a href="https://pytorch.org/blog/easily-list-and-initialize-models-with-new-apis-in-torchvision/">model registration API</a> to help users retrieving and listing models and weights. It also includes new image and video classification models such as MViT, S3D, Swin Transformer V2, and MaxViT. Last but not least, we also have new primitives and augmentation such as PolynomicalLR scheduler and SimpleCopyPaste.</p>

<h4 id="beta-model-registration-api">(Beta) Model Registration API</h4>

<p>Following up on the <a href="https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/">multi-weight support API</a> that was released on the previous version, we have added a new <a href="https://pytorch.org/blog/easily-list-and-initialize-models-with-new-apis-in-torchvision/">model registration API</a> to help users retrieve models and weights. There are now 4 new methods under the torchvision.models module: get_model, get_model_weights, get_weight, and list_models. Here are examples of how we can use them:</p>

<pre><code class="language-Python">import torchvision
from torchvision.models import get_model, get_model_weights, list_models


max_params = 5000000

tiny_models = []
for model_name in list_models(module=torchvision.models):
    weights_enum = get_model_weights(model_name)
    if len([w for w in weights_enum if w.meta["num_params"] &lt;= max_params]) &gt; 0:
        tiny_models.append(model_name)

print(tiny_models)
# ['mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mobilenet_v2', ...]

model = get_model(tiny_models[0], weights="DEFAULT")
print(sum(x.numel() for x in model.state_dict().values()))
# 2239188
</code></pre>

<h4 id="beta-new-video-classification-models">(Beta) New Video Classification Models</h4>

<p>We added two new video classification models, MViT and S3D. MViT is a state of the art video classification transformer model which has 80.757% accuracy on the Kinetics400 dataset, while S3D is a relatively small model with good accuracy for its size. These models can be used as follows:</p>

<pre><code class="language-Python">import torch
from torchvision.models.video import *

video = torch.rand(3, 32, 800, 600)
model = mvit_v2_s(weights="DEFAULT")
# model = s3d(weights="DEFAULT")
model.eval()
prediction = model(images)
</code></pre>

<p>Here is the table showing the accuracy of the new video classification models tested in the Kinetics400 dataset.</p>

<table>
  <thead>
    <tr>
      <th><strong>Model</strong></th>
      <th><strong>Acc@1</strong></th>
      <th><strong>Acc@5</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>mvit_v1_b</td>
      <td>81.474</td>
      <td>95.776</td>
    </tr>
    <tr>
      <td>mvit_v2_s</td>
      <td>83.196</td>
      <td>96.36</td>
    </tr>
    <tr>
      <td>s3d</td>
      <td>83.582</td>
      <td>96.64</td>
    </tr>
  </tbody>
</table>

<p>We would like to thank Haoqi Fan, Yanghao Li, Christoph Feichtenhofer and Wan-Yen Lo for their work on <a href="https://github.com/facebookresearch/pytorchvideo/">PyTorchVideo</a> and their support during the development of the MViT model. We would like to thank Sophia Zhi for her contribution implementing the S3D model in torchvision.</p>

<h4 id="stable-new-architecture-and-model-variants">(Stable) New Architecture and Model Variants</h4>

<p>For Classification Models, we’ve added the Swin Transformer V2 architecture along with pre-trained weights for its tiny/small/base variants. In addition, we have added support for the MaxViT transformer. Here is an example on how to use the models:</p>

<pre><code class="language-Python">import torch
from torchvision.models import *

image = torch.rand(1, 3, 224, 224)
model = swin_v2_t(weights="DEFAULT").eval()
# model = maxvit_t(weights="DEFAULT").eval()
prediction = model(image)
</code></pre>

<p>Here is the table showing the accuracy of the models tested on ImageNet1K dataset.</p>

<table>
  <thead>
    <tr>
      <th><strong>Model</strong></th>
      <th><strong>Acc@1</strong></th>
      <th><strong>Acc@1 change over V1</strong></th>
      <th><strong>Acc@5</strong></th>
      <th><strong>Acc@5 change over V1</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>swin_v2_t</td>
      <td>82.072</td>
      <td>+ 0.598</td>
      <td>96.132</td>
      <td>+ 0.356</td>
    </tr>
    <tr>
      <td>swin_v2_s</td>
      <td>83.712</td>
      <td>+ 0.516</td>
      <td>96.816</td>
      <td>+ 0.456</td>
    </tr>
    <tr>
      <td>swin_v2_b</td>
      <td>84.112</td>
      <td>+ 0.530</td>
      <td>96.864</td>
      <td>+ 0.224</td>
    </tr>
    <tr>
      <td>maxvit_t</td>
      <td>83.700</td>
      <td>-</td>
      <td>96.722</td>
      <td>-</td>
    </tr>
  </tbody>
</table>

<p>We would like to thank <a href="https://github.com/ain-soph">Ren Pang</a> and <a href="https://github.com/TeodorPoncu">Teodor Poncu</a> for contributing the 2 models to torchvision.</p>

<h3 id="stable-new-primitives--augmentations">(Stable) New Primitives &amp; Augmentations</h3>

<p>In this release we’ve added the <a href="https://arxiv.org/abs/2012.07177">SimpleCopyPaste</a> augmentation in our reference scripts and we up-streamed the PolynomialLR scheduler to PyTorch Core. We would like to thank <a href="https://github.com/lezwon">Lezwon Castelino</a> and <a href="https://github.com/federicopozzi33">Federico Pozzi</a> for their contributions. We are continuing our efforts to modernize TorchVision by adding more SoTA primitives, Augmentations and architectures with the help of our community. If you are interested in contributing, have a look at the following <a href="https://github.com/pytorch/vision/issues/6323">issue</a>.</p>

<h3 id="torch-tensorrt">Torch-TensorRT</h3>

<h4 id="prototype-tensorrt-with-fx2trt-frontend">(Prototype) TensorRT with FX2TRT frontend</h4>

<p>Torch-TensorRT is the PyTorch integration for TensorRT, providing high performance inference on NVIDIA GPUs. Torch-TRT allows for optimizing models directly in PyTorch for deployment providing up to 6x performance improvement.</p>

<p>Torch-TRT is an AoT compiler which ingests an nn.Module or TorchScript module, optimizes compatible subgraphs in TensorRT &amp; leaves the rest to run in PyTorch. This gives users the performance of TensorRT, but the usability and familiarity of Torch.</p>

<p>Torch-TensorRT is part of the PyTorch ecosystem, and was released as v1.0 in November ‘21. There are currently two distinct front-ends: Torchscript &amp; FX. Each provides the same value proposition and underlying operation with the primary difference being the input &amp; output formats (TS vs FX / Python).</p>

<p>The Torchscript front-end was included in v1.0 and should be considered stable. The FX front-end is first released in v1.2 and should be considered a Beta.</p>

<p>Relevant Links:</p>

<ul>
  <li><a href="https://github.com/pytorch/TensorRT">Github</a></li>
  <li><a href="https://pytorch.org/TensorRT/">Documentation</a></li>
  <li><a href="https://pytorch.org/TensorRT/getting_started/getting_started_with_python_api.html">Generic (TS) getting started guide</a></li>
  <li><a href="https://pytorch.org/TensorRT/tutorials/getting_started_with_fx_path.html">FX getting started guide</a></li>
</ul>

<h4 id="stable--introducing-torch-tensorrt">(Stable)  Introducing Torch-TensorRT</h4>

<p>Torch-TensorRT is an integration for PyTorch that leverages inference optimizations of TensorRT on NVIDIA GPUs. It takes advantage of TensorRT optimizations, such as FP16 and INT8 reduced precision, graph optimization, operation fusion, etc. while offering a fallback to native PyTorch when TensorRT does not support the model subgraphs. Currently, there are two frontend paths existing in the library that help to convert a PyTorch model to tensorRT engine. One path is through Torch Script (TS) and the other is through FX frontend. That being said, the models are traced by either TS or FX into their IR graph and then converted to TensorRT from it.</p>

<p>Learn more with our <a href="https://pytorch.org/TensorRT/">tutorial</a>.</p>

<h3 id="torchx">TorchX</h3>

<p>TorchX 0.3 updates include a new list API, experiment tracking, elastic training and improved scheduler support. There’s also a new Multi-Objective NAS <a href="https://pytorch.org/tutorials/intermediate/ax_multiobjective_nas_tutorial.html">tutorial</a> using TorchX + Ax.</p>

<h4 id="prototype-list">(Prototype) List</h4>

<p>The newly added list command and API allows you to list recently launched jobs and their statuses for a given scheduler directly from within TorchX.</p>

<ul>
  <li>This removes the need for using secondary tools to list the jobs.</li>
  <li>Full programmatic access to recent jobs for integration with custom tools.</li>
</ul>

<pre><code class="language-Python">$ torchx list -s kubernetes
APP HANDLE                                                       APP STATUS
-----------------------------------------------            -----------------
kubernetes://torchx/default:train-f2nx4459p5crr   SUCCEEDED
</code></pre>

<p>Learn more with our <a href="https://pytorch.org/torchx/main/schedulers.html#torchx.schedulers.Scheduler.list">documentation</a>.</p>

<h4 id="prototype-tracker">(Prototype) Tracker</h4>

<p>TorchX Tracker is a new prototype library that provides a flexible and customizable experiment and artifact tracking interface. This allows you to track inputs and outputs for jobs across multiple steps to make it easier to use TorchX with pipelines and other external systems.</p>

<pre><code class="language-Python">from torchx import tracker

app_run = tracker.app_run_from_env()
app_run.add_metadata(lr=lr, gamma=gamma) # hyper parameters
app_run.add_artifact("model", "storage://path/mnist_cnn.pt") # logs / checkpoints
app_run.add_source(parent_run_id, "model") # lineage
</code></pre>

<p>Example:</p>

<ul>
  <li><a href="https://github.com/pytorch/torchx/tree/main/torchx/examples/apps/tracker">https://github.com/pytorch/torchx/tree/main/torchx/examples/apps/tracker</a></li>
  <li><a href="https://pytorch.org/torchx/main/tracker.html">https://pytorch.org/torchx/main/tracker.html</a></li>
</ul>

<h4 id="prototype-elastic-training-and-autoscaling">(Prototype) Elastic Training and Autoscaling</h4>

<p>Elasticity on Ray and Kubernetes – automatic scale up of distributed training jobs when using a supported scheduler. Learn more with our <a href="https://pytorch.org/torchx/main/components/distributed.html">documentation</a>.</p>

<h4 id="prototype-scheduler-improvements-ibm-spectrum-lsf">(Prototype) Scheduler Improvements: IBM® Spectrum LSF</h4>

<p>Added prototype support for the IBM Spectrum LSF scheduler.</p>

<h4 id="beta-aws-batch-scheduler">(Beta) AWS Batch Scheduler</h4>

<p>The AWS Batch scheduler integration is now in beta.</p>

<ul>
  <li>log fetching and listing jobs is now supported.</li>
  <li>Added configs for job priorities and queue policies</li>
  <li>Easily access job UI via ui_url
<a href="https://pytorch.org/torchx/main/schedulers/aws_batch.html">https://pytorch.org/torchx/main/schedulers/aws_batch.html</a></li>
</ul>

<h4 id="prototype-anyprecision-optimizer">(Prototype) AnyPrecision Optimizer</h4>

<p>Drop in replacement for AdamW optimizer that reduces GPU memory, enables two main features:</p>

<ul>
  <li>Ability to successfully train the entire model pipeline in full BFloat16.
Kahan summation ensures precision.  This can improve training throughput, especially on huge models, by reduced memory and increased computation speed.</li>
  <li>Ability to change the variance state to BFloat16.  This can reduce overall memory required for model training with additional speed improvements.</li>
</ul>

<p>Find more information <a href="https://github.com/pytorch/torchdistx/pull/52">here</a>.</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="lf-grid">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org" class="footer-logo">
          <img src="/assets/images/logo-icon.svg" alt="PyTorch logo" width="40">
        </a>
      </div>

      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook"><path fill="currentColor" d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z"/></svg>	
        </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X"><path fill="currentColor" d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66"/></svg>
        </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube"><path fill="currentColor" d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z"/></svg>	
        </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn"><rect width="512" height="512" rx="0" fill="currentColor"/><circle fill="#000" cx="142" cy="138" r="37"/><path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198"/><path fill="#000" d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
        </a></li>
        <li><a href="https://social.lfx.dev/@pytorch" rel="me" target="_blank" title="PyTorch on Mastodon">
          <svg fill="currentColor" aria-label="Mastodon" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" xml:space="preserve"><path d="M21.327 8.566c0-4.339-2.843-5.61-2.843-5.61-1.433-.658-3.894-.935-6.451-.956h-.063c-2.557.021-5.016.298-6.45.956 0 0-2.843 1.272-2.843 5.61 0 .993-.019 2.181.012 3.441.103 4.243.778 8.425 4.701 9.463 1.809.479 3.362.579 4.612.51 2.268-.126 3.541-.809 3.541-.809l-.075-1.646s-1.621.511-3.441.449c-1.804-.062-3.707-.194-3.999-2.409a4.523 4.523 0 0 1-.04-.621s1.77.433 4.014.536c1.372.063 2.658-.08 3.965-.236 2.506-.299 4.688-1.843 4.962-3.254.434-2.223.398-5.424.398-5.424zm-3.353 5.59h-2.081V9.057c0-1.075-.452-1.62-1.357-1.62-1 0-1.501.647-1.501 1.927v2.791h-2.069V9.364c0-1.28-.501-1.927-1.502-1.927-.905 0-1.357.546-1.357 1.62v5.099H6.026V8.903c0-1.074.273-1.927.823-2.558.566-.631 1.307-.955 2.228-.955 1.065 0 1.872.409 2.405 1.228l.518.869.519-.869c.533-.819 1.34-1.228 2.405-1.228.92 0 1.662.324 2.228.955.549.631.822 1.484.822 2.558v5.253z"/></svg>
        </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/legal/policies/">Linux Foundation Policies</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">LF Projects, LLC Policies</a>. <a href="https://www.linuxfoundation.org/privacy">Privacy Policy</a> and <a href="https://www.linuxfoundation.org/terms">Terms of Use</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Learn</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/get-started">Get Started</a> 
          </li>
          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/#community-module">Community</a>
          </li>
          <li>
            <a href="https://discuss.pytorch.org">Forums</a>
          </li>
          <li>
            <a href="/resources">Developer Resources</a>
          </li>
          <li>
            <a href="/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Docs</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/docs">PyTorch</a>
          </li>
          <li>
            <a href="/pytorch-domains">PyTorch Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/blog">PyTorch Blog</a>
          </li>
          <li>
            <a href="/community-blog">Community Blog</a>
          </li>
          <li>
            <a href="/videos">Videos</a>
          </li>
          <li>
            <a href="/community-stories">Community Stories</a>
          </li>
          <li>
            <a href="/events">Events</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="/governing-board">Governing Board</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a href="/join">Become a Member</a>
        </li>
        <li class="resources-mobile-menu-title">
          <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub"><div id="topnav-gh-icon"></div></a>
        </li>

      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header, .announcement-header, .comm-stories-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      New library updates in PyTorch 1.12 | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="We are bringing a number of improvements to the current PyTorch libraries, alongside the PyTorch 1.12 release. These updates demonstrate our focus on developing common and extensible APIs across all domains to make it easier for our community to build ecosystem projects on PyTorch.

" />

  <meta property="og:image" content="https://pytorch.org/" />
  <meta name="twitter:image" content="https://pytorch.org/" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="New library updates in PyTorch 1.12" />
<meta property="og:description" content="We are bringing a number of improvements to the current PyTorch libraries, alongside the PyTorch 1.12 release. These updates demonstrate our focus on developing common and extensible APIs across all domains to make it easier for our community to build ecosystem projects on PyTorch.

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="New library updates in PyTorch 1.12" />
<meta name="twitter:description" content="We are bringing a number of improvements to the current PyTorch libraries, alongside the PyTorch 1.12 release. These updates demonstrate our focus on developing common and extensible APIs across all domains to make it easier for our community to build ecosystem projects on PyTorch.

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>
    <div class="hello-bar">
        <div class="container">
          Join us in Silicon Valley September 18-19 at the 2024 PyTorch Conference. <a target="_blank" href="https://events.linuxfoundation.org/pytorch-conference/">Learn more</a>.
        </div>
      </div>
      
    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Learn
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/get-started">
            <span class=dropdown-title>Get Started</span>
            <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/">
            <span class="dropdown-title">Tutorials</span>
            <p>Whats new in PyTorch tutorials</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
            <span class="dropdown-title">Learn the Basics</span>
            <p>Familiarize yourself with PyTorch concepts and modules</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
            <span class="dropdown-title">PyTorch Recipes</span>
            <p>Bite-size, ready-to-deploy PyTorch code examples</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
            <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
            <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/contributor-awards-2023">
            <span class="dropdown-title">Contributor Awards - 2023</span>
            <p>Award winners announced at this year's PyTorch Conference</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
            <p>Build innovative and privacy-aware AI experiences for edge devices</p>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
            <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://pytorch.org/docs">
            <span class="dropdown-title">PyTorch</span>
            <p>Explore the documentation for comprehensive guidance on how to use PyTorch.</p>
          </a>
          <a class="nav-dropdown-item" href="/pytorch-domains">
            <span class="dropdown-title">PyTorch Domains</span>
            <p> Read the PyTorch Domains documentation to learn more about domain-specific libraries.</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Blog & News
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/blog">
            <span class="dropdown-title">PyTorch Blog</span>
            <p>Catch up on the latest technical news and happenings</p>
          </a>
          <a class="nav-dropdown-item" href="/community-blog">
            <span class="dropdown-title">Community Blog</span>
            <p>Stories from the PyTorch ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/videos">
            <span class="dropdown-title">Videos</span>
            <p>Learn about the latest PyTorch tutorials, new, and more </p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class="dropdown-title">Community Stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          About
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/governing-board">
            <span class=dropdown-title>Governing Board</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <a href="/join" data-cta="join">
        Become a Member
      </a>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub">
        <div id="topnav-gh-icon"></div>
      </a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">June 28, 2022</p>
            <h1>
                <a class="blog-title">New library updates in PyTorch 1.12</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Team PyTorch
                      
                    </p>
                    <p>We are bringing a number of improvements to the current PyTorch libraries, alongside the <a href="https://github.com/pytorch/pytorch/releases/tag/v1.12.0">PyTorch 1.12 release</a>. These updates demonstrate our focus on developing common and extensible APIs across all domains to make it easier for our community to build ecosystem projects on PyTorch.</p>

<p>Summary:</p>
<ul>
  <li><strong>TorchVision</strong> - Added multi-weight support API, new architectures, model variants, and pretrained weight. See the release notes <a href="https://github.com/pytorch/vision/releases">here</a>.</li>
  <li><strong>TorchAudio</strong> - Introduced beta features including a streaming API, a CTC beam search decoder, and new beamforming modules and methods. See the release notes <a href="https://github.com/pytorch/audio/releases">here</a>.</li>
  <li><strong>TorchText</strong> - Extended support for scriptable BERT tokenizer and added datasets for GLUE benchmark. See the release notes <a href="https://github.com/pytorch/text/releases">here</a>.</li>
  <li><strong>TorchRec</strong> - Added EmbeddingModule benchmarks, examples for TwoTower Retrieval, inference and sequential embeddings, metrics, improved planner and demonstrated integration with production components. See the release notes <a href="https://github.com/pytorch/torchrec/releases">here</a>.</li>
  <li><strong>TorchX</strong> - Launch PyTorch trainers developed on local workspaces onto five different types of schedulers. See the release notes <a href="https://github.com/pytorch/torchx/blob/main/CHANGELOG.md?plain=1#L3">here</a>.</li>
  <li><strong>FBGemm</strong> - Added and improved kernels for Recommendation Systems inference workloads, including table batched embedding bag, jagged tensor operations, and other special-case optimizations.</li>
</ul>

<h2 id="torchvision-v013">TorchVision v0.13</h2>

<h3 id="multi-weight-support-api">Multi-weight support API</h3>

<p>TorchVision v0.13 offers a new <a href="https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/">Multi-weight support API</a> for loading different weights to the existing model builder methods:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Old weights with accuracy 76.130%
</span><span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet50_Weights</span><span class="p">.</span><span class="n">IMAGENET1K_V1</span><span class="p">)</span>

<span class="c1"># New weights with accuracy 80.858%
</span><span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet50_Weights</span><span class="p">.</span><span class="n">IMAGENET1K_V2</span><span class="p">)</span>

<span class="c1"># Best available weights (currently alias for IMAGENET1K_V2)
# Note that these weights may change across versions
</span><span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet50_Weights</span><span class="p">.</span><span class="n">DEFAULT</span><span class="p">)</span>

<span class="c1"># Strings are also supported
</span><span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s">"IMAGENET1K_V2"</span><span class="p">)</span>

<span class="c1"># No weights - random initialization
</span><span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p>The new API bundles along with the weights important details such as the preprocessing transforms and meta-data such as labels. Here is how to make the most out of it:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision.io</span> <span class="kn">import</span> <span class="n">read_image</span>
<span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="n">resnet50</span><span class="p">,</span> <span class="n">ResNet50_Weights</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="s">"test/assets/encode_jpeg/grace_hopper_517x606.jpg"</span><span class="p">)</span>

<span class="c1"># Step 1: Initialize model with the best available weights
</span><span class="n">weights</span> <span class="o">=</span> <span class="n">ResNet50_Weights</span><span class="p">.</span><span class="n">DEFAULT</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>

<span class="c1"># Step 2: Initialize the inference transforms
</span><span class="n">preprocess</span> <span class="o">=</span> <span class="n">weights</span><span class="p">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># Step 3: Apply inference preprocessing transforms
</span><span class="n">batch</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Step 4: Use the model and print the predicted category
</span><span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">).</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">softmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">class_id</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">.</span><span class="n">argmax</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="n">class_id</span><span class="p">].</span><span class="n">item</span><span class="p">()</span>
<span class="n">category_name</span> <span class="o">=</span> <span class="n">weights</span><span class="p">.</span><span class="n">meta</span><span class="p">[</span><span class="s">"categories"</span><span class="p">][</span><span class="n">class_id</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">category_name</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">score</span><span class="p">:.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">%"</span><span class="p">)</span>
</code></pre></div></div>

<p>You can read more about the new API in the <a href="https://pytorch.org/vision/0.13/models.html">docs</a>. To provide your feedback, please use this dedicated <a href="https://github.com/pytorch/vision/issues/5088">Github issue</a>.</p>

<h3 id="new-architectures-and-model-variants">New architectures and model variants</h3>

<h4 id="classification">Classification</h4>

<p>The <a href="https://arxiv.org/abs/2103.14030">Swin Transformer</a> and <a href="https://arxiv.org/abs/2104.00298">EfficienetNetV2</a> are two popular classification models which are often used for downstream vision tasks. This release includes 6 pre-trained weights for their classification variants. Here is how to use the new models:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">image</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">swin_t</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s">"DEFAULT"</span><span class="p">).</span><span class="nb">eval</span><span class="p">()</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

<span class="n">image</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">384</span><span class="p">,</span> <span class="mi">384</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">efficientnet_v2_s</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s">"DEFAULT"</span><span class="p">).</span><span class="nb">eval</span><span class="p">()</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</code></pre></div></div>

<p>In addition to the above, we also provide new variants for existing architectures such as ShuffleNetV2, ResNeXt and MNASNet. The accuracies of all the new pre-trained models obtained on ImageNet-1K are seen below:</p>

<table>
  <thead>
    <tr>
      <th><strong>Model</strong></th>
      <th><strong>Acc@1</strong></th>
      <th><strong>Acc@5</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>swin_t</td>
      <td>81.474</td>
      <td>95.776</td>
    </tr>
    <tr>
      <td>swin_s</td>
      <td>83.196</td>
      <td>96.36</td>
    </tr>
    <tr>
      <td>swin_b</td>
      <td>83.582</td>
      <td>96.64</td>
    </tr>
    <tr>
      <td>efficientnet_v2_s</td>
      <td>84.228</td>
      <td>96.878</td>
    </tr>
    <tr>
      <td>efficientnet_v2_m</td>
      <td>85.112</td>
      <td>97.156</td>
    </tr>
    <tr>
      <td>efficientnet_v2_l</td>
      <td>85.808</td>
      <td>97.788</td>
    </tr>
    <tr>
      <td>resnext101_64x4d</td>
      <td>83.246</td>
      <td>96.454</td>
    </tr>
    <tr>
      <td>resnext101_64x4d (quantized)</td>
      <td>82.898</td>
      <td>96.326</td>
    </tr>
    <tr>
      <td>shufflenet_v2_x1_5</td>
      <td>72.996</td>
      <td>91.086</td>
    </tr>
    <tr>
      <td>shufflenet_v2_x1_5 (quantized)</td>
      <td>72.052</td>
      <td>0.700</td>
    </tr>
    <tr>
      <td>shufflenet_v2_x2_0</td>
      <td>76.230</td>
      <td>93.006</td>
    </tr>
    <tr>
      <td>shufflenet_v2_x2_0 (quantized)</td>
      <td>75.354</td>
      <td>92.488</td>
    </tr>
    <tr>
      <td>mnasnet0_75</td>
      <td>71.180</td>
      <td>90.496</td>
    </tr>
    <tr>
      <td>mnas1_3</td>
      <td>76.506</td>
      <td>93.522</td>
    </tr>
  </tbody>
</table>

<p>We would like to thank Hu Ye for contributing to TorchVision the Swin Transformer implementation.</p>

<h4 id="beta-object-detection-and-instance-segmentation">(BETA) Object Detection and Instance Segmentation</h4>

<p>We have introduced 3 new model variants for RetinaNet, FasterRCNN and MaskRCNN that include several <a href="https://github.com/pytorch/vision/pull/5444">post-paper architectural optimizations</a> and improved training recipes. All models can be used similarly:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torchvision.models.detection</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">800</span><span class="p">,</span> <span class="mi">600</span><span class="p">)]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">retinanet_resnet50_fpn_v2</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s">"DEFAULT"</span><span class="p">)</span>
<span class="c1"># model = fasterrcnn_resnet50_fpn_v2(weights="DEFAULT")
# model = maskrcnn_resnet50_fpn_v2(weights="DEFAULT")
</span><span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</code></pre></div></div>

<p>Below we present the metrics of the new variants on COCO val2017. In parenthesis we denote the improvement over the old variants:</p>

<table>
  <thead>
    <tr>
      <th><strong>Model</strong></th>
      <th><strong>Box mAP</strong></th>
      <th><strong>Mask mAP</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>retinanet_resnet50_fpn_v2</td>
      <td>41.5 (+5.1)</td>
      <td>-</td>
    </tr>
    <tr>
      <td>fasterrcnn_resnet50_fpn_v2</td>
      <td>46.7 (+9.7)</td>
      <td>-</td>
    </tr>
    <tr>
      <td>maskrcnn_resnet50_fpn_v2</td>
      <td>47.4 (+9.5)</td>
      <td>41.8 (+7.2)</td>
    </tr>
  </tbody>
</table>

<p>We would like to thank Ross Girshick, Piotr Dollar, Vaibhav Aggarwal, Francisco Massa and Hu Ye for their past research and contributions to this work.</p>

<h3 id="new-pre-trained-weights">New pre-trained weights</h3>

<h4 id="swag-weights">SWAG weights</h4>

<p>The ViT and RegNet model variants offer new pre-trained <a href="https://arxiv.org/abs/2201.08371">SWAG</a> (​​Supervised Weakly from hashtAGs) weights. One of the biggest of these models achieves a whopping 88.6% accuracy on ImageNet-1K. We currently offer two versions of the weights: 1) fine-tuned end-to-end weights on ImageNet-1K (highest accuracy) and 2) frozen trunk weights with a linear classifier fit on ImageNet-1K (great for transfer learning). Below we see the detailed accuracies of each model variant:</p>

<table>
  <thead>
    <tr>
      <th><strong>Model Weights</strong></th>
      <th><strong>Acc@1</strong></th>
      <th><strong>Acc@5</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_E2E_V1</td>
      <td>86.012</td>
      <td>98.054</td>
    </tr>
    <tr>
      <td>RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_LINEAR_V1</td>
      <td>83.976</td>
      <td>97.244</td>
    </tr>
    <tr>
      <td>RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_E2E_V1</td>
      <td>86.838</td>
      <td>98.362</td>
    </tr>
    <tr>
      <td>RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_LINEAR_V1</td>
      <td>84.622</td>
      <td>97.48</td>
    </tr>
    <tr>
      <td>RegNet_Y_128GF_Weights.IMAGENET1K_SWAG_E2E_V1</td>
      <td>88.228</td>
      <td>98.682</td>
    </tr>
    <tr>
      <td>RegNet_Y_128GF_Weights.IMAGENET1K_SWAG_LINEAR_V1</td>
      <td>86.068</td>
      <td>97.844</td>
    </tr>
    <tr>
      <td>ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1</td>
      <td>85.304</td>
      <td>97.65</td>
    </tr>
    <tr>
      <td>ViT_B_16_Weights.IMAGENET1K_SWAG_LINEAR_V1</td>
      <td>81.886</td>
      <td>96.18</td>
    </tr>
    <tr>
      <td>ViT_L_16_Weights.IMAGENET1K_SWAG_E2E_V1</td>
      <td>88.064</td>
      <td>98.512</td>
    </tr>
    <tr>
      <td>ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1</td>
      <td>85.146</td>
      <td>97.422</td>
    </tr>
    <tr>
      <td>ViT_H_14_Weights.IMAGENET1K_SWAG_E2E_V1</td>
      <td>88.552</td>
      <td>98.694</td>
    </tr>
    <tr>
      <td>ViT_H_14_Weights.IMAGENET1K_SWAG_LINEAR_V1</td>
      <td>85.708</td>
      <td>97.73</td>
    </tr>
  </tbody>
</table>

<p>The SWAG weights are released under the <a href="https://github.com/facebookresearch/SWAG/blob/main/LICENSE">Attribution-NonCommercial 4.0 International</a> license. We would like to thank Laura Gustafson, Mannat Singh and Aaron Adcock for their work and support in making the weights available to TorchVision.</p>

<h4 id="model-refresh">Model Refresh</h4>

<p>The release of the Multi-weight support API enabled us to refresh the most popular models and offer more accurate weights. We improved on average each model by ~3 points. The new recipe used was learned on top of ResNet50 and its details were covered on a <a href="https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/">previous blog post</a>.</p>

<table>
  <thead>
    <tr>
      <th><strong>Model</strong></th>
      <th><strong>Old weights</strong></th>
      <th><strong>New weights</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>efficientnet_b1</td>
      <td>78.642</td>
      <td>79.838</td>
    </tr>
    <tr>
      <td>mobilenet_v2</td>
      <td>71.878</td>
      <td>72.154</td>
    </tr>
    <tr>
      <td>mobilenet_v3_large</td>
      <td>74.042</td>
      <td>75.274</td>
    </tr>
    <tr>
      <td>regnet_y_400mf</td>
      <td>74.046</td>
      <td>75.804</td>
    </tr>
    <tr>
      <td>regnet_y_800mf</td>
      <td>76.42</td>
      <td>78.828</td>
    </tr>
    <tr>
      <td>regnet_y_1_6gf</td>
      <td>77.95</td>
      <td>80.876</td>
    </tr>
    <tr>
      <td>regnet_y_3_2gf</td>
      <td>78.948</td>
      <td>81.982</td>
    </tr>
    <tr>
      <td>regnet_y_8gf</td>
      <td>80.032</td>
      <td>82.828</td>
    </tr>
    <tr>
      <td>regnet_y_16gf</td>
      <td>80.424</td>
      <td>82.886</td>
    </tr>
    <tr>
      <td>regnet_y_32gf</td>
      <td>80.878</td>
      <td>83.368</td>
    </tr>
    <tr>
      <td>regnet_x_400mf</td>
      <td>72.834</td>
      <td>74.864</td>
    </tr>
    <tr>
      <td>regnet_x_800mf</td>
      <td>75.212</td>
      <td>77.522</td>
    </tr>
    <tr>
      <td>regnet_x_1_6gf</td>
      <td>77.04</td>
      <td>79.668</td>
    </tr>
    <tr>
      <td>regnet_x_3_2gf</td>
      <td>78.364</td>
      <td>81.196</td>
    </tr>
    <tr>
      <td>regnet_x_8gf</td>
      <td>79.344</td>
      <td>81.682</td>
    </tr>
    <tr>
      <td>regnet_x_16gf</td>
      <td>80.058</td>
      <td>82.716</td>
    </tr>
    <tr>
      <td>regnet_x_32gf</td>
      <td>80.622</td>
      <td>83.014</td>
    </tr>
    <tr>
      <td>resnet50</td>
      <td>76.13</td>
      <td>80.858</td>
    </tr>
    <tr>
      <td>resnet50 (quantized)</td>
      <td>75.92</td>
      <td>80.282</td>
    </tr>
    <tr>
      <td>resnet101</td>
      <td>77.374</td>
      <td>81.886</td>
    </tr>
    <tr>
      <td>resnet152</td>
      <td>78.312</td>
      <td>82.284</td>
    </tr>
    <tr>
      <td>resnext50_32x4d</td>
      <td>77.618</td>
      <td>81.198</td>
    </tr>
    <tr>
      <td>resnext101_32x8d</td>
      <td>79.312</td>
      <td>82.834</td>
    </tr>
    <tr>
      <td>resnext101_32x8d (quantized)</td>
      <td>78.986</td>
      <td>82.574</td>
    </tr>
    <tr>
      <td>wide_resnet50_2</td>
      <td>78.468</td>
      <td>81.602</td>
    </tr>
    <tr>
      <td>wide_resnet101_2</td>
      <td>78.848</td>
      <td>82.51</td>
    </tr>
  </tbody>
</table>

<p>We would like to thank Piotr Dollar, Mannat Singh and Hugo Touvron for their past research and contributions to this work.</p>

<h3 id="new-augmentations-layers-and-losses">New Augmentations, Layers and Losses</h3>

<p>This release brings a bunch of new primitives which can be used to produce SOTA models. Some highlights include the addition of <a href="https://arxiv.org/abs/1912.02781">AugMix</a> data-augmentation method, the <a href="https://arxiv.org/abs/1810.12890">DropBlock</a> layer, the <a href="https://arxiv.org/abs/1911.08287">cIoU/dIoU</a> loss and <a href="https://github.com/pytorch/vision/issues/5410">many more</a>. We would like to thank Aditya Oke, Abhijit Deo, Yassine Alouini and Hu Ye for contributing to the project and for helping us maintain TorchVision relevant and fresh.</p>

<h3 id="documentation">Documentation</h3>

<p>We completely revamped our models documentation to make them easier to browse, and added various key information such as supported image sizes, or image pre-processing steps of pre-trained weights. We now have a <a href="https://pytorch.org/vision/main/models.html">main model page</a> with various <a href="https://pytorch.org/vision/main/models.html#table-of-all-available-classification-weights">summary tables</a> of available weights, and each model has a <a href="https://pytorch.org/vision/main/models/resnet.html">dedicated page</a>. Each model builder is also documented in their <a href="https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html#torchvision.models.resnet50">own page</a>, with more details about the available weights, including accuracy, minimal image size, link to training recipes, and other valuable info. For comparison, our previous models docs are <a href="https://pytorch.org/vision/0.12/models.html">here</a>. To provide feedback on the new documentation, please use the dedicated <a href="https://github.com/pytorch/vision/issues/5511">Github issue</a>.</p>

<h2 id="torchaudio-v012">TorchAudio v0.12</h2>

<h3 id="beta-streaming-api">(BETA) Streaming API</h3>

<p align="middle" float="left">
  <img src="/assets/images/streamingapi.jpeg" width="40%" /> <img src="/assets/images/torchaudio-0-12-streaming-ASR-2.gif" width="50%" />
</p>

<p>StreamReader is TorchAudio’s new I/O API. It is backed by FFmpeg†, and allows users to:</p>
<ul>
  <li>Decode audio and video formats, including MP4 and AAC</li>
  <li>Handle input forms, such as local files, network protocols, microphones, webcams, screen captures and file-like objects</li>
  <li>Iterate over and decode chunk-by-chunk, while changing the sample rate or frame rate</li>
  <li>Apply audio and video filters, such as low-pass filter and image scaling</li>
  <li>Decode video with Nvidia’s hardware-based decoder (NVDEC)</li>
</ul>

<p>For usage details, please check out the <a href="https://pytorch.org/audio/0.12.0/io.html#streamreader">documentation</a> and tutorials:</p>
<ul>
  <li><a href="https://pytorch.org/audio/0.12.0/tutorials/streaming_api_tutorial.html">Media Stream API - Pt.1</a></li>
  <li><a href="https://pytorch.org/audio/0.12.0/tutorials/streaming_api2_tutorial.html">Media Stream API - Pt.2</a></li>
  <li><a href="https://pytorch.org/audio/0.12.0/tutorials/online_asr_tutorial.html">Online ASR with Emformer RNN-T</a></li>
  <li><a href="https://pytorch.org/audio/0.12.0/tutorials/device_asr.html">Device ASR with Emformer RNN-T</a></li>
  <li><a href="https://pytorch.org/audio/0.12.0/hw_acceleration_tutorial.html">Accelerated Video Decoding with NVDEC</a></li>
</ul>

<p>† To use StreamReader, FFmpeg libraries are required. Please install FFmpeg. The coverage of codecs depends on how these libraries are configured. TorchAudio official binaries are compiled to work with FFmpeg 4 libraries; FFmpeg 5 can be used if TorchAudio is built from source.</p>

<h3 id="beta-ctc-beam-search-decoder">(BETA) CTC Beam Search Decoder</h3>

<p>TorchAudio integrates the wav2letter CTC beam search decoder from <a href="https://arxiv.org/pdf/2201.12465.pdf">Flashlight</a> (<a href="https://github.com/flashlight/flashlight">GitHub</a>). The addition of this inference time decoder enables running end-to-end CTC ASR evaluation using TorchAudio utils.</p>

<p>Customizable lexicon and lexicon-free decoders are supported, and both are compatible with KenLM n-gram language models or without using a language model. TorchAudio additionally supports downloading token, lexicon, and pretrained KenLM files for the LibriSpeech dataset.</p>

<p>For usage details, please check out the <a href="https://pytorch.org/audio/0.12.0/models.decoder.html#ctcdecoder">documentation</a> and <a href="https://pytorch.org/audio/0.12.0/tutorials/asr_inference_with_ctc_decoder_tutorial.html">ASR inference tutorial</a>.</p>

<h3 id="beta-new-beamforming-modules-and-methods">(BETA) New Beamforming Modules and Methods</h3>

<p>To improve flexibility in usage, the release adds two new beamforming modules under torchaudio.transforms: <a href="https://pytorch.org/audio/0.12.0/transforms.html#soudenmvdr">SoudenMVDR</a> and <a href="https://pytorch.org/audio/0.12.0/transforms.html#rtfmvdr">RTFMVDR</a>. The main differences from <a href="https://pytorch.org/audio/0.11.0/transforms.html#mvdr">MVDR</a> are:</p>
<ul>
  <li>Use power spectral density (PSD) and relative transfer function (RTF) matrices as inputs instead of time-frequency masks. The module can be integrated with neural networks that directly predict complex-valued STFT coefficients of speech and noise</li>
  <li>Add 'reference_channel' as an input argument in the forward method, to allow users to select the reference channel in model training or dynamically change the reference channel in inference</li>
</ul>

<p>Besides the two modules, new function-level beamforming methods are added under torchaudio.functional. These include:</p>
<ul>
  <li><a href="https://pytorch.org/audio/0.12.0/functional.html#psd">psd</a></li>
  <li><a href="https://pytorch.org/audio/0.12.0/functional.html#mvdr-weights-souden">mvdr_weights_souden</a></li>
  <li><a href="https://pytorch.org/audio/0.12.0/functional.html#mvdr-weights-rtf">mvdr_weights_rtf</a></li>
  <li><a href="https://pytorch.org/audio/0.12.0/functional.html#rtf-evd">rtf_evd</a></li>
  <li><a href="https://pytorch.org/audio/0.12.0/functional.html#rtf-power">rtf_power</a></li>
  <li><a href="https://pytorch.org/audio/0.12.0/functional.html#apply-beamforming">apply_beamforming</a></li>
</ul>

<p>For usage details, please check out the documentation at <a href="https://pytorch.org/audio/0.12.0/transforms.html#multi-channel">torchaudio.transforms</a> and <a href="https://pytorch.org/audio/0.12.0/functional.html#multi-channel">torchaudio.functional</a> and the <a href="https://pytorch.org/audio/0.12.0/tutorials/mvdr_tutorial.html">Speech Enhancement with MVDR Beamforming tutorial</a>.</p>

<h2 id="torchtext-v013">TorchText v0.13</h2>

<h3 id="glue-datasets">Glue Datasets</h3>

<p>We increased the number of datasets in TorchText from 22 to 30 by adding the remaining 8 datasets from the GLUE benchmark (SST-2 was already supported). The complete list of GLUE datasets is as follows:</p>
<ul>
  <li><a href="https://nyu-mll.github.io/CoLA/">CoLA</a> (<a href="https://arxiv.org/pdf/1805.12471.pdf">paper</a>): Single sentence binary classification acceptability task</li>
  <li><a href="https://nlp.stanford.edu/sentiment/">SST-2</a> (<a href="https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf">paper</a>): Single sentence binary classification sentiment task</li>
  <li><a href="https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf">MRPC</a> (<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/I05-50025B15D.pdf">paper</a>): Dual sentence binary classification paraphrase task</li>
  <li><a href="https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs">QQP</a>: Dual sentence binary classification paraphrase task</li>
  <li><a href="https://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark">STS-B</a> (<a href="https://aclanthology.org/S17-2001.pdf">paper</a>): Single sentence to float regression sentence similarity task</li>
  <li><a href="https://cims.nyu.edu/~sbowman/multinli/">MNLI</a> (<a href="https://cims.nyu.edu/~sbowman/multinli/paper.pdf">paper</a>): Sentence ternary classification NLI task</li>
  <li><a href="https://gluebenchmark.com/">QNLI</a> (<a href="https://arxiv.org/pdf/1804.07461.pdf">paper</a>): Sentence binary classification QA and NLI tasks</li>
  <li><a href="https://aclweb.org/aclwiki/Recognizing_Textual_Entailment">RTE</a> (<a href="https://arxiv.org/pdf/2010.03061.pdf">paper</a>): Dual sentence binary classification NLI task</li>
  <li><a href="https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html">WNLI</a> (<a href="http://commonsensereasoning.org/2011/papers/Levesque.pdf">paper</a>): Dual sentence binary classification coreference and NLI tasks</li>
</ul>

<h3 id="scriptable-bert-tokenizer">Scriptable BERT Tokenizer</h3>

<p>TorchText has extended support for scriptable tokenizer by adding the WordPiece tokenizer used in BERT. It is one of the commonly used algorithms for splitting input text into sub-words units and was introduced in <a href="https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf">Japanese and Korean Voice Search (Schuster et al., 2012)</a>.</p>

<p>TorchScriptabilty support would allow users to embed the BERT text-preprocessing natively in C++ without needing the support of python runtime. As TorchText now supports the CMAKE build system to natively link torchtext binaries with application code, users can easily integrate BERT tokenizers for deployment needs.</p>

<p>For usage details, please refer to the corresponding <a href="https://pytorch.org/text/main/transforms.html#torchtext.transforms.BERTTokenizer">documentation</a>.</p>

<h2 id="torchrec-v020">TorchRec v0.2.0</h2>

<h3 id="embeddingmodule--dlrm-benchmarks">EmbeddingModule + DLRM benchmarks</h3>

<p>A set of <a href="https://github.com/pytorch/torchrec/tree/main/benchmarks">benchmarking tests</a>, showing performance characteristics of TorchRec’s base modules  and research models built out of TorchRec.</p>

<h3 id="twotower-retrieval-example-with-faiss">TwoTower Retrieval Example, with FAISS</h3>

<p>We provide an <a href="https://github.com/pytorch/torchrec/tree/main/examples/retrieval">example</a> demonstrating training a distributed TwoTower (i.e. User-Item) Retrieval model that is sharded using TorchRec. The projected item embeddings are added to an IVFPQ FAISS index for candidate generation. The retrieval model and KNN lookup are bundled in a Pytorch model for efficient end-to-end retrieval.</p>

<h3 id="integrations">Integrations</h3>

<p>We demonstrate that TorchRec works out of the box with many components commonly used alongside PyTorch models in production like systems, such as</p>
<ul>
  <li><a href="https://github.com/pytorch/torchrec/tree/main/examples/ray">Training</a> a TorchRec model on Ray Clusters utilizing the Torchx Ray scheduler</li>
  <li><a href="https://github.com/pytorch/torchrec/tree/main/torchrec/datasets/scripts/nvt">Preprocessing</a> and DataLoading with NVTabular on DLRM</li>
  <li><a href="https://github.com/pytorch/torchrec/tree/main/examples/torcharrow">Training</a> a TorchRec model with on-the-fly preprocessing with TorchArrow showcasing RecSys domain UDFs</li>
</ul>

<h3 id="sequential-embeddings-example-bert4rec">Sequential Embeddings Example: Bert4Rec</h3>

<p>We provide an <a href="https://github.com/pytorch/torchrec/tree/main/examples/bert4rec">example</a>, using TorchRec, that reimplements the <a href="https://arxiv.org/abs/1904.06690">BERT4REC</a> paper, showcasing EmbeddingCollection for non-pooled embeddings. Using DistributedModelParallel we see a 35% QPS gain over conventional data parallelism.</p>

<h3 id="beta-planner">(Beta) Planner</h3>

<p>The TorchRec library includes a built-in <a href="https://pytorch.org/torchrec/torchrec.distributed.planner.html">planner</a> that selects near optimal sharding plan for a given model.  The planner attempts to identify the best sharding plan by evaluating a series of proposals which are statically analyzed and fed into an integer partitioner.  The planner is able to automatically adjust plans for a wide range of hardware setups, allowing users to scale performance seamlessly from local development environment to large scale production hardware. See this <a href="https://github.com/pytorch/torchrec/blob/main/torchrec/distributed/planner/Planner_Introduction.ipynb">notebook</a> for a more detailed tutorial.</p>

<h3 id="beta-inference">(Beta) Inference</h3>

<p><a href="https://github.com/pytorch/torchrec/tree/main/torchrec/inference">TorchRec Inference</a> is a C++ library that supports multi-gpu inference. The TorchRec library is used to shard models written and packaged in Python via torch.package (an alternative to TorchScript). The torch.deploy library is used to serve inference from C++ by launching multiple Python interpreters carrying the packaged model, thus subverting the GIL. Two models are provided as examples: <a href="https://github.com/pytorch/torchrec/blob/main/examples/inference/dlrm_predict.py">DLRM multi-GPU</a> (sharded via TorchRec) and <a href="https://github.com/pytorch/torchrec/blob/main/examples/inference/dlrm_predict_single_gpu.py">DLRM single-GPU</a>.</p>

<h3 id="beta-recmetrics">(Beta) RecMetrics</h3>

<p>RecMetrics is a <a href="https://github.com/pytorch/torchrec/tree/main/torchrec/metrics">metrics</a> library that collects common utilities and optimizations for Recommendation models.  It extends <a href="https://torchmetrics.readthedocs.io/en/stable/">torchmetrics</a>.</p>
<ul>
  <li>A centralized metrics module that allows users to add new metrics</li>
  <li>Commonly used metrics, including AUC, Calibration, CTR, MSE/RMSE, NE &amp; Throughput</li>
  <li>Optimization for metrics related operations to reduce the overhead of metric computation</li>
  <li>Checkpointing</li>
</ul>

<h3 id="prototype-single-process-batched--fused-embeddings">(Prototype) Single process Batched + Fused Embeddings</h3>

<p>Previously TorchRec’s abstractions (EmbeddingBagCollection/EmbeddingCollection) over FBGEMM kernels, which provide benefits such as table batching, optimizer fusion, and UVM placement, could only be used in conjunction with DistributedModelParallel. We’ve decoupled these notions from sharding, and introduced the <a href="https://github.com/pytorch/torchrec/blob/eb1247d8a2d16edc4952e5c2617e69acfe5477a5/torchrec/modules/fused_embedding_modules.py#L271">FusedEmbeddingBagCollection</a>, which can be used as a standalone module, with all of the above features, and can also be sharded.</p>

<h2 id="torchx-v020">TorchX v0.2.0</h2>

<p>TorchX is a job launcher that makes it easier to run PyTorch in distributed training clusters with many scheduler integrations including Kubernetes and Slurm. We’re excited to release TorchX 0.2.0 with a number of improvements. TorchX is currently being used in production in both on-premise and cloud environments.</p>

<p>Check out the <a href="https://pytorch.org/torchx/main/quickstart.html">quickstart</a> to start launching local and remote jobs.</p>

<h3 id="workspaces">Workspaces</h3>

<p>TorchX <a href="https://pytorch.org/torchx/main/workspace.html">now supports workspaces</a> which allows users to easily launch training jobs using their local workspace. TorchX can automatically build a patch with your local training code on top of a base image to minimize iteration time and time to training.</p>

<h3 id="torchxconfig">.torchxconfig</h3>

<p>Specifying options in <a href="https://pytorch.org/torchx/latest/runner.config.html">.torchxconfig</a> saves you from having to type long CLI commands each time you launch a job. You can also define project level generic configs and drop a config file in your home directory for user-level overrides.</p>

<h3 id="expanded-scheduler-support">Expanded Scheduler Support</h3>

<p>TorchX now supports <a href="https://pytorch.org/torchx/main/schedulers/aws_batch.html">AWS Batch</a> and <a href="https://pytorch.org/torchx/main/schedulers/ray.html">Ray (experimental)</a> schedulers in addition to our existing integrations.</p>

<h3 id="distributed-training-on-all-schedulers">Distributed Training On All Schedulers</h3>

<p>The TorchX dist.ddp component now works on all schedulers without any configuration. Distributed training workers will automatically discover each other when using <a href="https://pytorch.org/docs/stable/distributed.elastic.html">torchelastic</a> via <a href="https://pytorch.org/torchx/main/components/distributed.html">the builtin dist.ddp component</a>.</p>

<h3 id="hyper-parameter-optimization">Hyper Parameter Optimization</h3>

<p>TorchX <a href="https://ax.dev/versions/latest/api/runners.html#module-ax.runners.torchx">integrates with Ax</a> to let you scale hyper-parameter optimizations (HPO) by launching the search trials onto remote clusters.</p>

<h3 id="file-and-device-mounts">File and Device Mounts</h3>

<p>TorchX now supports <a href="https://pytorch.org/torchx/main/specs.html#mounts">remote filesystem mounts and custom devices</a>. This enables your PyTorch jobs to efficiently access cloud storage such as NFS or Lustre. The device mounts enables usage of network accelerators like Infiniband and custom inference/training accelerators.</p>

<h2 id="fbgemm-v020">FBGemm v0.2.0</h2>

<p>The FBGEMM library contains optimized kernels meant to improve the performance of PyTorch workloads. We’ve added a number of new features and optimizations over the last few months that we are excited to report.</p>

<h3 id="inference-table-batched-embedding-tbe">Inference Table Batched Embedding (TBE)</h3>

<p>The <a href="https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/fbgemm_gpu/split_table_batched_embeddings_ops.py#L1541">table batched embedding bag</a> (TBE) operator is an important base operation for embedding lookup for recommendation system inference on GPU. We added the following enhancements for performance and flexibility:</p>

<p>Alignment restriction removed</p>
<ul>
  <li>Embedding dimension * data type size had to be multiple of 4B before and now, it is 1B.</li>
</ul>

<p>Unified Virtual Memory (UVM) caching kernel optimizations</p>
<ul>
  <li>UVM caching kernels now scale linearly with # of tables using UVM caching. Previously, it was having similar overhead as all tables using UVM caching</li>
  <li>UVM caching kernel overhead is much smaller than before</li>
</ul>

<h3 id="inference-fp8-table-batched-embedding-tbe">Inference FP8 Table Batched Embedding (TBE)</h3>

<p>The <a href="https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/fbgemm_gpu/split_table_batched_embeddings_ops.py#L1541">table batched embedding bag</a> (TBE) previously supported FP32, FP16, INT8, INT4, and INT2 embedding weight types.  While these weight types work well in many models, we integrate FP8 weight types (in both GPU and CPU operations) to allow for numerical and performance evaluations of FP8 in our models.  Compared to INT8, FP8 does not require the additional bias and scale storage and calculations.  Additionally, the next generation of H100 GPUs has the FP8 support on Tensor Core (mainly matmul ops).</p>

<h3 id="jagged-tensor-kernels">Jagged Tensor Kernels</h3>

<p>We added optimized kernels to speed up <a href="https://pytorch.org/torchrec/torchrec.sparse.html">TorchRec JaggedTensor</a>. The purpose of JaggedTensor is to handle the case where one dimension of the input data is “jagged”, meaning that each consecutive row in a given dimension may be a different length, which is often the case with sparse feature inputs in recommendation systems. The internal representation is shown below:</p>

<p align="center">
  <img src="/assets/images/Jagged-Tensor-Figure-from-FBGEMM-section.png" width="80%" />
</p>

<p>We added ops for converting jagged tensors from sparse to dense formats and back, performing matrix multiplications with jagged tensors, and elementwise ops.</p>

<h3 id="optimized-permute102-baddbmm-permute102">Optimized permute102-baddbmm-permute102</h3>

<p>It is difficult to fuse various matrix multiplications where the batch size is not the batch size of the model, switching the batch dimension is a quick solution. We created the permute102_baddbmm_permute102 operation that switches the first and the second dimension, performs the batched matrix multiplication and then switches back. Currently we only support forward pass with FP16 data type and will support FP32 type and backward pass in the future.</p>

<h3 id="optimized-index_select-for-dim-0-index-selection">Optimized index_select for dim 0 index selection</h3>

<p>index_select is normally used as part of a sparse operation. While PyTorch supports a generic index_select for an arbitrary-dimension index selection, its performance for a special case like the dim 0 index selection is suboptimal. For this reason, we implement a specialized index_select for dim 0. In some cases, we have observed 1.4x performance gain from FBGEMM’s index_select compared to the one from PyTorch (using uniform index distribution).</p>

<p>More about the implementation of influential instances can be found on our <a href="https://github.com/pytorch/captum/tree/master/captum/influence">GitHub</a> page and <a href="https://captum.ai/tutorials/TracInCP_Tutorial">tutorials</a>.</p>

<p>Thanks for reading, If you’re interested in these updates and want to join the PyTorch community, we encourage you to join the <a href="https://discuss.pytorch.org/">discussion forums</a> and <a href="https://github.com/pytorch/pytorch/issues">open GitHub issues</a>. To get the latest news from PyTorch, follow us on <a href="https://twitter.com/PyTorch">Twitter</a>, <a href="https://medium.com/pytorch">Medium</a>, <a href="https://www.youtube.com/pytorch">YouTube</a>, and <a href="https://www.linkedin.com/company/pytorch">LinkedIn</a>.</p>

<p>Cheers!</p>

<p>Team PyTorch</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="lf-grid">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org" class="footer-logo">
          <img src="/assets/images/logo-icon.svg" alt="PyTorch logo" width="40">
        </a>
      </div>

      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook"><path fill="currentColor" d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z"/></svg>	
        </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X"><path fill="currentColor" d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66"/></svg>
        </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube"><path fill="currentColor" d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z"/></svg>	
        </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn"><rect width="512" height="512" rx="0" fill="currentColor"/><circle fill="#000" cx="142" cy="138" r="37"/><path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198"/><path fill="#000" d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
        </a></li>
        <li><a href="https://social.lfx.dev/@pytorch" rel="me" target="_blank" title="PyTorch on Mastodon">
          <svg fill="currentColor" aria-label="Mastodon" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" xml:space="preserve"><path d="M21.327 8.566c0-4.339-2.843-5.61-2.843-5.61-1.433-.658-3.894-.935-6.451-.956h-.063c-2.557.021-5.016.298-6.45.956 0 0-2.843 1.272-2.843 5.61 0 .993-.019 2.181.012 3.441.103 4.243.778 8.425 4.701 9.463 1.809.479 3.362.579 4.612.51 2.268-.126 3.541-.809 3.541-.809l-.075-1.646s-1.621.511-3.441.449c-1.804-.062-3.707-.194-3.999-2.409a4.523 4.523 0 0 1-.04-.621s1.77.433 4.014.536c1.372.063 2.658-.08 3.965-.236 2.506-.299 4.688-1.843 4.962-3.254.434-2.223.398-5.424.398-5.424zm-3.353 5.59h-2.081V9.057c0-1.075-.452-1.62-1.357-1.62-1 0-1.501.647-1.501 1.927v2.791h-2.069V9.364c0-1.28-.501-1.927-1.502-1.927-.905 0-1.357.546-1.357 1.62v5.099H6.026V8.903c0-1.074.273-1.927.823-2.558.566-.631 1.307-.955 2.228-.955 1.065 0 1.872.409 2.405 1.228l.518.869.519-.869c.533-.819 1.34-1.228 2.405-1.228.92 0 1.662.324 2.228.955.549.631.822 1.484.822 2.558v5.253z"/></svg>
        </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/legal/policies/">Linux Foundation Policies</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">LF Projects, LLC Policies</a>. <a href="https://www.linuxfoundation.org/privacy">Privacy Policy</a> and <a href="https://www.linuxfoundation.org/terms">Terms of Use</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Learn</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/get-started">Get Started</a> 
          </li>
          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/#community-module">Community</a>
          </li>
          <li>
            <a href="https://discuss.pytorch.org">Forums</a>
          </li>
          <li>
            <a href="/resources">Developer Resources</a>
          </li>
          <li>
            <a href="/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Docs</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/docs">PyTorch</a>
          </li>
          <li>
            <a href="/pytorch-domains">PyTorch Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/blog">PyTorch Blog</a>
          </li>
          <li>
            <a href="/community-blog">Community Blog</a>
          </li>
          <li>
            <a href="/videos">Videos</a>
          </li>
          <li>
            <a href="/community-stories">Community Stories</a>
          </li>
          <li>
            <a href="/events">Events</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="/governing-board">Governing Board</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a href="/join">Become a Member</a>
        </li>
        <li class="resources-mobile-menu-title">
          <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub"><div id="topnav-gh-icon"></div></a>
        </li>

      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header, .announcement-header, .comm-stories-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

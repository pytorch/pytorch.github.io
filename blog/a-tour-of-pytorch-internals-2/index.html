<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      PyTorch Internals Part II - The Build System | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="In the first post I explained how we generate a torch.Tensor object that you can use in your Python interpreter. Next, I will explore the build system for PyTorch. The PyTorch codebase has a variety of components:

" />

  <meta property="og:image" content="https://pytorch.org/assets/images/social-share.jpg" />
  <meta name="twitter:image" content="https://pytorch.org/assets/images/social-share.jpg" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="PyTorch Internals Part II - The Build System" />
<meta property="og:description" content="In the first post I explained how we generate a torch.Tensor object that you can use in your Python interpreter. Next, I will explore the build system for PyTorch. The PyTorch codebase has a variety of components:

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="PyTorch Internals Part II - The Build System" />
<meta name="twitter:description" content="In the first post I explained how we generate a torch.Tensor object that you can use in your Python interpreter. Next, I will explore the build system for PyTorch. The PyTorch codebase has a variety of components:

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>
    <div class="hello-bar">
        <div class="container">
          Join us in Silicon Valley September 18-19 at the 2024 PyTorch Conference. <a target="_blank" href="https://events.linuxfoundation.org/pytorch-conference/">Learn more</a>.
        </div>
      </div>
      
    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Learn
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/get-started">
            <span class=dropdown-title>Get Started</span>
            <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/">
            <span class="dropdown-title">Tutorials</span>
            <p>Whats new in PyTorch tutorials</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
            <span class="dropdown-title">Learn the Basics</span>
            <p>Familiarize yourself with PyTorch concepts and modules</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
            <span class="dropdown-title">PyTorch Recipes</span>
            <p>Bite-size, ready-to-deploy PyTorch code examples</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
            <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
            <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/contributor-awards-2023">
            <span class="dropdown-title">Contributor Awards - 2023</span>
            <p>Award winners announced at this year's PyTorch Conference</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
            <p>Build innovative and privacy-aware AI experiences for edge devices</p>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
            <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://pytorch.org/docs">
            <span class="dropdown-title">PyTorch</span>
            <p>Explore the documentation for comprehensive guidance on how to use PyTorch.</p>
          </a>
          <a class="nav-dropdown-item" href="/pytorch-domains">
            <span class="dropdown-title">PyTorch Domains</span>
            <p> Read the PyTorch Domains documentation to learn more about domain-specific libraries.</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Blog & News
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/blog">
            <span class="dropdown-title">PyTorch Blog</span>
            <p>Catch up on the latest technical news and happenings</p>
          </a>
          <a class="nav-dropdown-item" href="/community-blog">
            <span class="dropdown-title">Community Blog</span>
            <p>Stories from the PyTorch ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/videos">
            <span class="dropdown-title">Videos</span>
            <p>Learn about the latest PyTorch tutorials, new, and more </p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class="dropdown-title">Community Stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          About
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/governing-board">
            <span class=dropdown-title>Governing Board</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <a href="/join" data-cta="join">
        Become a Member
      </a>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub">
        <div id="topnav-gh-icon"></div>
      </a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">June 27, 2017</p>
            <h1>
                <a class="blog-title">PyTorch Internals Part II - The Build System</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Trevor Killeen
                      
                    </p>
                    <p>In the first <a href="/blog/a-tour-of-pytorch-internals-1/">post</a> I explained how we generate a <code class="language-plaintext highlighter-rouge">torch.Tensor</code> object that you can use in your Python interpreter. Next, I will explore the build system for PyTorch. The PyTorch codebase has a variety of components:</p>

<ul>
  <li>The core Torch libraries: TH, THC, THNN, THCUNN</li>
  <li>Vendor libraries: CuDNN, NCCL</li>
  <li>Python Extension libraries</li>
  <li>Additional third-party libraries: NumPy, MKL, LAPACK</li>
</ul>

<p>How does a simple invocation of <code class="language-plaintext highlighter-rouge">python setup.py install</code> do the work that allows you to call <code class="language-plaintext highlighter-rouge">import torch</code> and use the PyTorch library in your code?</p>

<p>The first part of this document will explain the build process from and end-user point of view. This will explain how we take the components above to build the library. The second part of the document will be important for PyTorch developers. It will document ways to improve your iteration speed by building only a subset of the code that you are working on.</p>

<h3 id="setuptools-and-pytorchs-setup--function">Setuptools and PyTorch’s setup( ) function</h3>

<p>Python uses <a href="https://setuptools.readthedocs.io/en/latest/index.html">Setuptools</a> to build the library. Setuptools is an extension to the original distutils system from the core Python library. The core component of Setuptools is the <code class="language-plaintext highlighter-rouge">setup.py</code> file which contains all the information needed to build the project. The most important function is the <code class="language-plaintext highlighter-rouge">setup()</code> function which serves as the main entry point. Let’s take a look at the one in PyTorch:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">setup</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"torch"</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="n">version</span><span class="p">,</span>
      <span class="n">description</span><span class="o">=</span><span class="s">"Tensors and Dynamic neural networks in Python with strong GPU acceleration"</span><span class="p">,</span>
      <span class="n">ext_modules</span><span class="o">=</span><span class="n">extensions</span><span class="p">,</span>
      <span class="n">cmdclass</span><span class="o">=</span><span class="p">{</span>
          <span class="s">'build'</span><span class="p">:</span> <span class="n">build</span><span class="p">,</span>
          <span class="s">'build_py'</span><span class="p">:</span> <span class="n">build_py</span><span class="p">,</span>
          <span class="s">'build_ext'</span><span class="p">:</span> <span class="n">build_ext</span><span class="p">,</span>
          <span class="s">'build_deps'</span><span class="p">:</span> <span class="n">build_deps</span><span class="p">,</span>
          <span class="s">'build_module'</span><span class="p">:</span> <span class="n">build_module</span><span class="p">,</span>
          <span class="s">'develop'</span><span class="p">:</span> <span class="n">develop</span><span class="p">,</span>
          <span class="s">'install'</span><span class="p">:</span> <span class="n">install</span><span class="p">,</span>
          <span class="s">'clean'</span><span class="p">:</span> <span class="n">clean</span><span class="p">,</span>
      <span class="p">},</span>
      <span class="n">packages</span><span class="o">=</span><span class="n">packages</span><span class="p">,</span>
      <span class="n">package_data</span><span class="o">=</span><span class="p">{</span><span class="s">'torch'</span><span class="p">:</span> <span class="p">[</span>
          <span class="s">'lib/*.so*'</span><span class="p">,</span> <span class="s">'lib/*.dylib*'</span><span class="p">,</span>
          <span class="s">'lib/torch_shm_manager'</span><span class="p">,</span>
          <span class="s">'lib/*.h'</span><span class="p">,</span>
          <span class="s">'lib/include/TH/*.h'</span><span class="p">,</span> <span class="s">'lib/include/TH/generic/*.h'</span><span class="p">,</span>
          <span class="s">'lib/include/THC/*.h'</span><span class="p">,</span> <span class="s">'lib/include/THC/generic/*.h'</span><span class="p">]},</span>
      <span class="n">install_requires</span><span class="o">=</span><span class="p">[</span><span class="s">'pyyaml'</span><span class="p">],</span>
      <span class="p">)</span>
</code></pre></div></div>

<p>The function is composed entirely of keyword arguments, which serve two purposes:</p>

<ul>
  <li>Metadata (e.g. name, description, version)</li>
  <li>The contents of the package</li>
</ul>

<p>We are concerned with #2. Let’s break down the individual components:</p>

<ul>
  <li><strong>ext_modules</strong>: Python modules are either “pure” modules, containing only Python code, or “extension” modules written in the low-level language of the Python implementation. Here we are listing the extension modules in the build, including the main <code class="language-plaintext highlighter-rouge">torch._C</code> library that contains our Python Tensor</li>
  <li><strong>cmdclass</strong>: When using the <code class="language-plaintext highlighter-rouge">setup.py</code> script from the command line, the user must specify one or more “commands”, code snippets that perform a specific action. For example, the “install” command builds and installs the package. This mapping routes specific commands to functions in <code class="language-plaintext highlighter-rouge">setup.py</code> that implement them</li>
  <li><strong>packages</strong>: The list of packages in the project. These are “pure” - i.e. they only contain Python code. These are defined elsewhere in <code class="language-plaintext highlighter-rouge">setup.py</code></li>
  <li><strong>package_data</strong>: Additional files that need to be installed into a package: in this case the header files and shared libraries that the build will generate must be included in our installation</li>
  <li><strong>install_requires</strong>: In order to build PyTorch, we need pyyaml. Setuptools will handle making sure that pyyaml will be available, downloading and installing it if necessary</li>
</ul>

<p>We will consider these components in more detail, but for now it is instructive to look at the end product of an installation – i.e. what Setuptools does after building the code.</p>

<h3 id="site_packages">site_packages</h3>

<p>Third party packages are by default installed into the <code class="language-plaintext highlighter-rouge">lib/&lt;version&gt;/site_packages</code>  directory associated with your Python binary. For example, because I am using an <a href="https://conda.io/miniconda.html">Miniconda</a> environment, my Python binary is found at:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:pytorch <span class="o">(</span>master<span class="o">)</span><span class="nv">$ </span>which python
~/local/miniconda2/envs/p3/bin/python
</code></pre></div></div>
<p>And thus packages are installed into:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/home/killeent/local/miniconda2/envs/p3/lib/python3.6/site-packages
</code></pre></div></div>
<p>I installed PyTorch, and let’s take a look into torch folder in site-packages:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:site-packages<span class="nv">$ </span><span class="nb">cd </span>torch
<span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:torch<span class="nv">$ </span><span class="nb">ls
</span>autograd  backends  _C.cpython-36m-x86_64-linux-gnu.so  cuda  distributed  _dl.cpython-36m-x86_64-linux-gnu.so  functional.py  __init__.py  legacy  lib  multiprocessing  nn  optim  __pycache__  serialization.py  _six.py  sparse  storage.py  _tensor_docs.py  tensor.py  _tensor_str.py  _thnn  _torch_docs.py  utils  _utils.py  version.py
</code></pre></div></div>

<p>Note that everything we would expect to be here is here:</p>

<ul>
  <li>All the “pure” packages are here [todo print packages from setup.py to explain]</li>
  <li>The extension libraries are here - the ._C* and ._dl* shared libraries</li>
  <li>The package_data is here: the contents of lib/ match exactly what we described in the setup function:</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:torch<span class="nv">$ </span><span class="nb">ls </span>lib/
include     libnccl.so.1  libTHC.so.1   libTHCUNN.so.1  libTHNN.so.1  libTH.so.1   THCUNN.h  torch_shm_manager libnccl.so  libshm.so     libTHCS.so.1  libTHD.so.1     libTHPP.so.1  libTHS.so.1  THNN.h
</code></pre></div></div>

<p>The Python interpreter looks into <code class="language-plaintext highlighter-rouge">site_packages</code> during an import. If we call <code class="language-plaintext highlighter-rouge">import torch</code> in our Python code it will find the module here and initialize and import it. You can read more about the import system <a href="https://docs.python.org/3/tutorial/modules.html">here</a>.</p>

<h3 id="building-individual-parts">Building Individual Parts</h3>

<p>Next, we will look at the various individual components of the build from start to finish. This will illustrate how we combine all the code we mentioned in the introduction.</p>

<h3 id="backend-torch-and-vendor-libraries">Backend Torch and Vendor Libraries</h3>

<p>Let’s take a look at the <code class="language-plaintext highlighter-rouge">install</code> cmd override in PyTorch’s <code class="language-plaintext highlighter-rouge">setup.py</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">install</span><span class="p">(</span><span class="n">setuptools</span><span class="p">.</span><span class="n">command</span><span class="p">.</span><span class="n">install</span><span class="p">.</span><span class="n">install</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">.</span><span class="n">skip_build</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">run_command</span><span class="p">(</span><span class="s">'build_deps'</span><span class="p">)</span>
        <span class="n">setuptools</span><span class="p">.</span><span class="n">command</span><span class="p">.</span><span class="n">install</span><span class="p">.</span><span class="n">install</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div></div>

<p>We note the first thing it does is run a command called “build_deps” - let’s take a look at it’s <code class="language-plaintext highlighter-rouge">run()</code> method:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">tools.nnwrap</span> <span class="kn">import</span> <span class="n">generate_wrappers</span> <span class="k">as</span> <span class="n">generate_nn_wrappers</span>
        <span class="n">build_all_cmd</span> <span class="o">=</span> <span class="p">[</span><span class="s">'bash'</span><span class="p">,</span> <span class="s">'torch/lib/build_all.sh'</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">WITH_CUDA</span><span class="p">:</span>
            <span class="n">build_all_cmd</span> <span class="o">+=</span> <span class="p">[</span><span class="s">'--with-cuda'</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">WITH_NCCL</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">SYSTEM_NCCL</span><span class="p">:</span>
            <span class="n">build_all_cmd</span> <span class="o">+=</span> <span class="p">[</span><span class="s">'--with-nccl'</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">WITH_DISTRIBUTED</span><span class="p">:</span>
            <span class="n">build_all_cmd</span> <span class="o">+=</span> <span class="p">[</span><span class="s">'--with-distributed'</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">subprocess</span><span class="p">.</span><span class="n">call</span><span class="p">(</span><span class="n">build_all_cmd</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">sys</span><span class="p">.</span><span class="nb">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">generate_nn_wrappers</span><span class="p">()</span>
</code></pre></div></div>

<p>Here we note that that we have a shell script <code class="language-plaintext highlighter-rouge">build_all.sh</code> in the <code class="language-plaintext highlighter-rouge">torch/lib/</code> directory. This script is configurable by whether we are on a system with CUDA enabled, the NCCL library enabled, and PyTorch’s distributed library enabled.</p>

<p>Let’s take a look in <code class="language-plaintext highlighter-rouge">torch/lib</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:lib <span class="o">(</span>master<span class="o">)</span><span class="nv">$ </span><span class="nb">ls
</span>build_all.sh  libshm  nccl  README.md  TH  THC  THCS  THCUNN  THD  THNN  THPP  THS
</code></pre></div></div>

<p>Here we see the directories for all the backend libraries. <code class="language-plaintext highlighter-rouge">TH</code>, <code class="language-plaintext highlighter-rouge">THC</code>, <code class="language-plaintext highlighter-rouge">THNN</code>,  <code class="language-plaintext highlighter-rouge">THCUNN</code>, and <code class="language-plaintext highlighter-rouge">nccl</code> are <a href="https://developer.atlassian.com/blog/2015/05/the-power-of-git-subtree/">git subtrees</a> that are in sync with the libraries in e.g. <a href="https://github.com/torch/torch7/tree/master/lib/TH">github.com/torch</a>. <code class="language-plaintext highlighter-rouge">THS</code>, <code class="language-plaintext highlighter-rouge">THCS</code>, <code class="language-plaintext highlighter-rouge">THD</code>, <code class="language-plaintext highlighter-rouge">THPP</code> and <code class="language-plaintext highlighter-rouge">libshm</code> are libraries specific to PyTorch. All of the libraries contain <code class="language-plaintext highlighter-rouge">CMakeLists.txt</code> - indicating they are built with CMake.</p>

<p>The <code class="language-plaintext highlighter-rouge">build_all.sh</code> is essentially a script that runs the CMake configure step on all of these libraries, and then <code class="language-plaintext highlighter-rouge">make install</code>. Let’s run <code class="language-plaintext highlighter-rouge">./build_all.sh</code> and see what we are left with:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:lib <span class="o">(</span>master<span class="o">)</span><span class="nv">$ </span>./build_all.sh <span class="nt">--with-cuda</span> <span class="nt">--with-nccl</span> <span class="nt">--with-distributed</span>
<span class="o">[</span>various CMake output logs]
<span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:lib <span class="o">(</span>master<span class="o">)</span><span class="nv">$ </span><span class="nb">ls
</span>build  build_all.sh  include  libnccl.so  libnccl.so.1  libshm  libshm.so  libTHC.so.1  libTHCS.so.1  libTHCUNN.so.1  libTHD.so.1  libTHNN.so.1  libTHPP.so.1  libTH.so.1  libTHS.so.1  nccl  README.md  TH  THC  THCS  THCUNN  THCUNN.h  THD  THNN  THNN.h  THPP  THS  tmp_install  torch_shm_manager
</code></pre></div></div>

<p>Now there are a number of extra things in the directory:</p>

<ul>
  <li>Shared library files for each library</li>
  <li>Headers for <code class="language-plaintext highlighter-rouge">THNN</code> and <code class="language-plaintext highlighter-rouge">THCUNN</code></li>
  <li><code class="language-plaintext highlighter-rouge">build</code> and <code class="language-plaintext highlighter-rouge">tmp_install</code> directories</li>
  <li>The <code class="language-plaintext highlighter-rouge">torch_shm_manager</code> executable</li>
</ul>

<p>Let’s explore further. In the shell script, we create the <code class="language-plaintext highlighter-rouge">build</code> directory and a subdir for each library to build:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># We create a build directory for the library, which will</span>
<span class="c"># contain the cmake output. $1 is the library to be built</span>
  <span class="nb">mkdir</span> <span class="nt">-p</span> build/<span class="nv">$1</span>
  <span class="nb">cd </span>build/<span class="nv">$1</span>
</code></pre></div></div>

<p>Thus e.g. <code class="language-plaintext highlighter-rouge">build/TH</code> contains the CMake configuration output including the <code class="language-plaintext highlighter-rouge">Makefile</code> for building TH, and also the result of running make install in this directory.</p>

<p>Let’s also look at <code class="language-plaintext highlighter-rouge">tmp_install</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:lib <span class="o">(</span>master<span class="o">)</span><span class="nv">$ </span><span class="nb">ls </span>tmp_install/
bin  include  lib  share
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">tmp_install</code> looks like a standard install directory containing binaries, header files and library files. For example, <code class="language-plaintext highlighter-rouge">tmp_install/include/TH</code> contains all the <code class="language-plaintext highlighter-rouge">TH</code> headers, and <code class="language-plaintext highlighter-rouge">tmp_install/lib/</code> contains the <code class="language-plaintext highlighter-rouge">libTH.so.1</code> file.</p>

<p>So why have this directory? It is used to compile the libraries that depend on each other. For example, the <code class="language-plaintext highlighter-rouge">THC</code> library depends on the <code class="language-plaintext highlighter-rouge">TH</code> library and its headers. This is referenced in the build shell script as arguments to the <code class="language-plaintext highlighter-rouge">cmake</code> command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># install_dir is tmp_install</span>
cmake ...
	<span class="nt">-DTH_INCLUDE_PATH</span><span class="o">=</span><span class="s2">"</span><span class="nv">$INSTALL_DIR</span><span class="s2">/include"</span> <span class="se">\</span>
	<span class="nt">-DTH_LIB_PATH</span><span class="o">=</span><span class="s2">"</span><span class="nv">$INSTALL_DIR</span><span class="s2">/lib"</span> <span class="se">\</span>
</code></pre></div></div>

<p>And indeed if we look at the <code class="language-plaintext highlighter-rouge">THC</code> library we built:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:lib <span class="o">(</span>master<span class="o">)</span><span class="nv">$ </span>ldd libTHC.so.1
	...
	libTH.so.1 <span class="o">=&gt;</span> /home/killeent/github/pytorch/torch/lib/tmp_install/lib/./libTH.so.1 <span class="o">(</span>0x00007f84478b7000<span class="o">)</span>
</code></pre></div></div>

<p>The way the <code class="language-plaintext highlighter-rouge">build_all.sh</code> specifies the include and library paths is a little messy but this is representative of the overall idea. Finally, at the end of the script:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># If all the builds succeed we copy the libraries, headers,</span>
<span class="c"># binaries to torch/lib</span>
<span class="nb">cp</span> <span class="nv">$INSTALL_DIR</span>/lib/<span class="k">*</span> <span class="nb">.</span>
<span class="nb">cp </span>THNN/generic/THNN.h <span class="nb">.</span>
<span class="nb">cp </span>THCUNN/generic/THCUNN.h <span class="nb">.</span>
<span class="nb">cp</span> <span class="nt">-r</span> <span class="nv">$INSTALL_DIR</span>/include <span class="nb">.</span>
<span class="nb">cp</span> <span class="nv">$INSTALL_DIR</span>/bin/<span class="k">*</span> <span class="nb">.</span>
</code></pre></div></div>

<p>As we can see, at the end, we copy everything to the top-level <code class="language-plaintext highlighter-rouge">torch/lib</code> directory - explaining the contents we saw above. We’ll see why we do this next:</p>

<h3 id="nn-wrappers">NN Wrappers</h3>

<p>Briefly, let’s touch on the last part of the <code class="language-plaintext highlighter-rouge">build_deps</code> command: <code class="language-plaintext highlighter-rouge">generate_nn_wrappers()</code>.  We bind into the backend libraries using PyTorch’s custom <code class="language-plaintext highlighter-rouge">cwrap</code> tooling, which we touched upon in a previous post. For binding <code class="language-plaintext highlighter-rouge">TH</code> and <code class="language-plaintext highlighter-rouge">THC</code> we manually write the YAML declarations for each function. However, due to the relative simplicity of the <code class="language-plaintext highlighter-rouge">THNN</code> and <code class="language-plaintext highlighter-rouge">THCUNN</code> libraries, we auto-generate both the cwrap declarations and the resulting C++ code.</p>

<p>The reason we copy the <code class="language-plaintext highlighter-rouge">THNN.h</code> and <code class="language-plaintext highlighter-rouge">THCUNN.h</code> header files into <code class="language-plaintext highlighter-rouge">torch/lib</code> is that this is where the <code class="language-plaintext highlighter-rouge">generate_nn_wrappers()</code> code expects these files to be located. <code class="language-plaintext highlighter-rouge">generate_nn_wrappers()</code> does a few things:</p>

<ol>
  <li>Parses the header files, generating cwrap YAML declarations and writing them to output <code class="language-plaintext highlighter-rouge">.cwrap</code> files</li>
  <li>Calls <code class="language-plaintext highlighter-rouge">cwrap</code> with the appropriate plugins on these <code class="language-plaintext highlighter-rouge">.cwrap</code> files to generate source code for each</li>
  <li>Parses the headers <em>a second time</em> to generate <code class="language-plaintext highlighter-rouge">THNN_generic.h</code> - a library that takes <code class="language-plaintext highlighter-rouge">THPP</code> Tensors, PyTorch’s “generic” C++ Tensor Library, and calls into the appropriate <code class="language-plaintext highlighter-rouge">THNN</code>/<code class="language-plaintext highlighter-rouge">THCUNN</code> library function based on the dynamic type of the Tensor</li>
</ol>

<p>If we take a look into <code class="language-plaintext highlighter-rouge">torch/csrc/nn</code> after running <code class="language-plaintext highlighter-rouge">generate_nn_wrappers()</code> we can see the output:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:nn <span class="o">(</span>master<span class="o">)</span><span class="nv">$ </span><span class="nb">ls
</span>THCUNN.cpp  THCUNN.cwrap  THNN.cpp  THNN.cwrap  THNN_generic.cpp  THNN_generic.cwrap  THNN_generic.h  THNN_generic.inc.h
</code></pre></div></div>

<p>For example, the code generates cwrap like:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[
  name: FloatBatchNormalization_updateOutput
  return: void
  cname: THNN_FloatBatchNormalization_updateOutput
  arguments:
    - void* state
    - THFloatTensor* input
    - THFloatTensor* output
    - type: THFloatTensor*
      name: weight
      nullable: True
    - type: THFloatTensor*
      name: bias
      nullable: True
    - THFloatTensor* running_mean
    - THFloatTensor* running_var
    - THFloatTensor* save_mean
    - THFloatTensor* save_std
    - bool train
    - double momentum
    - double eps
]]
</code></pre></div></div>

<p>with corresponding <code class="language-plaintext highlighter-rouge">.cpp</code>:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">extern</span> <span class="s">"C"</span> <span class="kt">void</span> <span class="nf">THNN_FloatBatchNormalization_updateOutput</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">,</span> <span class="n">THFloatTensor</span><span class="o">*</span><span class="p">,</span> <span class="n">THFloatTensor</span><span class="o">*</span><span class="p">,</span> <span class="n">THFloatTensor</span><span class="o">*</span><span class="p">,</span> <span class="n">THFloatTensor</span><span class="o">*</span><span class="p">,</span> <span class="n">THFloatTensor</span><span class="o">*</span><span class="p">,</span> <span class="n">THFloatTensor</span><span class="o">*</span><span class="p">,</span> <span class="n">THFloatTensor</span><span class="o">*</span><span class="p">,</span> <span class="n">THFloatTensor</span><span class="o">*</span><span class="p">,</span> <span class="kt">bool</span><span class="p">,</span> <span class="kt">double</span><span class="p">,</span> <span class="kt">double</span><span class="p">);</span>

<span class="n">PyObject</span> <span class="o">*</span> <span class="nf">FloatBatchNormalization_updateOutput</span><span class="p">(</span><span class="n">PyObject</span> <span class="o">*</span><span class="n">_unused</span><span class="p">,</span> <span class="n">PyObject</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="p">{</span>
	<span class="c1">// argument checking, unpacking</span>
	 <span class="n">PyThreadState</span> <span class="o">*</span><span class="n">_save</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
      <span class="k">try</span> <span class="p">{</span>
        <span class="n">Py_UNBLOCK_THREADS</span><span class="p">;</span>
        <span class="n">THNN_FloatBatchNormalization_updateOutput</span><span class="p">(</span><span class="n">arg_state</span><span class="p">,</span> <span class="n">arg_input</span><span class="p">,</span> <span class="n">arg_output</span><span class="p">,</span> <span class="n">arg_weight</span><span class="p">,</span> <span class="n">arg_bias</span><span class="p">,</span> <span class="n">arg_running_mean</span><span class="p">,</span> <span class="n">arg_running_var</span><span class="p">,</span> <span class="n">arg_save_mean</span><span class="p">,</span> <span class="n">arg_save_std</span><span class="p">,</span> <span class="n">arg_train</span><span class="p">,</span> <span class="n">arg_momentum</span><span class="p">,</span> <span class="n">arg_eps</span><span class="p">);</span>
        <span class="n">Py_BLOCK_THREADS</span><span class="p">;</span>
        <span class="n">Py_RETURN_NONE</span><span class="p">;</span>
      <span class="p">}</span> <span class="k">catch</span> <span class="p">(...)</span> <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">_save</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">Py_BLOCK_THREADS</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">throw</span><span class="p">;</span>
      <span class="p">}</span>

    <span class="p">...</span>
<span class="p">}</span>
</code></pre></div></div>

<p>In the <code class="language-plaintext highlighter-rouge">THPP</code> generated code, the function looks like this:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">BatchNormalization_updateOutput</span><span class="p">(</span><span class="n">thpp</span><span class="o">::</span><span class="n">Tensor</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="n">thpp</span><span class="o">::</span><span class="n">Tensor</span><span class="o">*</span> <span class="n">output</span><span class="p">,</span> <span class="n">thpp</span><span class="o">::</span><span class="n">Tensor</span><span class="o">*</span> <span class="n">weight</span><span class="p">,</span> <span class="n">thpp</span><span class="o">::</span><span class="n">Tensor</span><span class="o">*</span> <span class="n">bias</span><span class="p">,</span> <span class="n">thpp</span><span class="o">::</span><span class="n">Tensor</span><span class="o">*</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">thpp</span><span class="o">::</span><span class="n">Tensor</span><span class="o">*</span> <span class="n">running_var</span><span class="p">,</span> <span class="n">thpp</span><span class="o">::</span><span class="n">Tensor</span><span class="o">*</span> <span class="n">save_mean</span><span class="p">,</span> <span class="n">thpp</span><span class="o">::</span><span class="n">Tensor</span><span class="o">*</span> <span class="n">save_std</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">train</span><span class="p">,</span> <span class="kt">double</span> <span class="n">momentum</span><span class="p">,</span> <span class="kt">double</span> <span class="n">eps</span><span class="p">)</span> <span class="p">{</span>
	<span class="c1">// Call appropriate THNN function based on tensor type, whether its on CUDA, etc.</span>
<span class="p">}</span>
</code></pre></div></div>

<p>We will look a little more at how these source files are used later.</p>

<h3 id="building-the-pure-python-modules">“Building” the Pure Python Modules</h3>

<p>Now that we have built the backend libraries (the “dependencies”) we can move forward with building the actual PyTorch code. The next Setuptools command that runs is <code class="language-plaintext highlighter-rouge">build_py</code>, which is used to build all the “Pure” python modules in our library. These are the “packages” passed to <code class="language-plaintext highlighter-rouge">setup.py</code>.</p>

<p>The packages are found using the Setuptools’ utility function <code class="language-plaintext highlighter-rouge">find_packages()</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">packages</span> <span class="o">=</span> <span class="n">find_packages</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">(</span><span class="s">'tools.*'</span><span class="p">,))</span>
<span class="p">[</span><span class="s">'torch'</span><span class="p">,</span> <span class="s">'torch._thnn'</span><span class="p">,</span> <span class="s">'torch.autograd'</span><span class="p">,</span> <span class="s">'torch.backends'</span><span class="p">,</span> <span class="s">'torch.cuda'</span><span class="p">,</span> <span class="s">'torch.distributed'</span><span class="p">,</span> <span class="s">'torch.legacy'</span><span class="p">,</span> <span class="s">'torch.multiprocessing'</span><span class="p">,</span> <span class="s">'torch.nn'</span><span class="p">,</span> <span class="s">'torch.optim'</span><span class="p">,</span> <span class="s">'torch.sparse'</span><span class="p">,</span> <span class="s">'torch.utils'</span><span class="p">,</span> <span class="s">'torch.autograd._functions'</span><span class="p">,</span> <span class="s">'torch.backends.cudnn'</span><span class="p">,</span> <span class="s">'torch.legacy.nn'</span><span class="p">,</span> <span class="s">'torch.legacy.optim'</span><span class="p">,</span> <span class="s">'torch.nn._functions'</span><span class="p">,</span> <span class="s">'torch.nn.backends'</span><span class="p">,</span> <span class="s">'torch.nn.modules'</span><span class="p">,</span> <span class="s">'torch.nn.parallel'</span><span class="p">,</span> <span class="s">'torch.nn.utils'</span><span class="p">,</span> <span class="s">'torch.nn._functions.thnn'</span><span class="p">,</span> <span class="s">'torch.utils.data'</span><span class="p">,</span> <span class="s">'torch.utils.ffi'</span><span class="p">,</span> <span class="s">'torch.utils.serialization'</span><span class="p">,</span> <span class="s">'torch.utils.trainer'</span><span class="p">,</span> <span class="s">'torch.utils.backcompat'</span><span class="p">,</span> <span class="s">'torch.utils.trainer.plugins'</span><span class="p">]</span>
</code></pre></div></div>

<p>As we can see, <code class="language-plaintext highlighter-rouge">find_package</code> has recursively traversed the <code class="language-plaintext highlighter-rouge">torch</code> directory, finding all the directory paths that have an <code class="language-plaintext highlighter-rouge">__init__.py</code> file.</p>

<p>When building with Setuptools, the tool creates a <code class="language-plaintext highlighter-rouge">build</code> directory in the distribution root, i.e. the same location as the <code class="language-plaintext highlighter-rouge">setup.py</code> file. Because PyTorch is composed of both “Pure” python modules and Extension Modules, we need to preserve information about the Operating System and Python version used when performing the build. So if we look in my <code class="language-plaintext highlighter-rouge">build</code> directory, we see:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:pytorch <span class="o">(</span>master<span class="o">)</span><span class="nv">$ </span><span class="nb">ls </span>build
lib.linux-x86_64-3.6  temp.linux-x86_64-3.6
</code></pre></div></div>

<p>This indicates that I’ve built the project on <code class="language-plaintext highlighter-rouge">linux-x86-64</code> using Python 3.6. The lib directory contains the library files, while the temp directory contains files generated during the build that aren’t needed in the final installation.</p>

<p>Because “Pure” python modules are just Python code, and don’t need to be “compiled”, the <code class="language-plaintext highlighter-rouge">build_py</code> process simply copies files from their locations as found by <code class="language-plaintext highlighter-rouge">find_packages</code> to the equivalent location in <code class="language-plaintext highlighter-rouge">build/</code>. So our build output is littered with lines like:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>copying torch/autograd/_functions/blas.py -&gt; build/lib.linux-x86_64-3.6/torch/autograd/_functions
</code></pre></div></div>

<p>We also noted earlier that we could pass files and directories to the <code class="language-plaintext highlighter-rouge">package_data</code> keyword argument to the main <code class="language-plaintext highlighter-rouge">setup()</code> function, and that Setuptools would handle copying those files to the installation location. During <code class="language-plaintext highlighter-rouge">build_py</code>, these files are copied to the <code class="language-plaintext highlighter-rouge">build/</code> directory, so we also see lines like:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>copying torch/lib/libTH.so.1 -&gt; build/lib.linux-x86_64-3.6/torch/lib
...
copying torch/lib/include/THC/generic/THCTensor.h -&gt; build/lib.linux-x86_64-3.6/torch/lib/include/THC/generic
</code></pre></div></div>

<h3 id="building-the-extension-modules">Building the Extension Modules</h3>

<p>Finally, we need to build the Extension Modules, i.e. the PyTorch modules written in C++ using the CPython backend. This also constitutes the majority of the code logic in <code class="language-plaintext highlighter-rouge">setup.py</code>. Our overridden <code class="language-plaintext highlighter-rouge">build_ext</code> Command has some special logic before the extensions themselves are actually built:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tools.cwrap</span> <span class="kn">import</span> <span class="n">cwrap</span>
<span class="kn">from</span> <span class="nn">tools.cwrap.plugins.THPPlugin</span> <span class="kn">import</span> <span class="n">THPPlugin</span>
<span class="kn">from</span> <span class="nn">tools.cwrap.plugins.ArgcountSortPlugin</span> <span class="kn">import</span> <span class="n">ArgcountSortPlugin</span>
<span class="kn">from</span> <span class="nn">tools.cwrap.plugins.AutoGPU</span> <span class="kn">import</span> <span class="n">AutoGPU</span>
<span class="kn">from</span> <span class="nn">tools.cwrap.plugins.BoolOption</span> <span class="kn">import</span> <span class="n">BoolOption</span>
<span class="kn">from</span> <span class="nn">tools.cwrap.plugins.KwargsPlugin</span> <span class="kn">import</span> <span class="n">KwargsPlugin</span>
<span class="kn">from</span> <span class="nn">tools.cwrap.plugins.NullableArguments</span> <span class="kn">import</span> <span class="n">NullableArguments</span>
<span class="kn">from</span> <span class="nn">tools.cwrap.plugins.CuDNNPlugin</span> <span class="kn">import</span> <span class="n">CuDNNPlugin</span>
<span class="kn">from</span> <span class="nn">tools.cwrap.plugins.WrapDim</span> <span class="kn">import</span> <span class="n">WrapDim</span>
<span class="kn">from</span> <span class="nn">tools.cwrap.plugins.AssertNDim</span> <span class="kn">import</span> <span class="n">AssertNDim</span>
<span class="kn">from</span> <span class="nn">tools.cwrap.plugins.Broadcast</span> <span class="kn">import</span> <span class="n">Broadcast</span>
<span class="kn">from</span> <span class="nn">tools.cwrap.plugins.ProcessorSpecificPlugin</span> <span class="kn">import</span> <span class="n">ProcessorSpecificPlugin</span>
        <span class="n">thp_plugin</span> <span class="o">=</span> <span class="n">THPPlugin</span><span class="p">()</span>
        <span class="n">cwrap</span><span class="p">(</span><span class="s">'torch/csrc/generic/TensorMethods.cwrap'</span><span class="p">,</span> <span class="n">plugins</span><span class="o">=</span><span class="p">[</span>
            <span class="n">ProcessorSpecificPlugin</span><span class="p">(),</span> <span class="n">BoolOption</span><span class="p">(),</span> <span class="n">thp_plugin</span><span class="p">,</span>
            <span class="n">AutoGPU</span><span class="p">(</span><span class="n">condition</span><span class="o">=</span><span class="s">'IS_CUDA'</span><span class="p">),</span> <span class="n">ArgcountSortPlugin</span><span class="p">(),</span> <span class="n">KwargsPlugin</span><span class="p">(),</span>
            <span class="n">AssertNDim</span><span class="p">(),</span> <span class="n">WrapDim</span><span class="p">(),</span> <span class="n">Broadcast</span><span class="p">()</span>
        <span class="p">])</span>
        <span class="n">cwrap</span><span class="p">(</span><span class="s">'torch/csrc/cudnn/cuDNN.cwrap'</span><span class="p">,</span> <span class="n">plugins</span><span class="o">=</span><span class="p">[</span>
            <span class="n">CuDNNPlugin</span><span class="p">(),</span> <span class="n">NullableArguments</span><span class="p">()</span>
        <span class="p">])</span>
</code></pre></div></div>

<p>Recall above that I documented that we auto-generated C++ code for calling into the <code class="language-plaintext highlighter-rouge">THNN</code> etc. libraries. Here is where we bind <code class="language-plaintext highlighter-rouge">TH</code>, <code class="language-plaintext highlighter-rouge">THC</code> and <code class="language-plaintext highlighter-rouge">CuDNN</code>. We take the YAML declarations in <code class="language-plaintext highlighter-rouge">TensorMethods.cwrap</code>, and use them to generate output C++ source files that contain implementations that work within PyTorch’s C++ Ecosystem. For example, a simple declaration like zero_:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[
  name: zero_
  cname: zero
  return: self
  arguments:
    - THTensor* self
]]
</code></pre></div></div>

<p>Generates code like:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">PyObject</span> <span class="o">*</span> <span class="nf">THPTensor_</span><span class="p">(</span><span class="n">zero_</span><span class="p">)(</span><span class="n">PyObject</span> <span class="o">*</span><span class="n">self</span><span class="p">,</span> <span class="n">PyObject</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">PyObject</span> <span class="o">*</span><span class="n">kwargs</span><span class="p">)</span> <span class="p">{</span>
	<span class="p">...</span>
	<span class="n">THTensor_</span><span class="p">(</span><span class="n">zero</span><span class="p">)(</span><span class="n">LIBRARY_STATE</span> <span class="n">arg_self</span><span class="p">);</span>
	<span class="p">...</span>
<span class="p">}</span>
</code></pre></div></div>

<p>In the previous post we documented how these functions are tied to specific Tensor types, so I won’t expand on that there. For the build process its enough to know that these C++ files are generated prior to the extension being built, because these source files are used during Extension compilation.</p>

<h3 id="specifying-the-extensions">Specifying the Extensions</h3>

<p>Unlike pure modules, it’s not enough just to list modules or packages and expect the Setuptools to go out and find the right files; you have to specify the extension name, source file(s), and any compile/link requirements (include directories, libraries to link with, etc.).</p>

<p>The bulk (200~ LOC at the time of this writing) of the <code class="language-plaintext highlighter-rouge">setup.py</code> goes into specifying how to build these Extensions. Here, some of the choices we make in <code class="language-plaintext highlighter-rouge">build_all.sh</code> begin to make sense. For example, we saw that our build script specified a <code class="language-plaintext highlighter-rouge">tmp_install</code> directory where we installed our backend libraries. In our <code class="language-plaintext highlighter-rouge">setup.py</code> code, we reference this directory when adding to the list of directories containing header files to include:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># tmp_install_path is torch/lib/tmp_install
</span><span class="n">include_dirs</span> <span class="o">+=</span> <span class="p">[</span>
    <span class="n">cwd</span><span class="p">,</span>
    <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">cwd</span><span class="p">,</span> <span class="s">"torch"</span><span class="p">,</span> <span class="s">"csrc"</span><span class="p">),</span>
    <span class="n">tmp_install_path</span> <span class="o">+</span> <span class="s">"/include"</span><span class="p">,</span>
    <span class="n">tmp_install_path</span> <span class="o">+</span> <span class="s">"/include/TH"</span><span class="p">,</span>
    <span class="n">tmp_install_path</span> <span class="o">+</span> <span class="s">"/include/THPP"</span><span class="p">,</span>
    <span class="n">tmp_install_path</span> <span class="o">+</span> <span class="s">"/include/THNN"</span><span class="p">,</span>
</code></pre></div></div>

<p>Similarly, we copied the shared object libraries to <code class="language-plaintext highlighter-rouge">torch/csrc</code> at the end of the <code class="language-plaintext highlighter-rouge">build_all.sh</code> script. We reference these locations directly in our <code class="language-plaintext highlighter-rouge">setup.py</code> code when identifying libraries that we may link against:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># lib_path is torch/lib
</span><span class="n">TH_LIB</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">lib_path</span><span class="p">,</span> <span class="s">'libTH.so.1'</span><span class="p">)</span>
<span class="n">THS_LIB</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">lib_path</span><span class="p">,</span> <span class="s">'libTHS.so.1'</span><span class="p">)</span>
<span class="n">THC_LIB</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">lib_path</span><span class="p">,</span> <span class="s">'libTHC.so.1'</span><span class="p">)</span>
<span class="n">THCS_LIB</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">lib_path</span><span class="p">,</span> <span class="s">'libTHCS.so.1'</span><span class="p">)</span>
<span class="n">THNN_LIB</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">lib_path</span><span class="p">,</span> <span class="s">'libTHNN.so.1'</span><span class="p">)</span>
<span class="c1"># ...
</span></code></pre></div></div>

<p>Let’s consider how we build the main <code class="language-plaintext highlighter-rouge">torch._C</code> Extension Module:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">C</span> <span class="o">=</span> <span class="n">Extension</span><span class="p">(</span><span class="s">"torch._C"</span><span class="p">,</span>
              <span class="n">libraries</span><span class="o">=</span><span class="n">main_libraries</span><span class="p">,</span>
              <span class="n">sources</span><span class="o">=</span><span class="n">main_sources</span><span class="p">,</span>
              <span class="n">language</span><span class="o">=</span><span class="s">'c++'</span><span class="p">,</span>
              <span class="n">extra_compile_args</span><span class="o">=</span><span class="n">main_compile_args</span> <span class="o">+</span> <span class="n">extra_compile_args</span><span class="p">,</span>
              <span class="n">include_dirs</span><span class="o">=</span><span class="n">include_dirs</span><span class="p">,</span>
              <span class="n">library_dirs</span><span class="o">=</span><span class="n">library_dirs</span><span class="p">,</span>
              <span class="n">extra_link_args</span><span class="o">=</span><span class="n">extra_link_args</span> <span class="o">+</span> <span class="n">main_link_args</span> <span class="o">+</span> <span class="p">[</span><span class="n">make_relative_rpath</span><span class="p">(</span><span class="s">'lib'</span><span class="p">)],</span>
              <span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>The main libraries are all the libraries we link against. This includes things like <code class="language-plaintext highlighter-rouge">shm</code>, PyTorch’s shared memory management library, and also system libraries like <code class="language-plaintext highlighter-rouge">cudart</code> and <code class="language-plaintext highlighter-rouge">cudnn</code>. Note that the <code class="language-plaintext highlighter-rouge">TH</code> libraries <em>are not</em> listed here</li>
  <li>The main sources are the C++ files that make up the C++ backend for PyTorch</li>
  <li>The compile args are various flags that configure compilation. For example, we might want to add debug flags when compiling in debug mode</li>
  <li>The include dirs are the paths to all the directories containing header files. This is also another example where the <code class="language-plaintext highlighter-rouge">build_all.sh</code> script is important - for example, we look for the <code class="language-plaintext highlighter-rouge">TH</code> header files in <code class="language-plaintext highlighter-rouge">torch/lib/tmp_install/include/TH</code> - which is the install location we specified with our CMake configuration</li>
  <li>The library dirs are directories to search for shared libraries at link time. For example, we include <code class="language-plaintext highlighter-rouge">torch/lib</code> - the location we copied our <code class="language-plaintext highlighter-rouge">.so</code> files to at the end of <code class="language-plaintext highlighter-rouge">build_all.sh</code>, but also the paths to the CUDA and CuDNN directories</li>
  <li>The link arguments are used when linking object files together to create the extension. In PyTorch, this includes more <em>normal</em> options like decided to link <code class="language-plaintext highlighter-rouge">libstdc++</code> statically. However, there is one key component: <strong>this is where we link the backend TH libraries</strong>. Note that we have lines like:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The explicit paths to .so files we described above
</span><span class="n">main_link_args</span> <span class="o">=</span> <span class="p">[</span><span class="n">TH_LIB</span><span class="p">,</span> <span class="n">THS_LIB</span><span class="p">,</span> <span class="n">THPP_LIB</span><span class="p">,</span> <span class="n">THNN_LIB</span><span class="p">]</span>
</code></pre></div></div>

<p>You might be wondering why we do this as opposed to adding these libraries to the list we pass to the <code class="language-plaintext highlighter-rouge">libraries</code> keyword argument. After all, that is a list of libraries to link against. The issue is that Lua Torch installs often set the <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> variable, and thus we could mistakenly link against a <code class="language-plaintext highlighter-rouge">TH</code> library built for Lua Torch, instead of the library we have built locally. This would be problematic because the code could be out of date, and also there are various configuration options for Lua Torch’s <code class="language-plaintext highlighter-rouge">TH</code> that would not play nicely with PyTorch.</p>

<p>As such, we manually specify the paths to the shared libraries we generated directly to the linker.</p>

<p>There are other extensions needed to power PyTorch and they are built in a similar way. The Setuptools library invokes the C++ compiler and linker to build all of these extensions. If the builds succeed, we have successfully <em>built</em> the PyTorch library and we can move on to installation.</p>

<h3 id="installation">Installation</h3>

<p>After building has finished, installation is quite simple. We simply have to copy everything from our <code class="language-plaintext highlighter-rouge">build/lib.linux-x86_64-3.6</code> directory to the appropriate installation directory. Recall that we noted above that this directory is the <code class="language-plaintext highlighter-rouge">site_packages</code> directory associated with our Python binary. As a result, we see lines like:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>running install_lib
creating /home/killeent/local/miniconda2/envs/p3/lib/python3.6/site-packages/torch
copying build/lib.linux-x86_64-3.6/torch/_C.cpython-36m-x86_64-linux-gnu.so -&gt; /home/killeent/local/miniconda2/envs/p3/lib/python3.6/site-packages/torch
copying build/lib.linux-x86_64-3.6/torch/_dl.cpython-36m-x86_64-linux-gnu.so -&gt; /home/killeent/local/miniconda2/envs/p3/lib/python3.6/site-packages/torch
creating /home/killeent/local/miniconda2/envs/p3/lib/python3.6/site-packages/torch/_thnn
copying build/lib.linux-x86_64-3.6/torch/_thnn/_THNN.cpython-36m-x86_64-linux-gnu.so -&gt; /home/killeent/local/miniconda2/envs/p3/lib/python3.6/site-packages/torch/_thnn
copying build/lib.linux-x86_64-3.6/torch/_thnn/_THCUNN.cpython-36m-x86_64-linux-gnu.so -&gt; /home/killeent/local/miniconda2/envs/p3/lib/python3.6/site-packages/torch/_thnn
</code></pre></div></div>

<p>Finally lets power up the Python interpreter. When the Python interpreter executes an import statement, it searches for Python code and extension modules along a search path. A default value for the path is configured into the Python binary when the interpreter is built.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># note we are now in my home directory</span>
<span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:~<span class="nv">$ </span>python
Python 3.6.1 |Continuum Analytics, Inc.| <span class="o">(</span>default, Mar 22 2017, 19:54:23<span class="o">)</span>
<span class="o">[</span>GCC 4.4.7 20120313 <span class="o">(</span>Red Hat 4.4.7-1<span class="o">)]</span> on linux
Type <span class="s2">"help"</span>, <span class="s2">"copyright"</span>, <span class="s2">"credits"</span> or <span class="s2">"license"</span> <span class="k">for </span>more information.
<span class="o">&gt;&gt;&gt;</span> import sys
<span class="o">&gt;&gt;&gt;</span> sys.path
<span class="o">[</span><span class="s1">''</span>, <span class="s1">'/home/killeent/local/miniconda2/envs/p3/lib/python36.zip'</span>, <span class="s1">'/home/killeent/local/miniconda2/envs/p3/lib/python3.6'</span>, <span class="s1">'/home/killeent/local/miniconda2/envs/p3/lib/python3.6/lib-dynload'</span>, <span class="s1">'/home/killeent/.local/lib/python3.6/site-packages'</span>, <span class="s1">'/home/killeent/local/miniconda2/envs/p3/lib/python3.6/site-packages'</span>, <span class="s1">'/home/killeent/github/pytorch'</span>, <span class="s1">'/home/killeent/local/miniconda2/envs/p3/lib/python3.6/site-packages/setuptools-27.2.0-py3.6.egg'</span><span class="o">]</span>
</code></pre></div></div>

<p>As we can see, the <code class="language-plaintext highlighter-rouge">site-packages</code> directory we copied our PyTorch installation to is part of search path. Now let’s load the <code class="language-plaintext highlighter-rouge">torch</code> module and see its location:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">torch</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">inspect</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inspect</span><span class="p">.</span><span class="n">getfile</span><span class="p">(</span><span class="n">torch</span><span class="p">)</span>
<span class="s">'/home/killeent/local/miniconda2/envs/p3/lib/python3.6/site-packages/torch/__init__.py'</span>
</code></pre></div></div>

<p>As we can see, we have loaded the module from <code class="language-plaintext highlighter-rouge">site_packages</code> as expected - and our build and installation is successful!</p>

<p><strong>Note:</strong> Python prepends the empty string to <code class="language-plaintext highlighter-rouge">sys.path</code> to represent the current working directory - making it the first place we search for a module. So if we run Python from the pytorch directory, we would accidentally load the local version of PyTorch rather than our installed version. This is something to watch out for.</p>

<h3 id="addendum---developer-efficiency-3rd-party-libraries-things-i-didnt-cover">Addendum - Developer Efficiency, 3rd Party Libraries, Things I Didn’t Cover</h3>

<p>The entire installation loop for PyTorch can be quite time-consuming. On my devserver, it takes around 5 minutes for an installation from source. Often times, when developing PyTorch, we only want to work on a subset of the entire project, and re-build only that subset in order to test changes. Fortunately, our build system enables this.</p>

<h3 id="setuptools-develop-mode">Setuptools Develop Mode</h3>

<p>The main tool that supports this is Setuptools <code class="language-plaintext highlighter-rouge">develop</code> command. The documentation states that:</p>

<blockquote>
  <p>This command allows you to deploy your project’s source for use in one or more “staging areas” where it will be available for importing. This deployment is done in such a way that changes to the project source are immediately available in the staging area(s), without needing to run a build or install step after each change.</p>
</blockquote>

<p>But how does it work? Suppose we run <code class="language-plaintext highlighter-rouge">python setup.py build develop</code> in the PyTorch directory. The <code class="language-plaintext highlighter-rouge">build</code> command is run, building our dependencies (<code class="language-plaintext highlighter-rouge">TH</code>, <code class="language-plaintext highlighter-rouge">THPP</code>, etc.) and the extension libraries. However, if we look inside <code class="language-plaintext highlighter-rouge">site-packages</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:site-packages<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-la</span> torch<span class="k">*</span>
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 killeent <span class="nb">users </span>31 Jun 27 08:02 torch.egg-link
</code></pre></div></div>

<p>Looking at the contents of the <code class="language-plaintext highlighter-rouge">torch.egg-link</code> file, it simply references the PyTorch directory:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:site-packages<span class="nv">$ </span><span class="nb">cat </span>torch.egg-link
/home/killeent/github/pytorch
</code></pre></div></div>

<p>If we navigate back to the PyTorch directory, we see there is a new directory <code class="language-plaintext highlighter-rouge">torch.egg-info</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:pytorch <span class="o">(</span>master<span class="o">)</span><span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-la</span> torch.egg-info/
total 28
drwxr-xr-x.  2 killeent <span class="nb">users  </span>4096 Jun 27 08:09 <span class="nb">.</span>
drwxr-xr-x. 10 killeent <span class="nb">users  </span>4096 Jun 27 08:01 ..
<span class="nt">-rw-r--r--</span><span class="nb">.</span>  1 killeent <span class="nb">users     </span>1 Jun 27 08:01 dependency_links.txt
<span class="nt">-rw-r--r--</span><span class="nb">.</span>  1 killeent <span class="nb">users   </span>255 Jun 27 08:01 PKG-INFO
<span class="nt">-rw-r--r--</span><span class="nb">.</span>  1 killeent <span class="nb">users     </span>7 Jun 27 08:01 requires.txt
<span class="nt">-rw-r--r--</span><span class="nb">.</span>  1 killeent <span class="nb">users </span>16080 Jun 27 08:01 SOURCES.txt
<span class="nt">-rw-r--r--</span><span class="nb">.</span>  1 killeent <span class="nb">users    </span>12 Jun 27 08:01 top_level.txt
</code></pre></div></div>

<p>This file contains metadata about the PyTorch project. For example, <code class="language-plaintext highlighter-rouge">requirements.txt</code> lists all of the dependencies for setting up PyTorch:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:pytorch <span class="o">(</span>master<span class="o">)</span><span class="nv">$ </span><span class="nb">cat </span>torch.egg-info/requires.txt
pyyaml
</code></pre></div></div>

<p>Without going into too much detail, <code class="language-plaintext highlighter-rouge">develop</code> allows us to essentially treat the PyTorch repo itself as if it were in <code class="language-plaintext highlighter-rouge">site-packages</code>, so we can import the module and it just works:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:~<span class="nv">$ </span>python
Python 3.6.1 |Continuum Analytics, Inc.| <span class="o">(</span>default, Mar 22 2017, 19:54:23<span class="o">)</span>
<span class="o">[</span>GCC 4.4.7 20120313 <span class="o">(</span>Red Hat 4.4.7-1<span class="o">)]</span> on linux
Type <span class="s2">"help"</span>, <span class="s2">"copyright"</span>, <span class="s2">"credits"</span> or <span class="s2">"license"</span> <span class="k">for </span>more information.
<span class="o">&gt;&gt;&gt;</span> import torch
<span class="o">&gt;&gt;&gt;</span> torch.__file__
<span class="s1">'/home/killeent/github/pytorch/torch/__init__.py'</span>
</code></pre></div></div>

<p>As a result, the following consequences hold:</p>

<ul>
  <li>If we change a Python source file, the changes are automatically picked up, and we don’t have to run any commands to let the Python interpreter <em>see</em> this change</li>
  <li>If we change a C++ Source File in one of the extension libraries, we can re-run the <code class="language-plaintext highlighter-rouge">develop</code> command, it will re-build the extension</li>
</ul>

<p>Thus we can develop the PyTorch codebases seamlessly, and test our changes in an easy way.</p>

<h4 id="working-on-the-dependency-libraries">Working on the Dependency Libraries</h4>

<p>If we are working on the dependencies (e.g. <code class="language-plaintext highlighter-rouge">TH</code>, <code class="language-plaintext highlighter-rouge">THPP</code>, etc.) we can re-build our changes more quickly by simply running the <code class="language-plaintext highlighter-rouge">build_deps</code> command directly. This will automatically call into <code class="language-plaintext highlighter-rouge">build_all.sh</code> to re-build our libraries, and copy the generated libraries appropriately. If we are using Setuptools <code class="language-plaintext highlighter-rouge">develop</code> mode, we will be using the local extension library built in the PyTorch directory. Because we have specified the paths to the shared libraries when compiling our Extension Libraries, the changes will be picked up:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># we are using the local extension</span>
<span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:~<span class="nv">$ </span>python
Python 3.6.1 |Continuum Analytics, Inc.| <span class="o">(</span>default, Mar 22 2017, 19:54:23<span class="o">)</span>
<span class="o">[</span>GCC 4.4.7 20120313 <span class="o">(</span>Red Hat 4.4.7-1<span class="o">)]</span> on linux
Type <span class="s2">"help"</span>, <span class="s2">"copyright"</span>, <span class="s2">"credits"</span> or <span class="s2">"license"</span> <span class="k">for </span>more information.
<span class="o">&gt;&gt;&gt;</span> import torch
<span class="o">&gt;&gt;&gt;</span> torch._C.__file__
<span class="s1">'/home/killeent/github/pytorch/torch/_C.cpython-36m-x86_64-linux-gnu.so'</span>

<span class="c"># it references the local shared object library we just re-built</span>
<span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:~<span class="nv">$ </span>ldd /home/killeent/github/pytorch/torch/_C.cpython-36m-x86_64-linux-gnu.so
<span class="c"># ...</span>
libTH.so.1 <span class="o">=&gt;</span> /home/killeent/github/pytorch/torch/lib/libTH.so.1 <span class="o">(</span>0x00007f543d0e2000<span class="o">)</span>
<span class="c"># ...</span>
</code></pre></div></div>

<p>As such, we can test any changes here without having to do a full rebuild.</p>

<h4 id="3rd-party-libraries">3rd Party Libraries</h4>

<p>PyTorch has dependencies on some 3rd party libraries. The usual mechanism for using these libraries is to install them via Anaconda, and then link against them. For example, we can use the <code class="language-plaintext highlighter-rouge">mkl</code> library with PyTorch by doing:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># installed to miniconda2/envs/p3/lib/libmkl_intel_lp64.so</span>
conda <span class="nb">install </span>mkl
</code></pre></div></div>

<p>And then as long as we have the path to this <code class="language-plaintext highlighter-rouge">lib</code> directory on our <code class="language-plaintext highlighter-rouge">$CMAKE_PREFIX_PATH</code>, it will successfully find this library when compiling:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># in the site-packages dir</span>
<span class="o">(</span>p3<span class="o">)</span> killeent@devgpu047:torch<span class="nv">$ </span>ldd _C.cpython-36m-x86_64-linux-gnu.so
<span class="c"># ...</span>
libmkl_intel_lp64.so <span class="o">=&gt;</span> /home/killeent/local/miniconda2/envs/p3/lib/libmkl_intel_lp64.so <span class="o">(</span>0x00007f3450bba000<span class="o">)</span>
<span class="c"># ...</span>
</code></pre></div></div>

<h3 id="not-covered-but-also-relevant">Not Covered, But Also Relevant</h3>

<ul>
  <li>How <code class="language-plaintext highlighter-rouge">ccache</code> is used to speed up build times</li>
  <li>How PyTorch’s top-level <code class="language-plaintext highlighter-rouge">__init__.py</code> file handles the initial module import and pulling together all the various modules and extension libraries</li>
  <li>The CMake build system, how the backend libraries are configured and built with CMake</li>
</ul>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="lf-grid">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org" class="footer-logo">
          <img src="/assets/images/logo-icon.svg" alt="PyTorch logo" width="40">
        </a>
      </div>

      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook"><path fill="currentColor" d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z"/></svg>	
        </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X"><path fill="currentColor" d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66"/></svg>
        </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube"><path fill="currentColor" d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z"/></svg>	
        </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn"><rect width="512" height="512" rx="0" fill="currentColor"/><circle fill="#000" cx="142" cy="138" r="37"/><path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198"/><path fill="#000" d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
        </a></li>
        <li><a href="https://social.lfx.dev/@pytorch" rel="me" target="_blank" title="PyTorch on Mastodon">
          <svg fill="currentColor" aria-label="Mastodon" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" xml:space="preserve"><path d="M21.327 8.566c0-4.339-2.843-5.61-2.843-5.61-1.433-.658-3.894-.935-6.451-.956h-.063c-2.557.021-5.016.298-6.45.956 0 0-2.843 1.272-2.843 5.61 0 .993-.019 2.181.012 3.441.103 4.243.778 8.425 4.701 9.463 1.809.479 3.362.579 4.612.51 2.268-.126 3.541-.809 3.541-.809l-.075-1.646s-1.621.511-3.441.449c-1.804-.062-3.707-.194-3.999-2.409a4.523 4.523 0 0 1-.04-.621s1.77.433 4.014.536c1.372.063 2.658-.08 3.965-.236 2.506-.299 4.688-1.843 4.962-3.254.434-2.223.398-5.424.398-5.424zm-3.353 5.59h-2.081V9.057c0-1.075-.452-1.62-1.357-1.62-1 0-1.501.647-1.501 1.927v2.791h-2.069V9.364c0-1.28-.501-1.927-1.502-1.927-.905 0-1.357.546-1.357 1.62v5.099H6.026V8.903c0-1.074.273-1.927.823-2.558.566-.631 1.307-.955 2.228-.955 1.065 0 1.872.409 2.405 1.228l.518.869.519-.869c.533-.819 1.34-1.228 2.405-1.228.92 0 1.662.324 2.228.955.549.631.822 1.484.822 2.558v5.253z"/></svg>
        </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/legal/policies/">Linux Foundation Policies</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">LF Projects, LLC Policies</a>. <a href="https://www.linuxfoundation.org/privacy">Privacy Policy</a> and <a href="https://www.linuxfoundation.org/terms">Terms of Use</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Learn</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/get-started">Get Started</a> 
          </li>
          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/#community-module">Community</a>
          </li>
          <li>
            <a href="https://discuss.pytorch.org">Forums</a>
          </li>
          <li>
            <a href="/resources">Developer Resources</a>
          </li>
          <li>
            <a href="/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Docs</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/docs">PyTorch</a>
          </li>
          <li>
            <a href="/pytorch-domains">PyTorch Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/blog">PyTorch Blog</a>
          </li>
          <li>
            <a href="/community-blog">Community Blog</a>
          </li>
          <li>
            <a href="/videos">Videos</a>
          </li>
          <li>
            <a href="/community-stories">Community Stories</a>
          </li>
          <li>
            <a href="/events">Events</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="/governing-board">Governing Board</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a href="/join">Become a Member</a>
        </li>
        <li class="resources-mobile-menu-title">
          <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub"><div id="topnav-gh-icon"></div></a>
        </li>

      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header, .announcement-header, .comm-stories-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

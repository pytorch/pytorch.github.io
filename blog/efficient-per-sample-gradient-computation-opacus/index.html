<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      Differential Privacy Series Part 2 | Efficient Per-Sample Gradient Computation in Opacus | PyTorch
    
  </title>
  
  <meta property="og:title" content="Differential Privacy Series Part 2 | Efficient Per-Sample Gradient Computation in Opacus" />
  <meta property="og:description" content="Introduction

" />
  <meta property="og:image" content="https://pytorch.org/assets/images/image-opacus.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Differential Privacy Series Part 2 | Efficient Per-Sample Gradient Computation in Opacus">
  <meta name="twitter:description" content="Introduction

" />


<meta property="og:type" content="website" />
<meta name="robots" content="index, follow" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> 
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
</head>


<body class="blog">
    <div class="main-background blog-background blog-detail-background"></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow" href="/ecosystem">
          Ecosystem
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/pted/2021">
            <span class="dropdown-title">Ecosystem Day - 2021</span>
            <p>See the posters presented at ecosystem day 2021</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item ">
      <a href="/mobile">Mobile</a>
    </li>

    <li class="main-menu-item active">
      <a href="/blog">Blog</a>
    </li>

    <li class="main-menu-item">
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="doc-option with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/docs">
            <span class="dropdown-title docs-title">PyTorch</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/audio">
            <span class="dropdown-title docs-title">torchaudio</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/text">
            <span class="dropdown-title docs-title">torchtext</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/vision">
            <span class="dropdown-title docs-title">torchvision</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/serve">
            <span class="dropdown-title docs-title">TorchServe</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/xla/release/1.6/index.html">
            <span class="dropdown-title docs-title">PyTorch on XLA Devices</span>
            <p></p>
          </a>
        </div>
      </div>
    </li>

    

    <li class="main-menu-item ">

      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Resources
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/features">
            <span class=dropdown-title>About</span>
            <p>Learn about PyTorch‚Äôs features and capabilities</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/hub">
            <span class=dropdown-title>Models (Beta)</span>
            <p>Discover, publish, and reuse pre-trained models</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">September 15, 2021</p>
            <h1>
                <a class="blog-title">Differential Privacy Series Part 2 | Efficient Per-Sample Gradient Computation in Opacus</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Ashkan Yousefpour, Davide Testuggine, Alex Sablayrolles, and Ilya Mironov
                      
                    </p>
                    <h2 id="introduction">Introduction</h2>

<p>In our <a href="https://medium.com/pytorch/differential-privacy-series-part-1-dp-sgd-algorithm-explained-12512c3959a3">previous blog post</a>, we went over the basics of the DP-SGD algorithm and introduced <a href="https://opacus.ai">Opacus</a>, a PyTorch library for training ML models with differential privacy. In this blog post, we explain how performance-improving vectorized computation is done in Opacus and why Opacus can compute ‚Äúper-sample gradients‚Äù a lot faster than ‚Äúmicrobatching‚Äù (read on to see what all these terms mean!)</p>

<h2 id="context">Context</h2>

<p>Recall that differential privacy is all about worst-case guarantees, which means we need to check the gradient of each and every sample in a batch of data.</p>

<p>Conceptually, this is akin to writing the following PyTorch code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">lr</span><span class="p">)</span>

<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">Dataloader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="n">all_per_sample_gradients</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># will have len = batch_size
</span>    <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Now p.grad for this x is filled
</span>
        <span class="c1"># Need to clone it to save it
</span>        <span class="n">per_sample_gradients</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">detach</span><span class="p">().</span><span class="n">clone</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">()]</span>

        <span class="n">all_per_sample_gradients</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">per_sample_gradients</span><span class="p">)</span>
        <span class="n">model</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># p.grad is cumulative so we'd better reset it
</span></code></pre></div></div>

<p>While the above procedure (called the ‚Äúmicro batch method‚Äù, or ‚Äúmicro batching‚Äù) does indeed yield correct per-sample gradients, it‚Äôs grossly inefficient: GPUs really like vectorized computation, and going sample-by-sample in a for-loop entirely misses that. We acknowledged this in the ending of our last entry, leaving the explanation of how to make this faster for a later post. This is it, folks!</p>

<h2 id="vectorized-computation">Vectorized Computation</h2>

<p>One of the features of Opacus is ‚Äúvectorized computation‚Äù, in that it can compute per-sample gradients a lot faster than microbatching (they depend on the model, but we observed speedups from ~10x for small MNIST examples to ~50x for Transformers). Microbatching is simply not fast enough to run experiments and conduct research.</p>

<p>So, how do we do vectorized computation in Opacus? We derive the per-sample gradient formula, and implement a vectorized version of it. We will get to this soon. Let us mention that there are other methods  (like <a href="https://arxiv.org/abs/1510.01799">this</a> and <a href="https://arxiv.org/abs/2009.03106">this</a>) that rely on computing the norm of the per-sample gradients directly. It is worth noting that since these approaches are based on computing the norm of the per-sample gradients, they do two passes of back-propagation to compute the per-sample gradients: one pass for obtaining the norm, and one pass for using the norm as a weight (see the links above for details). Although they are considered efficient, in Opacus we set out to be even more efficient (!) and do everything in one back-propagation pass.</p>

<p>In this blog post, we focus on the approach for efficiently computing per-sample gradients that is based on deriving the per-sample gradient formula and implementing a vectorized version of it. To make this blog post short, we focus on simple linear layers - building blocks for multi-layer perceptrons (MLPs). In our next blog post, we will talk about how we extend this approach to other layers (e.g., convolutions, or LSTMs) in Opacus.</p>

<h2 id="efficient-per-sample-gradient-computation-for-mlp">Efficient Per-Sample Gradient Computation for MLP</h2>

<p>To understand the idea for efficiently computing per-sample gradients, let‚Äôs start by talking about how AutoGrad works in the commonly-used deep learning frameworks. We‚Äôll focus on PyTorch from now on, but to the best of our knowledge the same applies to other frameworks (with the exception of Jax).</p>

<p>For simplicity of explanation, we focus on one linear layer in a neural network, with weight matrix W. Also, we omit the bias from the forward pass equation: assume the forward pass is denoted by Y=WX where X is the input and Y is the output of the linear layer. If we are processing a single sample, X is a vector. On the other hand, if we are processing a batch (and that‚Äôs what we do in Opacus), X is a matrix of size d* B, with B columns (B is the batch size), where each column is an input vector of dimension d. Similarly, the output matrix Y would be of size r* B where each column is the output vector corresponding to an element in the batch and r is the output dimension.</p>

<p>The forward pass can be written as the following equation that captures the computation for each element in the matrix Y:
<img src="https://render.githubusercontent.com/render/math?math=Yi(b)=j=1dWi,jXj(b)" /></p>

<p>We will return to this equation shortly. <img src="https://render.githubusercontent.com/render/math?math=Yi(b)" /> denotes the element at row i and column b (remember that the dimension of Y is r*B).</p>

<p>In any machine learning problem, we normally need the derivative of the loss with respect to weights W. Comparably, in Opacus we need the ‚Äúper-sample‚Äù version of that, meaning, per-sample derivative of the loss with respect to weights W. Let‚Äôs first get the derivative of the loss with respect to weights, and soon, we will get to the per-sample part.</p>

<p>To obtain the derivative of the loss with respect to weights, we use the chain rule, whose general form is:</p>

<p><img src="https://render.githubusercontent.com/render/math?math=Lz=LY*Yz," /></p>

<p>which can be written as</p>

<p><img src="https://render.githubusercontent.com/render/math?math=Lz=b=1Bi'=1rLYi'(b)Yi'(b)z" />.</p>

<p>Now, we can replace <img src="https://render.githubusercontent.com/render/math?math=z" /> with <img src="https://render.githubusercontent.com/render/math?math=Wi,j" /> and get</p>

<p><img src="https://render.githubusercontent.com/render/math?math=LWi,j=b=1Bi'=1rLYi'(b)Yi'(b)Wi,j" />.</p>

<p>We know from the equation <img src="https://render.githubusercontent.com/render/math?math=Y=WX" /> that <img src="https://render.githubusercontent.com/render/math?math=Yi'(b)Wi,j" /> is <img src="https://render.githubusercontent.com/render/math?math=Y=WX" /> that <img src="https://render.githubusercontent.com/render/math?math=Xj(b)" /> when <img src="https://render.githubusercontent.com/render/math?math=i=i" />, and is 0 otherwise. Hence, we will have</p>

<p><img src="https://render.githubusercontent.com/render/math?math=LWi,j=b=1BLYi(b)Xj(b)(*)" /></p>

<p>This equation corresponds to a matrix multiplication in PyTorch.</p>

<p>As we can see, the gradient of loss with respect to the weight relies on the gradient of loss with respect to the output Y. In a regular backpropagation, the gradients of loss with respect to weights (or simply put, the ‚Äúgradients‚Äù) are computed for the output of each layer, but they are reduced (i.e., summed up over the batch). Since Opacus requires computing <strong>per-sample gradients</strong>, what we need is the following</p>

<p><img src="https://render.githubusercontent.com/render/math?math=LbatchWi,j=LYi(b)Xj(b)(**)" /></p>

<p>Note that the two equations are very similar; one equation has the sum over the batch and the other one does not. Let‚Äôs now focus on how we compute the per-sample gradient (equation ** ) in Opacus efficiently.</p>

<p align="center">
<img src="/assets/images/image-opacus.png" width="560" />
<br />
Figure 6. The partition boundary is in the middle of a skip connection
</p>

<p>A bit of notation and terminology. Recall that we used the notation Y = WX for forward pass of a single layer of a neural network. When the neural network has more layers, a better notation would be <img src="https://render.githubusercontent.com/render/math?math=Z(l+1)= W (l+1)Z(l)" />, where <img src="https://render.githubusercontent.com/render/math?math=l" /> corresponds to each layer of the neural network. In that case, we can call the gradients with respect to any activations <img src="https://render.githubusercontent.com/render/math?math=Z(l)" /> the ‚Äúhighway gradients‚Äù and the gradients with respect to the weights the ‚Äúexit gradients‚Äù. They are shown in this picture.</p>

<p>If we go with this picture, explaining the issue with Autograd is a one-liner: highway gradients retain per-sample information, but exit gradients do not. Or, highway gradients are per-sample, but exit gradients are not necessarily. This is unfortunate because the per-sample exit gradients are exactly what we need!</p>

<p>So here‚Äôs the question for us: given that we do have vectorized information in the highway, can we compute the per-sample exit gradients efficiently?</p>

<p>Luckily for us, there is a solution for this:</p>
<ol>
  <li>Store the activations somewhere.</li>
  <li>Find a way to access the highway gradients.</li>
</ol>

<p>So far, so good; but how do we store the activations and how do we access the highway gradients? Well, PyTorch has a feature to do just these: module (and tensor) hooks! Read on.</p>

<p>Under the hood, PyTorch is event-based and will call the hooks at the right places (your forward and backward functions are indeed being hooked where they need to go). In addition, PyTorch exposes hooks so that anyone can leverage them. The ones we care about here are these:</p>

<ol>
  <li><strong>Parameter hook</strong>. This attaches to a nn.Module‚Äôs Parameter tensor and will always run during the backward pass. The signature is this: <code class="language-plaintext highlighter-rouge">hook(grad) -&gt; Tensor or None</code></li>
  <li><strong>nn.Module hook</strong>. There are two types of these:
    <ol>
      <li><strong>Forward hook</strong>. The signature for this is <code class="language-plaintext highlighter-rouge">hook(module, input, output) -&gt; None</code></li>
      <li><strong>Backward hook</strong>. The signature for this is <code class="language-plaintext highlighter-rouge">hook(module, grad_input, grad_output) -&gt; Tensor or None</code></li>
    </ol>
  </li>
</ol>

<p>To learn more about these fundamental primitives, check out our <a href="https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks">official tutorial</a> on hooks, or one of the excellent explainers, such as <a href="https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/">Paperspace‚Äôs</a> or this <a href="https://www.kaggle.com/sironghuang/understanding-pytorch-hooks">Kaggle notebook</a>. Finally, if you want to play with hooks more interactively, we also made a <a href="https://colab.research.google.com/drive/1zDidGCNI3DJk1oSPIpmB89b5cCWyuHao?usp=sharing">notebook</a> for you.</p>

<p>We use two hooks, one forward hook and one backward hook. In the forward hook, we simply store the activations:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
    <span class="n">module</span><span class="p">.</span><span class="n">activations</span> <span class="o">=</span> <span class="nb">input</span>
</code></pre></div></div>

<p>In the backward hook, we use the grad_output (highway gradient), along with the stored activations (input to the layer) to compute the per-sample gradient as below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">backward_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
    <span class="n">module</span><span class="p">.</span><span class="n">grad_sample</span> <span class="o">=</span> <span class="n">compute_grad_sample</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">activations</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span>
</code></pre></div></div>

<p>Now the final piece of the puzzle is the computation of the per-sample gradient itself, in the method <code class="language-plaintext highlighter-rouge">compute_grad_sample</code> above. Recall from Equation (* ) that the (average) gradient of loss with respect to the weights is the result of a matrix multiplication. In order to get the per-sample gradient, we want to remove the sum reduction, as in Equation (** ).  This corresponds to replacing the matrix multiplication with a batched outer product. Luckily for us, torch einsum allows us to do that in vectorized form. The method <code class="language-plaintext highlighter-rouge">compute_grad_sample</code> is defined based on einsum throughout our code. For instance, for the linear layer, the meat of the code is</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_linear_grad_sample</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
    <span class="s">"""A: activations   B: backpropagations"""</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"n...i,n...j-&gt;nij"</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
</code></pre></div></div>

<p>You can find the full implementation for the linear module <a href="https://github.com/pytorch/opacus/blob/204328947145d1759fcb26171368fcff6d652ef6/opacus/grad_sample/linear.py">here</a>. The actual code has some bookkeeping around the einsum call, but the einsum call is the main building block of the efficient per-sample computation for us.</p>

<p>Since this post is already long, we refer the interested reader to read about <a href="https://rockt.github.io/2018/04/30/einsum">einsum in PyTorch</a> and do not get into the details of einsum. However, we really encourage you to check it out, as it‚Äôs kind of a magical thing! Just as an example, a matrix multiplication describe in</p>

<p><img src="https://render.githubusercontent.com/render/math?math=Cij=kAikBkj" /></p>

<p>can be implemented beautifully in this line:</p>

<p><code class="language-plaintext highlighter-rouge">c = torch.einsum('ik,kj-&gt;ij', [a, b])</code></p>

<p>We like to highlight that einsum is really the key for us to have vectorized computation. That is it folks! We just explained the last piece of the puzzle, computation of the per-sample gradient.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this blog post, we explained how vectorized computation is done in Opacus and why Opacus can compute per-sample gradients a lot faster than micro batching (for reference, <a href="https://github.com/tensorflow/privacy">TensorFlow Privacy</a> is based on micro batching üòú). We explained the idea to compute per-sample gradients efficiently for an MLP. Stay tuned for more blog posts! In our next blog post, we will talk about how we compute per-sample gradients efficiently for other layers (e.g. convolutions, or LSTMs).</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>Stay up to date</p></li>
          <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
          <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
          <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
          <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>PyTorch Podcasts</p></li>
          <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
          <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
          <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
          <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
        </ul>
      </div>
    </div>

    <div class="privacy-policy">
      <ul>
        <li class="privacy-policy-links"><a href="/assets/tos-oss-privacy-policy/fb-tos-privacy-policy.pdf" target="_blank">Terms</a></li>
        <li class="privacy-policy-links">|</li>
        <li class="privacy-policy-links"><a href="/assets/tos-oss-privacy-policy/fb-oss-privacy-policy.pdf" target="_blank">Privacy</a></li>
      </ul>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/ecosystem">Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/ecosystem/pted/2021">Ecosystem Day 2021</a>
          </li>
        </ul>

        <li class="">
          <a href="/mobile">Mobile</a>
        </li>

        <li class="active">
          <a href="/blog">Blog</a>
        </li>

        <li>
          <a href="https://pytorch.org/tutorials">Tutorials</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/docs">Docs</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/docs">PyTorch</a>
          </li>

          <li class="">
            <a href="/audio">torchaudio</a>
          </li>

          <li class="">
            <a href="/text">torchtext</a>
          </li>

          <li class="">
            <a href="/docs/stable/torchvision">torchvision</a>
          </li>

          <li class="">
            <a href="/elastic">TorchElastic</a>
          </li>

          <li class="">
            <a href="/serve">TorchServe</a>
          </li>

          <li class="">
            <a href="/xla/release/1.6/index.html">PyTorch on XLA Devices</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          Resources
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/features">About</a>
          </li>

          <li>
            <a href="/#community-module">Community</a>
          </li>

          <li>
            <a href="/events">Events</a>
          </li>

          <li class="">
            <a href="/resources">Developer Resources</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.org">Forum</a>
          </li>

          <li class="">
            <a href="/hub">Models (Beta)</a>
          </li>

        </ul>

        <li id="github-mobile-menu-link">
          <a href="https://github.com/pytorch/pytorch">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook‚Äôs Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

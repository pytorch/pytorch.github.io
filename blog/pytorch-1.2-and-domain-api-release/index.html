<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      New Releases: PyTorch 1.2, torchtext 0.4, torchaudio 0.3, and torchvision 0.4 | PyTorch
    
  </title>
  <meta property="og:title" content="PyTorch"/>
<meta property="og:description" content="An open source deep learning platform that provides a seamless path from research prototyping to production deployment."/>
<meta property="og:url" content="https://www.pytorch.org"/>
<meta property="og:type" content="website"/>
<meta property="og:image" content="https://pytorch.org/assets/images/pytorch-logo.png"/>

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('create', 'UA-117752657-2', 'auto', 'newCampaignTracker');
  ga('send', 'pageview');

</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>

  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');

</script>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
</head>


<body class="blog">
    <div class="main-background blog-background blog-detail-background"></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="main-menu-item ">

      <div class="ecosystem-dropdown">
        <a id="dropdownMenuButton" data-toggle="ecosystem-dropdown">
          Ecosystem
        </a>
        <div class="ecosystem-dropdown-menu">
          <a class="nav-dropdown-item" href="/hub">
            <span class=dropdown-title>Models (Beta)</span>
            <p>Discover, publish, and reuse pre-trained models</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class=dropdown-title>Tools & Libraries</span>
            <p>Explore the ecosystem of tools and libraries</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item ">
      <a href="/mobile">Mobile</a>
    </li>

    <li class="main-menu-item active">
      <a href="/blog">Blog</a>
    </li>

    <li class="main-menu-item">
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li class="main-menu-item">
      <a href="/docs">Docs</a>
    </li>

    <li class="main-menu-item ">

      <div class="resources-dropdown">
        <a id="resourcesDropdownButton" data-toggle="resources-dropdown">
          Resources
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/features">
            <span class=dropdown-title>About</span>
            <p>Learn about PyTorch’s features and capabilities</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">August 08, 2019</p>
            <h1>
                <a class="blog-title">New Releases: PyTorch 1.2, torchtext 0.4, torchaudio 0.3, and torchvision 0.4</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Team PyTorch
                      
                    </p>
                    <p>Since the release of PyTorch 1.0, we’ve seen the community expand to add new tools, contribute to a growing set of models available in the PyTorch Hub, and continually increase usage in both research and production.</p>

<p>From a core perspective, PyTorch has continued to add features to support both research and production usage, including the ability to bridge these two worlds via <a href="https://pytorch.org/docs/stable/jit.html">TorchScript</a>. Today, we are excited to announce that we have four new releases including PyTorch 1.2, torchvision 0.4, torchaudio 0.3, and torchtext 0.4. You can get started now with any of these releases at <a href="https://pytorch.org/get-started/locally/">pytorch.org</a>.</p>

<h1 id="pytorch-12">PyTorch 1.2</h1>

<p>With PyTorch 1.2, the open source ML framework takes a major step forward for production usage with the addition of an improved and more polished TorchScript environment. These improvements make it even easier to ship production models, expand support for exporting ONNX formatted models, and enhance module level support for Transformers. In addition to these new features, <a href="https://pytorch.org/docs/stable/tensorboard.html">TensorBoard</a> is now no longer experimental - you can simply type <code class="highlighter-rouge">from torch.utils.tensorboard import SummaryWriter</code> to get started.</p>

<h2 id="torchscript-improvements">TorchScript Improvements</h2>

<p>Since its release in PyTorch 1.0, TorchScript has provided a path to production for eager PyTorch models. The TorchScript compiler converts PyTorch models to a statically typed graph representation, opening up opportunities for
optimization and execution in constrained environments where Python is not available. You can incrementally convert your model to TorchScript, mixing compiled code seamlessly with Python.</p>

<p>PyTorch 1.2 significantly expands TorchScript’s support for the subset of Python used in PyTorch models and delivers a new, easier-to-use API for compiling your models to TorchScript. See the <a href="https://pytorch.org/docs/master/jit.html#migrating-to-pytorch-1-2-recursive-scripting-api">migration guide</a> for details. Below is an example usage of the new API:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
          <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">+</span> <span class="nb">input</span>
        <span class="k">return</span> <span class="n">output</span>

<span class="c"># Compile the model code to a static representation</span>
<span class="n">my_script_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">MyModule</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c"># Save the compiled code and model data so it can be loaded elsewhere</span>
<span class="n">my_script_module</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">"my_script_module.pt"</span><span class="p">)</span>
</code></pre></div></div>

<p>To learn more, see our <a href="https://pytorch.org/tutorials/beginner/Intro_to_TorchScript.html">Introduction to TorchScript</a> and <a href="https://pytorch.org/tutorials/advanced/cpp_export.html">Loading a
PyTorch Model in C++</a> tutorials.</p>

<h2 id="expanded-onnx-export">Expanded ONNX Export</h2>

<p>The <a href="http://onnx.ai/">ONNX</a> community continues to grow with an open <a href="https://github.com/onnx/onnx/wiki/Expanded-ONNX-Steering-Committee-Announced!">governance structure</a> and additional steering committee members, special interest groups (SIGs), and working groups (WGs). In collaboration with Microsoft, we’ve added full support to export ONNX Opset versions 7(v1.2), 8(v1.3), 9(v1.4) and 10 (v1.5). We’ve have also enhanced the constant folding pass to support Opset 10, the latest available version of ONNX. ScriptModule has also been improved including support for multiple outputs, tensor factories, and tuples as inputs and outputs. Additionally, users are now able to register their own symbolic to export custom ops, and specify the dynamic dimensions of inputs during export. Here is a summary of the all of the major improvements:</p>

<ul>
  <li>Support for multiple Opsets including the ability to export dropout, slice, flip, and interpolate in Opset 10.</li>
  <li>Improvements to ScriptModule including support for multiple outputs, tensor factories, and tuples as inputs and outputs.</li>
  <li>More than a dozen additional PyTorch operators supported including the ability to export a custom operator.</li>
  <li>Many big fixes and test infra improvements.</li>
</ul>

<p>You can try out the latest tutorial <a href="https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html">here</a>, contributed by @lara-hdr at Microsoft. A big thank you to the entire Microsoft team for all of their hard work to make this release happen!</p>

<h2 id="nntransformer">nn.Transformer</h2>

<p>In PyTorch 1.2, we now include a standard <a href="https://pytorch.org/docs/stable/nn.html?highlight=transformer#torch.nn.Transformer">nn.Transformer</a> module, based on the paper “<a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a>”. The <code class="highlighter-rouge">nn.Transformer</code> module relies entirely on an <a href="https://pytorch.org/docs/stable/nn.html?highlight=nn%20multiheadattention#torch.nn.MultiheadAttention">attention mechanism</a> to draw global dependencies between input and output.  The individual components of the <code class="highlighter-rouge">nn.Transformer</code> module are designed so they can be adopted independently. For example, the <a href="https://pytorch.org/docs/stable/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder">nn.TransformerEncoder</a> can be used by itself, without the larger <code class="highlighter-rouge">nn.Transformer</code>. The new APIs include:</p>

<ul>
  <li><code class="highlighter-rouge">nn.Transformer</code></li>
  <li><code class="highlighter-rouge">nn.TransformerEncoder</code> and <code class="highlighter-rouge">nn.TransformerEncoderLayer</code></li>
  <li><code class="highlighter-rouge">nn.TransformerDecoder</code> and <code class="highlighter-rouge">nn.TransformerDecoderLayer</code></li>
</ul>

<div class="text-center">
  <img src="https://pytorch.org/assets/images/transformer.png" width="70%" />
</div>

<p>See the <a href="https://pytorch.org/docs/stable/nn.html#transformer-layers">Transformer Layers</a> documentation for more information. See <a href="https://github.com/pytorch/pytorch/releases">here</a> for the full PyTorch 1.2 release notes.</p>

<h1 id="domain-api-library-updates">Domain API Library Updates</h1>

<p>PyTorch domain libraries like torchvision, torchtext, and torchaudio provide convenient access to common datasets, models, and transforms that can be used to quickly create a state-of-the-art baseline. Moreover, they also provide common abstractions to reduce boilerplate code that users might have to otherwise repeatedly write. Since research domains have distinct requirements, an ecosystem of specialized libraries called domain APIs (DAPI) has emerged around PyTorch to simplify the development of new and existing algorithms in a number of fields. We’re excited to release three updated DAPI libraries for text, audio, and vision that compliment the PyTorch 1.2 core release.</p>

<h2 id="torchaudio-03-with-kaldi-compatibility-new-transforms">Torchaudio 0.3 with Kaldi Compatibility, New Transforms</h2>

<div class="text-center">
  <img src="https://pytorch.org/assets/images/spectrograms.png" width="100%" />
</div>

<p>Torchaudio specializes in machine understanding of audio waveforms. It is an ML library that provides relevant signal processing functionality (but is not a general signal processing library). It leverages PyTorch’s GPU support to provide many tools and transformations for waveforms to make data loading and standardization easier and more readable. For example, it offers data loaders for waveforms using sox, and transformations such as spectrograms, resampling, and mu-law encoding and decoding.</p>

<p>We are happy to announce the availability of torchaudio 0.3.0, with a focus on standardization and complex numbers, a transformation (resample) and two new functionals (phase_vocoder, ISTFT), Kaldi compatibility, and a new tutorial. Torchaudio was redesigned to be an extension of PyTorch and a part of the domain APIs (DAPI) ecosystem.</p>

<h3 id="standardization">Standardization</h3>

<p>Significant effort in solving machine learning problems goes into data preparation. In this new release, we’ve updated torchaudio’s interfaces for its transformations to standardize around the following vocabulary and conventions.</p>

<p>Tensors are assumed to have channel as the first dimension and time as the last dimension (when applicable). This makes it consistent with PyTorch’s dimensions. For size names, the prefix <code class="highlighter-rouge">n_</code> is used (e.g. “a tensor of size (<code class="highlighter-rouge">n_freq</code>, <code class="highlighter-rouge">n_mel</code>)”) whereas dimension names do not have this prefix (e.g. “a tensor of dimension (channel, time)”). The input of all transforms and functions now assumes channel first. This is done to be consistent with PyTorch, which has channel followed by the number of samples. The channel parameter of all transforms and functions is now deprecated.</p>

<p>The output of <code class="highlighter-rouge">STFT</code> is (channel, frequency, time, 2), meaning for each channel, the columns are the Fourier transform of a certain window, so as we travel horizontally we can see each column (the Fourier transformed waveform) change over time. This matches the output of librosa so we no longer need to transpose in our test comparisons with <code class="highlighter-rouge">Spectrogram</code>, <code class="highlighter-rouge">MelScale</code>, <code class="highlighter-rouge">MelSpectrogram</code>, and <code class="highlighter-rouge">MFCC</code>. Moreover, because of these new conventions, we deprecated <code class="highlighter-rouge">LC2CL</code> and <code class="highlighter-rouge">BLC2CBL</code> which were used to transfer from one shape of signal to another.</p>

<p>As part of this release, we’re also introducing support for complex numbers via tensors of dimension (…, 2), and providing <code class="highlighter-rouge">magphase</code> to convert such a tensor into its magnitude and phase, and similarly <code class="highlighter-rouge">complex_norm</code> and <code class="highlighter-rouge">angle</code>.</p>

<p>The details of the standardization are provided in the <a href="https://github.com/pytorch/audio/blob/v0.3.0/README.md#Conventions">README</a>.</p>

<h3 id="functionals-transformations-and-kaldi-compatibility">Functionals, Transformations, and Kaldi Compatibility</h3>

<p>Prior to the standardization, we separated state and computation into <code class="highlighter-rouge">torchaudio.transforms</code> and <code class="highlighter-rouge">torchaudio.functional</code>.</p>

<p>As part of the transforms, we’re adding a new transformation in 0.3.0: <code class="highlighter-rouge">Resample</code>. <code class="highlighter-rouge">Resample</code> can upsample or downsample a waveform to a different frequency.</p>

<p>As part of the functionals, we’re introducing: <code class="highlighter-rouge">phase_vocoder</code>, a phase vocoder to change the speed of a waveform without changing its pitch, and <code class="highlighter-rouge">ISTFT</code>, the inverse <code class="highlighter-rouge">STFT</code> implemented to be compatible with STFT provided by PyTorch. This separation allows us to make functionals weak scriptable and to utilize JIT in 0.3.0. We thus have JIT and CUDA support for the following transformations: <code class="highlighter-rouge">Spectrogram</code>, <code class="highlighter-rouge">AmplitudeToDB</code> (previously named <code class="highlighter-rouge">SpectrogramToDB</code>), <code class="highlighter-rouge">MelScale</code>,
<code class="highlighter-rouge">MelSpectrogram</code>, <code class="highlighter-rouge">MFCC</code>, <code class="highlighter-rouge">MuLawEncoding</code>, <code class="highlighter-rouge">MuLawDecoding</code> (previously named <code class="highlighter-rouge">MuLawExpanding</code>).</p>

<p>We now also provide a compatibility interface with Kaldi to ease onboarding and reduce a user’s code dependency on Kaldi. We now have an interface for <code class="highlighter-rouge">spectrogram</code>, <code class="highlighter-rouge">fbank</code>, and <code class="highlighter-rouge">resample_waveform</code>.</p>

<h3 id="new-tutorial">New Tutorial</h3>

<p>To showcase the new conventions and transformations, we have a <a href="https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html">new tutorial</a> demonstrating how to preprocess waveforms using torchaudio. This tutorial walks through an example of loading a waveform and applying some of the available transformations to it.</p>

<p>We are excited to see an active community around torchaudio and eager to further grow and support it. We encourage you to go ahead and experiment for yourself with this tutorial and the two datasets that are available: VCTK and YESNO! They have an interface to download the datasets and preprocess them in a convenient format. You can find the details in the release notes <a href="https://github.com/pytorch/audio/releases">here</a>.</p>

<h2 id="torchtext-04-with-supervised-learning-datasets">Torchtext 0.4 with supervised learning datasets</h2>

<p>A key focus area of torchtext is to provide the fundamental elements to help accelerate NLP research. This includes easy access to commonly used datasets and basic preprocessing pipelines for working on raw text based data. The torchtext 0.4.0 release includes several popular supervised learning baselines with “one-command” data loading. A <a href="https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html">tutorial</a> is included to show how to use the new datasets for text classification analysis. We also added and improved on a few functions such as <a href="https://pytorch.org/text/data.html?highlight=get_tokenizer#torchtext.data.get_tokenizer">get_tokenizer</a> and <a href="https://pytorch.org/text/vocab.html#build-vocab-from-iterator">build_vocab_from_iterator</a> to make it easier to implement future datasets. Additional examples can be found <a href="https://github.com/pytorch/text/tree/master/examples/text_classification">here</a>.</p>

<p>Text classification is an important task in Natural Language Processing with many applications, such as sentiment analysis. The new release includes several popular <a href="https://pytorch.org/text/datasets.html?highlight=textclassification#torchtext.datasets.TextClassificationDataset">text classification datasets</a> for supervised learning including:</p>

<ul>
  <li>AG_NEWS</li>
  <li>SogouNews</li>
  <li>DBpedia</li>
  <li>YelpReviewPolarity</li>
  <li>YelpReviewFull</li>
  <li>YahooAnswers</li>
  <li>AmazonReviewPolarity</li>
  <li>AmazonReviewFull</li>
</ul>

<p>Each dataset comes with two parts (train vs. test), and can be easily loaded with a single command. The datasets also support an ngrams feature to capture the partial information about the local word order. Take a look at the tutorial <a href="https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html">here</a> to learn more about how to use the new datasets for supervised problems such as text classification analysis.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchtext.datasets.text_classification</span> <span class="kn">import</span> <span class="n">DATASETS</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">DATASETS</span><span class="p">[</span><span class="s">'AG_NEWS'</span><span class="p">](</span><span class="n">ngrams</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>In addition to the domain library, PyTorch provides many tools to make data loading easy. Users now can load and preprocess the text classification datasets with some well supported tools, like <a href="https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html">torch.utils.data.DataLoader</a> and <a href="https://pytorch.org/docs/master/data.html#torch.utils.data.IterableDataset">torch.utils.data.IterableDataset</a>. Here are a few lines to wrap the data with DataLoader. More examples can be found <a href="https://github.com/pytorch/text/tree/master/examples/text_classification">here</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">generate_batch</span><span class="p">)</span>
</code></pre></div></div>

<p>Check out the release notes <a href="https://github.com/pytorch/text/releases">here</a> to learn more and try out the <a href="http://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html">tutorial here</a>.</p>

<h2 id="torchvision-04-with-support-for-video">Torchvision 0.4 with Support for Video</h2>

<p>Video is now a first-class citizen in torchvision, with support for data loading, datasets, pre-trained models, and transforms. The 0.4 release of torchvision includes:</p>

<ul>
  <li>Efficient IO primitives for reading/writing video files (including audio), with support for arbitrary encodings and formats.</li>
  <li>Standard video datasets, compatible with <code class="highlighter-rouge">torch.utils.data.Dataset</code> and <code class="highlighter-rouge">torch.utils.data.DataLoader</code>.</li>
  <li>Pre-trained models built on the Kinetics-400 dataset for action classification on videos (including the training scripts).</li>
  <li>Reference training scripts for training your own video models.</li>
</ul>

<p>We wanted working with video data in PyTorch to be as straightforward as possible, without compromising too much on performance.
As such, we avoid the steps that would require re-encoding the videos beforehand, as it would involve:</p>

<ul>
  <li>A preprocessing step which duplicates the dataset in order to re-encode it.</li>
  <li>An overhead in time and space because this re-encoding is time-consuming.</li>
  <li>Generally, an external script should be used to perform the re-encoding.</li>
</ul>

<p>Additionally, we provide APIs such as the utility class, <code class="highlighter-rouge">VideoClips</code>, that simplifies the task of enumerating all possible clips of fixed size in a list of video files by creating an index of all clips in a set of videos. It also allows you to specify a fixed frame-rate for the videos. An example of the API is provided below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision.datasets.video_utils</span> <span class="kn">import</span> <span class="n">VideoClips</span>

<span class="k">class</span> <span class="nc">MyVideoDataset</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">video_paths</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">video_clips</span> <span class="o">=</span> <span class="n">VideoClips</span><span class="p">(</span><span class="n">video_paths</span><span class="p">,</span>
                                      <span class="n">clip_length_in_frames</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
                                      <span class="n">frames_between_clips</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                      <span class="n">frame_rate</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">video</span><span class="p">,</span> <span class="n">audio</span><span class="p">,</span> <span class="n">info</span><span class="p">,</span> <span class="n">video_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">video_clips</span><span class="o">.</span><span class="n">get_clip</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">video</span><span class="p">,</span> <span class="n">audio</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">video_clips</span><span class="o">.</span><span class="n">num_clips</span><span class="p">()</span>
</code></pre></div></div>

<p>Most of the user-facing API is in Python, similar to PyTorch, which makes it easily extensible. Plus, the underlying implementation is fast — torchvision decodes as little as possible from the video on-the-fly in order to return a clip from the video.</p>

<p>Check out the torchvision 0.4 <a href="https://github.com/pytorch/vision/releases">release notes here</a> for more details.</p>

<p>We look forward to continuing our collaboration with the community and hearing your feedback as we further improve and expand the PyTorch deep learning platform.</p>

<p><em>We’d like to thank the entire PyTorch team and the community for all of the contributions to this work!</em></p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="https://goo.gl/forms/PP1AGvNHpSaJP8to1" target="_blank">Slack</a></li>
        </ul>
      </div>

      <div class="footer-links-col follow-us-col">
        <ul>
          <li class="list-title">Stay Connected</li>
          <li>
            
            <div id="mc_embed_signup">
  <form
    action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
    method="post"
    id="mc-embedded-subscribe-form"
    name="mc-embedded-subscribe-form"
    class="email-subscribe-form validate"
    target="_blank"
    novalidate>
    <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
      <div class="mc-field-group">
        <label for="mce-EMAIL" style="display:none;">Email Address</label>
        <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
      </div>

      <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
      </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

      <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

      <div class="clear">
        <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
      </div>
    </div>
  </form>
</div>

          </li>
        </ul>

        <div class="footer-social-icons">
          <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
          <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
        </div>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <li class="">
          <a href="/features">Features</a>
        </li>

        <li class="">
          <a href="/ecosystem">Ecosystem</a>
        </li>

        <li class="">
          <a href="/mobile">Mobile</a>
        </li>

        <li class="">
          <a href="/hub">PyTorch Hub</a>
        </li>

        <li class="active">
          <a href="/blog">Blog</a>
        </li>

        <li>
          <a href="https://pytorch.org/tutorials">Tutorials</a>
        </li>

        <li>
          <a href="/docs">Docs</a>
        </li>

        <li class="">
          <a href="/resources">Resources</a>
        </li>

        <li id="github-mobile-menu-link">
          <a href="https://github.com/pytorch/pytorch">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top === 0) {
        $(".header-holder").css({"backgroundColor": "transparent"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>



</body>

</html>

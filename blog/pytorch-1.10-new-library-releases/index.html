<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      New Library Releases in PyTorch 1.10, including TorchX, TorchAudio, TorchVision | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="Today, we are announcing a number of new features and improvements to PyTorch libraries, alongside the PyTorch 1.10 release. Some highlights include:

" />

  <meta property="og:image" content="https://pytorch.org/assets/images/social-share.jpg" />
  <meta name="twitter:image" content="https://pytorch.org/assets/images/social-share.jpg" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="New Library Releases in PyTorch 1.10, including TorchX, TorchAudio, TorchVision" />
<meta property="og:description" content="Today, we are announcing a number of new features and improvements to PyTorch libraries, alongside the PyTorch 1.10 release. Some highlights include:

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="New Library Releases in PyTorch 1.10, including TorchX, TorchAudio, TorchVision" />
<meta name="twitter:description" content="Today, we are announcing a number of new features and improvements to PyTorch libraries, alongside the PyTorch 1.10 release. Some highlights include:

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>
    <div class="hello-bar">
        <div class="container">
          Join us in Silicon Valley September 18-19 at the 2024 PyTorch Conference. <a target="_blank" href="https://events.linuxfoundation.org/pytorch-conference/">Learn more</a>.
        </div>
      </div>
      
    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Learn
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/get-started">
            <span class=dropdown-title>Get Started</span>
            <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/">
            <span class="dropdown-title">Tutorials</span>
            <p>Whats new in PyTorch tutorials</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
            <span class="dropdown-title">Learn the Basics</span>
            <p>Familiarize yourself with PyTorch concepts and modules</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
            <span class="dropdown-title">PyTorch Recipes</span>
            <p>Bite-size, ready-to-deploy PyTorch code examples</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
            <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
            <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/contributor-awards-2023">
            <span class="dropdown-title">Contributor Awards - 2023</span>
            <p>Award winners announced at this year's PyTorch Conference</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
            <p>Build innovative and privacy-aware AI experiences for edge devices</p>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
            <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://pytorch.org/docs">
            <span class="dropdown-title">PyTorch</span>
            <p>Explore the documentation for comprehensive guidance on how to use PyTorch.</p>
          </a>
          <a class="nav-dropdown-item" href="/pytorch-domains">
            <span class="dropdown-title">PyTorch Domains</span>
            <p> Read the PyTorch Domains documentation to learn more about domain-specific libraries.</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Blog & News
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/blog">
            <span class="dropdown-title">PyTorch Blog</span>
            <p>Catch up on the latest technical news and happenings</p>
          </a>
          <a class="nav-dropdown-item" href="/community-blog">
            <span class="dropdown-title">Community Blog</span>
            <p>Stories from the PyTorch ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/videos">
            <span class="dropdown-title">Videos</span>
            <p>Learn about the latest PyTorch tutorials, new, and more </p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class="dropdown-title">Community Stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          About
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/governing-board">
            <span class=dropdown-title>Governing Board</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <a href="/join" data-cta="join">
        Become a Member
      </a>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub">
        <div id="topnav-gh-icon"></div>
      </a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">October 21, 2021</p>
            <h1>
                <a class="blog-title">New Library Releases in PyTorch 1.10, including TorchX, TorchAudio, TorchVision</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Team PyTorch
                      
                    </p>
                    <p>Today, we are announcing a number of new features and improvements to PyTorch libraries, alongside the <a href="https://pytorch.org/blog/pytorch-1.10-released/">PyTorch 1.10 release</a>. Some highlights include:</p>

<p>Some highlights include:</p>

<ul>
  <li><strong>TorchX</strong> - a new SDK for quickly building and deploying ML applications from research &amp; development to production.</li>
  <li><strong>TorchAudio</strong> - Added text-to-speech pipeline, self-supervised model support, multi-channel support and MVDR beamforming module, RNN transducer (RNNT) loss function, and batch and filterbank support to <code class="language-plaintext highlighter-rouge">lfilter</code> function. See the TorchAudio release notes <a href="https://github.com/pytorch/audio/releases">here</a>.</li>
  <li><strong>TorchVision</strong> - Added new RegNet and EfficientNet models, FX based feature extraction added to utilities, two new Automatic Augmentation techniques: Rand Augment and Trivial Augment, and updated training recipes. See the TorchVision release notes <a href="https://github.com/pytorch/vision/releases">here</a>.</li>
</ul>

<h1 id="introducing-torchx">Introducing TorchX</h1>
<p>TorchX is a new SDK for quickly building and deploying ML applications from research &amp; development to production. It offers various builtin components that encode MLOps best practices and make advanced features like distributed training and hyperparameter optimization accessible to all.</p>

<p>Users can get started with TorchX 0.1 with no added setup cost since it supports popular ML schedulers and pipeline orchestrators that are already widely adopted and deployed in production. No two production environments are the same. To comply with various use cases, TorchX’s core APIs allow tons of customization at well-defined extension points so that even the most unique applications can be serviced without customizing the whole vertical stack.</p>

<p>Read the <a href="https://pytorch.org/torchx">documentation</a> for more details and try out this feature using this quickstart <a href="https://pytorch.org/torchx/latest/quickstart.html">tutorial</a>.</p>

<h1 id="torchaudio-010">TorchAudio 0.10</h1>

<h3 id="beta-text-to-speech-pipeline">[Beta] Text-to-speech pipeline</h3>
<p>TorchAudio now adds the Tacotron2 model and pretrained weights.  It is now possible to build a text-to-speech pipeline with existing vocoder implementations like WaveRNN and Griffin-Lim. Building a TTS pipeline requires matching data processing and pretrained weights, which are often non-trivial to users. So TorchAudio introduces a bundle API so that constructing pipelines for specific pretrained weights is easy. The following example illustrates this.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">torchaudio</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">bundle</span> <span class="o">=</span> <span class="n">torchaudio</span><span class="p">.</span><span class="n">pipelines</span><span class="p">.</span><span class="n">TACOTRON2_WAVERNN_CHAR_LJSPEECH</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Build text processor, Tacotron2 and vocoder (WaveRNN) model
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">processor</span> <span class="o">=</span> <span class="n">bundle</span><span class="p">.</span><span class="n">get_text_processor</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tacotron2</span> <span class="o">=</span> <span class="n">bundle</span><span class="p">.</span><span class="n">get_tacotron2</span><span class="p">()</span>
<span class="n">Downloading</span><span class="p">:</span>
<span class="mi">100</span><span class="o">%|</span><span class="err">███████████████████████████████</span><span class="o">|</span> <span class="mi">107</span><span class="n">M</span><span class="o">/</span><span class="mi">107</span><span class="n">M</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">01</span><span class="o">&lt;</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">87.9</span><span class="n">MB</span><span class="o">/</span><span class="n">s</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vocoder</span> <span class="o">=</span> <span class="n">bundle</span><span class="p">.</span><span class="n">get_vocoder</span><span class="p">()</span>
<span class="n">Downloading</span><span class="p">:</span>
<span class="mi">100</span><span class="o">%|</span><span class="err">███████████████████████████████</span><span class="o">|</span> <span class="mf">16.7</span><span class="n">M</span><span class="o">/</span><span class="mf">16.7</span><span class="n">M</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="o">&lt;</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">78.1</span><span class="n">MB</span><span class="o">/</span><span class="n">s</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">text</span> <span class="o">=</span> <span class="s">"Hello World!"</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Encode text
</span><span class="o">&gt;&gt;&gt;</span> <span class="nb">input</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Generate (mel-scale) spectrogram
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">specgram</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tacotron2</span><span class="p">.</span><span class="n">infer</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Convert spectrogram to waveform
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">waveforms</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="n">vocoder</span><span class="p">(</span><span class="n">specgram</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Save audio
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">torchaudio</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">'hello-world.wav'</span><span class="p">,</span> <span class="n">waveforms</span><span class="p">,</span> <span class="n">vocoder</span><span class="p">.</span><span class="n">sample_rate</span><span class="p">)</span>

</code></pre></div></div>

<p>For the details of this API please refer to <a href="https://pytorch.org/audio/0.10.0/pipelines#tacotron2-text-to-speech">the documentation</a>. You can also try this from <a href="https://pytorch.org/tutorials/intermediate/text_to_speech_with_torchaudio.html">the tutorial</a>.</p>

<h3 id="beta-self-supervised-model-support">(Beta) Self-Supervised Model Support</h3>
<p>TorchAudio added HuBERT model architecture and pre-trained weight support for wav2vec 2.0 and HuBERT. HuBERT and wav2vec 2.0 are novel ways for audio representation learning and they yield high accuracy when fine-tuned on downstream tasks. These models can serve as baseline in future research, therefore, TorchAudio is providing a simple way to run the model. Similar to the TTS pipeline, the pretrained weights and associated information, such as expected sample rates and output class labels (for fine-tuned weights) are put together as a bundle, so that they can be used to build pipelines. The following example illustrates this.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">torchaudio</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">bundle</span> <span class="o">=</span> <span class="n">torchaudio</span><span class="p">.</span><span class="n">pipelines</span><span class="p">.</span><span class="n">HUBERT_ASR_LARGE</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Build the model and load pretrained weight.
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">bundle</span><span class="p">.</span><span class="n">get_model</span><span class="p">()</span>
<span class="n">Downloading</span><span class="p">:</span>
<span class="mi">100</span><span class="o">%|</span><span class="err">███████████████████████████████</span><span class="o">|</span> <span class="mf">1.18</span><span class="n">G</span><span class="o">/</span><span class="mf">1.18</span><span class="n">G</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">17</span><span class="o">&lt;</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">73.8</span><span class="n">MB</span><span class="o">/</span><span class="n">s</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Check the corresponding labels of the output.
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">bundle</span><span class="p">.</span><span class="n">get_labels</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
<span class="p">(</span><span class="s">'&lt;s&gt;'</span><span class="p">,</span> <span class="s">'&lt;pad&gt;'</span><span class="p">,</span> <span class="s">'&lt;/s&gt;'</span><span class="p">,</span> <span class="s">'&lt;unk&gt;'</span><span class="p">,</span> <span class="s">'|'</span><span class="p">,</span> <span class="s">'E'</span><span class="p">,</span> <span class="s">'T'</span><span class="p">,</span> <span class="s">'A'</span><span class="p">,</span> <span class="s">'O'</span><span class="p">,</span> <span class="s">'N'</span><span class="p">,</span> <span class="s">'I'</span><span class="p">,</span> <span class="s">'H'</span><span class="p">,</span> <span class="s">'S'</span><span class="p">,</span> <span class="s">'R'</span><span class="p">,</span> <span class="s">'D'</span><span class="p">,</span> <span class="s">'L'</span><span class="p">,</span> <span class="s">'U'</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="s">'W'</span><span class="p">,</span> <span class="s">'C'</span><span class="p">,</span> <span class="s">'F'</span><span class="p">,</span> <span class="s">'G'</span><span class="p">,</span> <span class="s">'Y'</span><span class="p">,</span> <span class="s">'P'</span><span class="p">,</span> <span class="s">'B'</span><span class="p">,</span> <span class="s">'V'</span><span class="p">,</span> <span class="s">'K'</span><span class="p">,</span> <span class="s">"'"</span><span class="p">,</span> <span class="s">'X'</span><span class="p">,</span> <span class="s">'J'</span><span class="p">,</span> <span class="s">'Q'</span><span class="p">,</span> <span class="s">'Z'</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Infer the label probability distribution
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">waveform</span><span class="p">,</span> <span class="n">sample_rate</span> <span class="o">=</span> <span class="n">torchaudio</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">hello</span><span class="o">-</span><span class="n">world</span><span class="p">.</span><span class="n">wav</span><span class="s">')
&gt;&gt;&gt;
&gt;&gt;&gt; emissions, _ = model(waveform)
&gt;&gt;&gt;
&gt;&gt;&gt; # Pass emission to (hypothetical) decoder
&gt;&gt;&gt; transcripts = ctc_decode(emissions, labels)
&gt;&gt;&gt; print(transcripts[0])
HELLO WORLD

</span></code></pre></div></div>

<p>Please refer to the <a href="https://pytorch.org/audio/0.10.0/pipelines#wav2vec-2-0-hubert-representation-learning">documentation</a> for more details and try out this feature using this <a href="https://pytorch.org/tutorials/intermediate/speech_command_recognition_with_torchaudio_tutorial.html">tutorial</a>.</p>

<h3 id="beta-multi-channel-support-and-mvdr-beamforming">(Beta) Multi-channel support and MVDR beamforming</h3>
<p>Far-field speech recognition is a more challenging task compared to near-field recognition. Multi-channel methods such as beamforming help reduce the noises and enhance the target speech.</p>

<p>TorchAudio now adds support for differentiable Minimum Variance Distortionless Response (MVDR) beamforming on multi-channel audio using Time-Frequency masks. Researchers can easily assemble it with any multi-channel ASR pipeline. There are three solutions (ref_channel, stv_evd, stv_power) and it supports single-channel and multi-channel (perform average in the method) masks. It provides an online option that recursively updates the parameters for streaming audio. We also provide a tutorial on how to apply MVDR beamforming to the multi-channel audio in the example directory.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">torchaudio.transforms</span> <span class="kn">import</span> <span class="n">MVDR</span><span class="p">,</span> <span class="n">Spectrogram</span><span class="p">,</span> <span class="n">InverseSpectrogram</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Load the multi-channel noisy audio
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">waveform_mix</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">torchaudio</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'mix.wav'</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Initialize the stft and istft modules
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">stft</span> <span class="o">=</span> <span class="n">Spectrogram</span><span class="p">(</span><span class="n">n_fft</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">hop_length</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">return_complex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">istft</span> <span class="o">=</span> <span class="n">InverseSpectrogram</span><span class="p">(</span><span class="n">n_fft</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">hop_length</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Get the noisy spectrogram
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">specgram_mix</span> <span class="o">=</span> <span class="n">stft</span><span class="p">(</span><span class="n">waveform_mix</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Get the Time-Frequency mask via machine learning models
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">waveform</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Initialize the MVDR module 
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">mvdr</span> <span class="o">=</span> <span class="n">MVDR</span><span class="p">(</span><span class="n">ref_channel</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">solution</span><span class="o">=</span><span class="err">”</span><span class="n">ref_channel</span><span class="err">”</span><span class="p">,</span> <span class="n">multi_mask</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Apply MVDR beamforming
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">specgram_enhanced</span> <span class="o">=</span> <span class="n">mvdr</span><span class="p">(</span><span class="n">specgram_mix</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Get the enhanced waveform via iSTFT
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">waveform_enhanced</span> <span class="o">=</span> <span class="n">istft</span><span class="p">(</span><span class="n">specgram_enhanced</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">waveform</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>
<p>Please refer to the <a href="https://pytorch.org/audio/0.10.0/transforms.html#mvdr">documentation</a> for more details and try out this feature using the MVDR tutorial.</p>

<h3 id="beta-rnn-transducer-loss">(Beta) RNN Transducer Loss</h3>
<p>The RNN transducer (RNNT) loss is part of the RNN transducer pipeline, which is a popular architecture for speech recognition tasks. Recently it has gotten attention for being used in a streaming setting, and has also achieved state-of-the-art WER for the LibriSpeech benchmark.</p>

<p>TorchAudio’s loss function supports float16 and float32 logits, has autograd and torchscript support, and can be run on both CPU and GPU, which has a custom CUDA kernel implementation for improved performance. The implementation is consistent with the original loss function in <a href="https://arxiv.org/pdf/1211.3711.pdf">Sequence Transduction with Recurrent Neural Networks</a>, but relies on code from <a href="https://arxiv.org/pdf/2011.03072.pdf">Alignment Restricted Streaming Recurrent Neural Network Transducer</a>. Special thanks to Jay Mahadeokar and Ching-Feng Yeh for their code contributions and guidance.</p>

<p>Please refer to the <a href="https://pytorch.org/audio/0.10.0/transforms.html#rnntloss">documentation</a> for more details.</p>

<h3 id="beta-batch-support-and-filter-bank-support">(Beta) Batch support and filter bank support</h3>
<p><code class="language-plaintext highlighter-rouge">torchaudio.functional.lfilter</code> now supports batch processing and multiple filters.</p>

<h3 id="prototype-emformer-module">(Prototype) Emformer Module</h3>
<p>Automatic speech recognition (ASR) research and productization have increasingly focused on on-device applications. Towards supporting such efforts, TorchAudio now includes <a href="https://arxiv.org/abs/2010.10759">Emformer</a>, a memory-efficient transformer architecture that has achieved state-of-the-art results on LibriSpeech in low-latency streaming scenarios, as a prototype feature.</p>

<p>Please refer to the <a href="https://pytorch.org/audio/main/prototype.html#emformer">documentation</a> for more details.</p>

<h3 id="gpu-build">GPU Build</h3>
<p>GPU builds that support custom CUDA kernels in TorchAudio, like the one being used for RNN transducer loss, have been added. Following this change, TorchAudio’s binary distribution now includes CPU-only versions and CUDA-enabled versions. To use CUDA-enabled binaries, PyTorch also needs to be compatible with CUDA.</p>

<h1 id="torchvision-011">TorchVision 0.11</h1>

<h3 id="stable-new-models">(Stable) New Models</h3>
<p><a href="https://arxiv.org/abs/2003.13678">RegNet</a> and <a href="https://arxiv.org/abs/1905.11946">EfficientNet</a> are two popular architectures that can be scaled to different computational budgets. In this release we include 22 pre-trained weights for their classification variants. The models were trained on ImageNet and the accuracies of the pre-trained models obtained on ImageNet val can be found below (see <a href="https://github.com/pytorch/vision/pull/4403#issuecomment-930381524">#4403</a>, <a href="https://github.com/pytorch/vision/pull/4530#issuecomment-933213238">#4530</a> and <a href="https://github.com/pytorch/vision/pull/4293">#4293</a> for more details).</p>

<p>The models can be used as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">models</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>

<span class="n">regnet</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">regnet_y_400mf</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">regnet</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">regnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">efficientnet</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">efficientnet_b0</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">efficientnet</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">efficientnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>
<p>See the full list of new models on the <a href="https://pytorch.org/vision/master/models.html">torchvision.models</a> documentation page.</p>

<p>We would like to thank Ross Wightman and Luke Melas-Kyriazi for contributing the weights of the EfficientNet variants.</p>

<h3 id="beta-fx-based-feature-extraction">(Beta) FX-based Feature Extraction</h3>
<p>A new Feature Extraction method has been added to our utilities. It uses <a href="https://pytorch.org/docs/stable/fx.html">torch.fx</a> and enables us to retrieve the outputs of intermediate layers of a network which is useful for feature extraction and visualization.</p>

<p>Here is an example of how to use the new utility:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="n">resnet50</span>
<span class="kn">from</span> <span class="nn">torchvision.models.feature_extraction</span> <span class="kn">import</span> <span class="n">create_feature_extractor</span>


<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">()</span>

<span class="n">return_nodes</span> <span class="o">=</span> <span class="p">{</span>
<span class="s">"layer4.2.relu_2"</span><span class="p">:</span> <span class="s">"layer4"</span>
<span class="p">}</span>
<span class="n">model2</span> <span class="o">=</span> <span class="n">create_feature_extractor</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">return_nodes</span><span class="o">=</span><span class="n">return_nodes</span><span class="p">)</span>
<span class="n">intermediate_outputs</span> <span class="o">=</span> <span class="n">model2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">intermediate_outputs</span><span class="p">[</span><span class="s">'layer4'</span><span class="p">].</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<p>We would like to thank Alexander Soare for developing this utility.</p>

<h3 id="stable-new-data-augmentations">(Stable) New Data Augmentations</h3>
<p>Two new Automatic Augmentation techniques were added: <a href="https://arxiv.org/abs/1909.13719">RandAugment</a> and <a href="https://arxiv.org/abs/2103.10158">Trivial Augment</a>. They apply a series of transformations on the original data to enhance them and to boost the performance of the models. The new techniques build on top of the previously added <a href="https://github.com/pytorch/vision/pull/3123">AutoAugment</a>  and focus on simplifying the approach, reducing the search space for the optimal policy and improving the performance gain in terms of accuracy. These techniques enable users to reproduce recipes to achieve state-of-the-art performance on the offered models. Additionally, it enables users to apply these techniques in order to do transfer learning and achieve optimal accuracy on new datasets.</p>

<p>Both methods can be used as drop-in replacement of the AutoAugment technique as seen below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>

<span class="n">t</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">RandAugment</span><span class="p">()</span>
<span class="c1"># t = transforms.TrivialAugmentWide()
</span><span class="n">transformed</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
<span class="n">transforms</span><span class="p">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
<span class="n">transforms</span><span class="p">.</span><span class="n">RandAugment</span><span class="p">(),</span> <span class="c1"># transforms.TrivialAugmentWide()
</span><span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">()])</span>
</code></pre></div></div>
<p>Read the <a href="https://pytorch.org/vision/master/transforms.html#automatic-augmentation-transforms">automatic augmentation transforms</a> for more details.</p>

<p>We would like to thank Samuel G. Müller for contributing to Trivial Augment and for his help on refactoring the AA package.</p>

<h3 id="updated-training-recipes">Updated Training Recipes</h3>
<p>We have updated our training reference scripts to add support for Exponential Moving Average, Label Smoothing, Learning-Rate Warmup, <a href="https://arxiv.org/abs/1710.09412">Mixup</a>, <a href="https://arxiv.org/abs/1905.04899">Cutmix</a> and other <a href="https://github.com/pytorch/vision/issues/3911">SOTA primitives</a>. The above enabled us to improve the classification Acc@1 of some pre-trained models by over 4 points. A major update of the existing pre-trained weights is expected in the next release.</p>

<p>Thanks for reading. If you’re interested in these updates and want to join the PyTorch community, we encourage you to join <a href="https://discuss.pytorch.org/">the discussion</a> forums and <a href="https://github.com/pytorch/pytorch/issues">open GitHub issues</a>. To get the latest news from PyTorch, follow us on <a href="https://twitter.com/PyTorch">Twitter</a>, <a href="https://medium.com/pytorch">Medium</a>, <a href="https://www.youtube.com/pytorch">YouTube</a> and <a href="https://www.linkedin.com/company/pytorch">LinkedIn</a>.</p>

<p>Cheers!
Team PyTorch</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="lf-grid">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org" class="footer-logo">
          <img src="/assets/images/logo-icon.svg" alt="PyTorch logo" width="40">
        </a>
      </div>

      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook"><path fill="currentColor" d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z"/></svg>	
        </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X"><path fill="currentColor" d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66"/></svg>
        </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube"><path fill="currentColor" d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z"/></svg>	
        </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn"><rect width="512" height="512" rx="0" fill="currentColor"/><circle fill="#000" cx="142" cy="138" r="37"/><path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198"/><path fill="#000" d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
        </a></li>
        <li><a href="https://social.lfx.dev/@pytorch" rel="me" target="_blank" title="PyTorch on Mastodon">
          <svg fill="currentColor" aria-label="Mastodon" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" xml:space="preserve"><path d="M21.327 8.566c0-4.339-2.843-5.61-2.843-5.61-1.433-.658-3.894-.935-6.451-.956h-.063c-2.557.021-5.016.298-6.45.956 0 0-2.843 1.272-2.843 5.61 0 .993-.019 2.181.012 3.441.103 4.243.778 8.425 4.701 9.463 1.809.479 3.362.579 4.612.51 2.268-.126 3.541-.809 3.541-.809l-.075-1.646s-1.621.511-3.441.449c-1.804-.062-3.707-.194-3.999-2.409a4.523 4.523 0 0 1-.04-.621s1.77.433 4.014.536c1.372.063 2.658-.08 3.965-.236 2.506-.299 4.688-1.843 4.962-3.254.434-2.223.398-5.424.398-5.424zm-3.353 5.59h-2.081V9.057c0-1.075-.452-1.62-1.357-1.62-1 0-1.501.647-1.501 1.927v2.791h-2.069V9.364c0-1.28-.501-1.927-1.502-1.927-.905 0-1.357.546-1.357 1.62v5.099H6.026V8.903c0-1.074.273-1.927.823-2.558.566-.631 1.307-.955 2.228-.955 1.065 0 1.872.409 2.405 1.228l.518.869.519-.869c.533-.819 1.34-1.228 2.405-1.228.92 0 1.662.324 2.228.955.549.631.822 1.484.822 2.558v5.253z"/></svg>
        </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/legal/policies/">Linux Foundation Policies</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">LF Projects, LLC Policies</a>. <a href="https://www.linuxfoundation.org/privacy">Privacy Policy</a> and <a href="https://www.linuxfoundation.org/terms">Terms of Use</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Learn</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/get-started">Get Started</a> 
          </li>
          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/#community-module">Community</a>
          </li>
          <li>
            <a href="https://discuss.pytorch.org">Forums</a>
          </li>
          <li>
            <a href="/resources">Developer Resources</a>
          </li>
          <li>
            <a href="/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Docs</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/docs">PyTorch</a>
          </li>
          <li>
            <a href="/pytorch-domains">PyTorch Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/blog">PyTorch Blog</a>
          </li>
          <li>
            <a href="/community-blog">Community Blog</a>
          </li>
          <li>
            <a href="/videos">Videos</a>
          </li>
          <li>
            <a href="/community-stories">Community Stories</a>
          </li>
          <li>
            <a href="/events">Events</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="/governing-board">Governing Board</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a href="/join">Become a Member</a>
        </li>
        <li class="resources-mobile-menu-title">
          <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub"><div id="topnav-gh-icon"></div></a>
        </li>

      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header, .announcement-header, .comm-stories-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

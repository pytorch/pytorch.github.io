<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      How Computational Graphs are Constructed in PyTorch | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="In the previous post we went over the theoretical foundations of automatic differentiation and reviewed the implementation in PyTorch. In this post, we will be showing the parts of PyTorch involved in creating the graph and executing it. In order to understand the following contents, please read @ezyang’s wonderful blog post about PyTorch internals.

" />

  <meta property="og:image" content="https://pytorch.org/assets/images/augmented_computational_graph.png" />
  <meta name="twitter:image" content="https://pytorch.org/assets/images/augmented_computational_graph.png" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="How Computational Graphs are Constructed in PyTorch" />
<meta property="og:description" content="In the previous post we went over the theoretical foundations of automatic differentiation and reviewed the implementation in PyTorch. In this post, we will be showing the parts of PyTorch involved in creating the graph and executing it. In order to understand the following contents, please read @ezyang’s wonderful blog post about PyTorch internals.

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="How Computational Graphs are Constructed in PyTorch" />
<meta name="twitter:description" content="In the previous post we went over the theoretical foundations of automatic differentiation and reviewed the implementation in PyTorch. In this post, we will be showing the parts of PyTorch involved in creating the graph and executing it. In order to understand the following contents, please read @ezyang’s wonderful blog post about PyTorch internals.

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>
    <div class="hello-bar">
        <div class="container">
          Join us in Silicon Valley September 18-19 at the 2024 PyTorch Conference. <a target="_blank" href="https://events.linuxfoundation.org/pytorch-conference/">Learn more</a>.
        </div>
      </div>
      
    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Learn
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/get-started">
            <span class=dropdown-title>Get Started</span>
            <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/">
            <span class="dropdown-title">Tutorials</span>
            <p>Whats new in PyTorch tutorials</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
            <span class="dropdown-title">Learn the Basics</span>
            <p>Familiarize yourself with PyTorch concepts and modules</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
            <span class="dropdown-title">PyTorch Recipes</span>
            <p>Bite-size, ready-to-deploy PyTorch code examples</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
            <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
            <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/contributor-awards-2023">
            <span class="dropdown-title">Contributor Awards - 2023</span>
            <p>Award winners announced at this year's PyTorch Conference</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
            <p>Build innovative and privacy-aware AI experiences for edge devices</p>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
            <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://pytorch.org/docs">
            <span class="dropdown-title">PyTorch</span>
            <p>Explore the documentation for comprehensive guidance on how to use PyTorch.</p>
          </a>
          <a class="nav-dropdown-item" href="/pytorch-domains">
            <span class="dropdown-title">PyTorch Domains</span>
            <p> Read the PyTorch Domains documentation to learn more about domain-specific libraries.</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Blog & News
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/blog">
            <span class="dropdown-title">PyTorch Blog</span>
            <p>Catch up on the latest technical news and happenings</p>
          </a>
          <a class="nav-dropdown-item" href="/community-blog">
            <span class="dropdown-title">Community Blog</span>
            <p>Stories from the PyTorch ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/videos">
            <span class="dropdown-title">Videos</span>
            <p>Learn about the latest PyTorch tutorials, new, and more </p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class="dropdown-title">Community Stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          About
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/governing-board">
            <span class=dropdown-title>Governing Board</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <a href="/join" data-cta="join">
        Become a Member
      </a>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub">
        <div id="topnav-gh-icon"></div>
      </a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">August 31, 2021</p>
            <h1>
                <a class="blog-title">How Computational Graphs are Constructed in PyTorch</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Preferred Networks
                      
                    </p>
                    <p>In the previous <a href="https://pytorch.org/blog/overview-of-pytorch-autograd-engine/">post</a> we went over the theoretical foundations of automatic differentiation and reviewed the implementation in PyTorch. In this post, we will be showing the parts of PyTorch involved in creating the graph and executing it. In order to understand the following contents, please read @ezyang’s wonderful <a href="http://blog.ezyang.com/2019/05/pytorch-internals/">blog post</a> about PyTorch internals.</p>

<h1 id="autograd-components">Autograd components</h1>

<p>First of all, let’s look at where the different components of autograd live:</p>

<p><a href="https://github.com/pytorch/pytorch/tree/release/1.9/tools/autograd">tools/autograd</a>: Here we can find the definition of the derivatives as we saw in the previous post <a href="https://github.com/pytorch/pytorch/blob/release/1.9/tools/autograd/derivatives.yaml">derivatives.yaml</a>, several python scripts and a folder called <a href="https://github.com/pytorch/pytorch/tree/release/1.9/tools/autograd/templates">templates</a>. These scripts and the templates are used at building time to generate the C++ code for the derivatives as specified in the yaml file. Also, the scripts here generate wrappers for the regular ATen functions so that the computational graph can be constructed.</p>

<p><a href="https://github.com/pytorch/pytorch/tree/release/1.9/torch/autograd">torch/autograd</a>: This folder is where the autograd components that can be used directly from python are located. In <a href="https://github.com/pytorch/pytorch/blob/release/1.9/torch/autograd/function.py">function.py</a> we find the actual definition of  <code class="language-plaintext highlighter-rouge">torch.autograd.Function</code>, a class used by users to write their own differentiable functions in python as per the documentation. <a href="https://github.com/pytorch/pytorch/blob/release/1.9/torch/autograd/functional.py">functional.py</a> holds components for functionally computing the jacobian vector product, hessian, and other gradient related computations of a given function.
The rest of the files have additional components such as gradient checkers, anomaly detection, and the autograd profiler.</p>

<p><a href="https://github.com/pytorch/pytorch/tree/release/1.9/torch/csrc/autograd">torch/csrc/autograd</a>: This is where the graph creation and execution-related code lives.
All this code is written in C++, since it is a critical part that is required to be extremely performant. Here we have several files that implement the engine, metadata storage, and all the needed components. Alongside this, we have several files whose names start with <code class="language-plaintext highlighter-rouge">python_</code>, and their main responsibility is to allow python objects to be used in the autograd engine.</p>

<h1 id="graph-creation">Graph Creation</h1>

<p><a href="https://pytorch.org/blog/overview-of-pytorch-autograd-engine/">Previously</a>, we described the creation of a computational graph. Now, we will see how PyTorch creates these graphs with references to the actual codebase.</p>

<p align="center">
<img src="https://pytorch.org/assets/images/augmented_computational_graph.png" width="100%" />
<br />
Figure 1: Example of an augmented computational graph
</p>

<p>It all starts when in our python code, where we request a tensor to require the gradient.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>When the <code class="language-plaintext highlighter-rouge">required_grad</code> flag is set in tensor creation, c10 will <a href="https://github.com/pytorch/pytorch/blob/e7cd59c7a061c78d8d0265e4308b5933e44f9176/c10/core/TensorImpl.cpp#L382-L406">allocate</a> an <code class="language-plaintext highlighter-rouge">AutogradMeta</code> object that is used to hold the graph information.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kt">void</span> <span class="n">TensorImpl</span><span class="o">::</span><span class="n">set_requires_grad</span><span class="p">(</span><span class="kt">bool</span> <span class="n">requires_grad</span><span class="p">)</span> <span class="p">{</span>
  <span class="p">...</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">autograd_meta_</span><span class="p">)</span>
    <span class="n">autograd_meta_</span> <span class="o">=</span> <span class="n">impl</span><span class="o">::</span><span class="n">GetAutogradMetaFactory</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">make</span><span class="p">();</span>
    <span class="n">autograd_meta_</span><span class="o">-&gt;</span><span class="n">set_requires_grad</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">,</span> <span class="k">this</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">AutogradMeta</code> object is defined in <a href="https://github.com/pytorch/pytorch/blob/release/1.9/torch/csrc/autograd/variable.h#L190-L286">torch/csrc/autograd/variable.h</a> as follows:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">struct</span> <span class="nc">TORCH_API</span> <span class="n">AutogradMeta</span> <span class="o">:</span> <span class="k">public</span> <span class="n">c10</span><span class="o">::</span><span class="n">AutogradMetaInterface</span> <span class="p">{</span>
  <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">name_</span><span class="p">;</span>

  <span class="n">Variable</span> <span class="n">grad_</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">grad_fn_</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">weak_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">grad_accumulator_</span><span class="p">;</span>
  <span class="c1">// other fields and methods</span>
  <span class="p">...</span>
<span class="p">};</span>
</code></pre></div></div>

<p>The most important fields in this structure are the computed gradient in <code class="language-plaintext highlighter-rouge">grad_</code> and a pointer to the function <code class="language-plaintext highlighter-rouge">grad_fn</code> that will be called by the engine to produce the actual gradient. Also, there is a gradient accumulator object that is used to add together all the different gradients where this tensor is involved as we will see in the graph execution.</p>

<h3 id="graphs-nodes-and-edges">Graphs, Nodes and Edges.</h3>

<p>Now, when we call a differentiable function that takes this tensor as an argument, the associated metadata will be populated. Let’s suppose that we call a regular torch function that is implemented in ATen. Let it be the multiplication as in our previous blog post example. The resulting tensor has a field called <code class="language-plaintext highlighter-rouge">grad_fn</code> that is essentially a pointer to the function that will be used to compute the gradient of that operation.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">v</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">v</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">0.3750</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MulBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</code></pre></div></div>

<p>Here we see that the tensors’ <code class="language-plaintext highlighter-rouge">grad_fn</code> has a <code class="language-plaintext highlighter-rouge">MulBackward0</code> value. This function is the same that was written in the <a href="https://github.com/pytorch/pytorch/blob/e7cd59c7a061c78d8d0265e4308b5933e44f9176/tools/autograd/derivatives.yaml#L840-L843">derivatives.yaml</a> file, and its C++ code was generated automatically by all the scripts in <code class="language-plaintext highlighter-rouge">tools/autograd</code>. It’s auto-generated source code can be seen in <code class="language-plaintext highlighter-rouge">torch/csrc/autograd/generated/Functions.cpp</code>.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">variable_list</span> <span class="n">MulBackward0</span><span class="o">::</span><span class="n">apply</span><span class="p">(</span><span class="n">variable_list</span><span class="o">&amp;&amp;</span> <span class="n">grads</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">std</span><span class="o">::</span><span class="n">lock_guard</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">mutex_</span><span class="p">);</span>

  <span class="n">IndexRangeGenerator</span> <span class="n">gen</span><span class="p">;</span>
  <span class="k">auto</span> <span class="n">self_ix</span> <span class="o">=</span> <span class="n">gen</span><span class="p">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
  <span class="k">auto</span> <span class="n">other_ix</span> <span class="o">=</span> <span class="n">gen</span><span class="p">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
  <span class="n">variable_list</span> <span class="n">grad_inputs</span><span class="p">(</span><span class="n">gen</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
  <span class="k">auto</span> <span class="n">self</span> <span class="o">=</span> <span class="n">self_</span><span class="p">.</span><span class="n">unpack</span><span class="p">();</span>
  <span class="k">auto</span> <span class="n">other</span> <span class="o">=</span> <span class="n">other_</span><span class="p">.</span><span class="n">unpack</span><span class="p">();</span>
  <span class="kt">bool</span> <span class="n">any_grad_defined</span> <span class="o">=</span> <span class="n">any_variable_defined</span><span class="p">(</span><span class="n">grads</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">should_compute_output</span><span class="p">({</span> <span class="n">other_ix</span> <span class="p">}))</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">grad_result</span> <span class="o">=</span> <span class="n">any_grad_defined</span> <span class="o">?</span> <span class="p">(</span><span class="n">mul_tensor_backward</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">self</span><span class="p">,</span> <span class="n">other_scalar_type</span><span class="p">))</span> <span class="o">:</span> <span class="n">Tensor</span><span class="p">();</span>
    <span class="n">copy_range</span><span class="p">(</span><span class="n">grad_inputs</span><span class="p">,</span> <span class="n">other_ix</span><span class="p">,</span> <span class="n">grad_result</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">should_compute_output</span><span class="p">({</span> <span class="n">self_ix</span> <span class="p">}))</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">grad_result</span> <span class="o">=</span> <span class="n">any_grad_defined</span> <span class="o">?</span> <span class="p">(</span><span class="n">mul_tensor_backward</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">self_scalar_type</span><span class="p">))</span> <span class="o">:</span> <span class="n">Tensor</span><span class="p">();</span>
    <span class="n">copy_range</span><span class="p">(</span><span class="n">grad_inputs</span><span class="p">,</span> <span class="n">self_ix</span><span class="p">,</span> <span class="n">grad_result</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">grad_inputs</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">grad_fn</code> objects inherit from the <a href="https://github.com/pytorch/pytorch/blob/e7cd59c7a061c78d8d0265e4308b5933e44f9176/torch/csrc/autograd/function.h#L535-L541"><code class="language-plaintext highlighter-rouge">TraceableFunction</code></a> class, a descendant of  <code class="language-plaintext highlighter-rouge">Node</code> with just a property set to enable tracing for debugging and optimization purposes. A graph by definition has nodes and edges, so these functions are indeed the nodes of the computational graph that are linked together by using <code class="language-plaintext highlighter-rouge">Edge</code> objects to enable the graph traversal later on.</p>

<p>The <code class="language-plaintext highlighter-rouge">Node</code> definition can be found in the <a href="https://github.com/pytorch/pytorch/blob/e7cd59c7a061c78d8d0265e4308b5933e44f9176/torch/csrc/autograd/function.h#L50-L533">torch/csrc/autograd/function.h</a> file.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">struct</span> <span class="nc">TORCH_API</span> <span class="n">Node</span> <span class="o">:</span> <span class="n">std</span><span class="o">::</span><span class="n">enable_shared_from_this</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="p">{</span>
 <span class="p">...</span>
 <span class="c1">/// Evaluates the function on the given inputs and returns the result of the</span>
  <span class="c1">/// function call.</span>
  <span class="n">variable_list</span> <span class="k">operator</span><span class="p">()(</span><span class="n">variable_list</span><span class="o">&amp;&amp;</span> <span class="n">inputs</span><span class="p">)</span> <span class="p">{</span>
  <span class="p">...</span>
  <span class="p">}</span>

<span class="nl">protected:</span>
  <span class="c1">/// Performs the `Node`'s actual operation.</span>
  <span class="k">virtual</span> <span class="n">variable_list</span> <span class="n">apply</span><span class="p">(</span><span class="n">variable_list</span><span class="o">&amp;&amp;</span> <span class="n">inputs</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="err">…</span>
  <span class="n">edge_list</span> <span class="n">next_edges_</span><span class="p">;</span>
</code></pre></div></div>

<p>Essentially we see that it has an override of the <code class="language-plaintext highlighter-rouge">operator ()</code> that performs the call to the actual function, and a pure virtual function called <code class="language-plaintext highlighter-rouge">apply</code>. The automatically generated functions override this <code class="language-plaintext highlighter-rouge">apply</code> method as we saw in the <code class="language-plaintext highlighter-rouge">MulBackward0</code> example above. Finally, the node also has a list of edges to enable graph connectivity.</p>

<p>The <a href="https://github.com/pytorch/pytorch/blob/e7cd59c7a061c78d8d0265e4308b5933e44f9176/torch/csrc/autograd/edge.h#L14-L39">Edge</a> object is used to link <code class="language-plaintext highlighter-rouge">Node</code>s together and its implementation is straightforward.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">struct</span> <span class="nc">Edge</span> <span class="p">{</span>
  <span class="p">...</span>
  <span class="c1">/// The function this `Edge` points to.</span>
  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">function</span><span class="p">;</span>
  <span class="c1">/// The identifier of a particular input to the function.</span>
  <span class="kt">uint32_t</span> <span class="n">input_nr</span><span class="p">;</span>
<span class="p">};</span>
</code></pre></div></div>

<p>It only requires a function pointer (the actual <code class="language-plaintext highlighter-rouge">grad_fn</code> objects that the edges link together), and an input number that acts as an id for the edge.</p>

<h3 id="linking-nodes-together">Linking nodes together</h3>

<p>When we invoke the product operation of two tensors, we enter into the realm of autogenerated code. All the scripts that we saw in <code class="language-plaintext highlighter-rouge">tools/autograd</code> fill a series of templates that wrap the differentiable functions in ATen. These functions have code to construct the backward graph during the forward pass.</p>

<p>The <a href="https://github.com/pytorch/pytorch/blob/release/1.9/tools/autograd/gen_variable_type.py">gen_variable_type.py</a> script is in charge of writing all this wrapping code. This script is called from the <a href="https://github.com/pytorch/pytorch/blob/release/1.9/tools/autograd/gen_autograd.py">tools/autograd/gen_autograd.py</a> during the pytorch build process and it will output the automatically generated function wrappers to <code class="language-plaintext highlighter-rouge">torch/csrc/autograd/generated/</code>.</p>

<p>Let’s take a look at how the tensor multiplication generated function looks like. The code has been simplified, but it can be found in the <code class="language-plaintext highlighter-rouge">torch/csrc/autograd/generated/VariableType_4.cpp</code> file when compiling pytorch from source.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="nf">mul_Tensor</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">DispatchKeySet</span> <span class="n">ks</span><span class="p">,</span> <span class="k">const</span> <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">&amp;</span> <span class="n">self</span><span class="p">,</span> <span class="k">const</span> <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">&amp;</span> <span class="n">other</span><span class="p">)</span> <span class="p">{</span>
  <span class="p">...</span>
  <span class="k">auto</span> <span class="n">_any_requires_grad</span> <span class="o">=</span> <span class="n">compute_requires_grad</span><span class="p">(</span> <span class="n">self</span><span class="p">,</span> <span class="n">other</span> <span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">MulBackward0</span><span class="o">&gt;</span> <span class="n">grad_fn</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">_any_requires_grad</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Creates the link to the actual grad_fn and links the graph for backward traversal</span>
    <span class="n">grad_fn</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">MulBackward0</span><span class="o">&gt;</span><span class="p">(</span><span class="k">new</span> <span class="n">MulBackward0</span><span class="p">(),</span> <span class="n">deleteNode</span><span class="p">);</span>
    <span class="n">grad_fn</span><span class="o">-&gt;</span><span class="n">set_next_edges</span><span class="p">(</span><span class="n">collect_next_edges</span><span class="p">(</span> <span class="n">self</span><span class="p">,</span> <span class="n">other</span> <span class="p">));</span>
    <span class="p">...</span>
  <span class="p">}</span>
  <span class="err">…</span>
  <span class="c1">// Does the actual function call to ATen</span>
  <span class="k">auto</span> <span class="n">_tmp</span> <span class="o">=</span> <span class="p">([</span><span class="o">&amp;</span><span class="p">]()</span> <span class="p">{</span>
    <span class="n">at</span><span class="o">::</span><span class="n">AutoDispatchBelowADInplaceOrView</span> <span class="n">guard</span><span class="p">;</span>
    <span class="k">return</span> <span class="n">at</span><span class="o">::</span><span class="n">redispatch</span><span class="o">::</span><span class="n">mul</span><span class="p">(</span><span class="n">ks</span> <span class="o">&amp;</span> <span class="n">c10</span><span class="o">::</span><span class="n">after_autograd_keyset</span><span class="p">,</span> <span class="n">self_</span><span class="p">,</span> <span class="n">other_</span><span class="p">);</span>
  <span class="p">})();</span>

  <span class="k">auto</span> <span class="n">result</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">_tmp</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">grad_fn</span><span class="p">)</span> <span class="p">{</span>
       <span class="c1">// Connects the result to the graph</span>
      <span class="n">set_history</span><span class="p">(</span><span class="n">flatten_tensor_args</span><span class="p">(</span> <span class="n">result</span> <span class="p">),</span> <span class="n">grad_fn</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="p">...</span>
  <span class="k">return</span> <span class="n">result</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Let’s walk through the most important lines of this code.
First of all, the <code class="language-plaintext highlighter-rouge">grad_fn</code> object is created with: ` grad_fn = std::shared_ptr<MulBackward0>(new MulBackward0(), deleteNode);`.</MulBackward0></p>

<p>After the <code class="language-plaintext highlighter-rouge">grad_fn</code> object is created, the edges used to link the nodes together are created by using the <code class="language-plaintext highlighter-rouge">grad_fn-&gt;set_next_edges(collect_next_edges( self, other ));</code> calls.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">struct</span> <span class="nc">MakeNextFunctionList</span> <span class="o">:</span> <span class="n">IterArgs</span><span class="o">&lt;</span><span class="n">MakeNextFunctionList</span><span class="o">&gt;</span> <span class="p">{</span>
  <span class="n">edge_list</span> <span class="n">next_edges</span><span class="p">;</span>
  <span class="k">using</span> <span class="n">IterArgs</span><span class="o">&lt;</span><span class="n">MakeNextFunctionList</span><span class="o">&gt;::</span><span class="k">operator</span><span class="p">();</span>
  <span class="kt">void</span> <span class="k">operator</span><span class="p">()(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span> <span class="n">variable</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">variable</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span> <span class="p">{</span>
      <span class="n">next_edges</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">impl</span><span class="o">::</span><span class="n">gradient_edge</span><span class="p">(</span><span class="n">variable</span><span class="p">));</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">next_edges</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">();</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="kt">void</span> <span class="k">operator</span><span class="p">()(</span><span class="k">const</span> <span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">Variable</span><span class="o">&gt;&amp;</span> <span class="n">variable</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">variable</span><span class="p">.</span><span class="n">has_value</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="n">variable</span><span class="o">-&gt;</span><span class="n">defined</span><span class="p">())</span> <span class="p">{</span>
      <span class="n">next_edges</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">impl</span><span class="o">::</span><span class="n">gradient_edge</span><span class="p">(</span><span class="o">*</span><span class="n">variable</span><span class="p">));</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">next_edges</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">();</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">};</span>

<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span><span class="o">...</span> <span class="nc">Variables</span><span class="p">&gt;</span>
<span class="n">edge_list</span> <span class="nf">collect_next_edges</span><span class="p">(</span><span class="n">Variables</span><span class="o">&amp;&amp;</span><span class="p">...</span> <span class="n">variables</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">detail</span><span class="o">::</span><span class="n">MakeNextFunctionList</span> <span class="n">make</span><span class="p">;</span>
  <span class="n">make</span><span class="p">.</span><span class="n">apply</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">forward</span><span class="o">&lt;</span><span class="n">Variables</span><span class="o">&gt;</span><span class="p">(</span><span class="n">variables</span><span class="p">)...);</span>
  <span class="k">return</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">make</span><span class="p">.</span><span class="n">next_edges</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Given an input variable (it’s just a regular tensor), <a href="https://github.com/pytorch/pytorch/blob/e7cd59c7a061c78d8d0265e4308b5933e44f9176/torch/csrc/autograd/function.h#L597-L603"><code class="language-plaintext highlighter-rouge">collect_next_edges</code></a>
 will create an <code class="language-plaintext highlighter-rouge">Edge</code> object by calling <a href="https://github.com/pytorch/pytorch/blob/e7cd59c7a061c78d8d0265e4308b5933e44f9176/torch/csrc/autograd/variable.cpp#L228-L240."><code class="language-plaintext highlighter-rouge">impl::gradient_edge</code></a></p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">Edge</span> <span class="nf">gradient_edge</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span> <span class="n">self</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// If grad_fn is null (as is the case for a leaf node), we instead</span>
    <span class="c1">// interpret the gradient function to be a gradient accumulator, which will</span>
    <span class="c1">// accumulate its inputs into the grad property of the variable. These</span>
    <span class="c1">// nodes get suppressed in some situations, see "suppress gradient</span>
    <span class="c1">// accumulation" below. Note that only variables which have `requires_grad =</span>
    <span class="c1">// True` can have gradient accumulators.</span>
    <span class="k">if</span> <span class="p">(</span><span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="n">gradient</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">())</span> <span class="p">{</span>
      <span class="k">return</span> <span class="n">Edge</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">output_nr</span><span class="p">());</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="k">return</span> <span class="n">Edge</span><span class="p">(</span><span class="n">grad_accumulator</span><span class="p">(</span><span class="n">self</span><span class="p">),</span> <span class="mi">0</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
</code></pre></div></div>

<p>To understand how edges work, let’s assume that an early executed function produced two output tensors, both with their <code class="language-plaintext highlighter-rouge">grad_fn</code> set, each tensor also has an <code class="language-plaintext highlighter-rouge">output_nr</code> property with the order in which they were returned. When creating the edges for the current <code class="language-plaintext highlighter-rouge">grad_fn</code>, an <code class="language-plaintext highlighter-rouge">Edge</code> object per input variable will be created. The edges will point to the variable’s grad_fn and will also track the <code class="language-plaintext highlighter-rouge">output_nr</code> to establish ids used when traversing the graph. In the case that the input variables are “leaf”, i.e. they were not produced by any differentiable function, they don’t have a <code class="language-plaintext highlighter-rouge">grad_fn</code> attribute set. A special function called a gradient accumulator is set by default as seen in the above code snippet.</p>

<p>After the edges are created, the <code class="language-plaintext highlighter-rouge">grad_fn</code> graph Node object that is being currently created will hold them using the <a href="https://github.com/pytorch/pytorch/blob/e7cd59c7a061c78d8d0265e4308b5933e44f9176/torch/csrc/autograd/function.h#L258-L263"><code class="language-plaintext highlighter-rouge">set_next_edges</code></a> function. This is what connects <code class="language-plaintext highlighter-rouge">grad_fn</code>s together, producing the computational graph.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="kt">void</span> <span class="nf">set_next_edges</span><span class="p">(</span><span class="n">edge_list</span><span class="o">&amp;&amp;</span> <span class="n">next_edges</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">next_edges_</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">next_edges</span><span class="p">);</span>
    <span class="k">for</span><span class="p">(</span><span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="n">next_edge</span> <span class="o">:</span> <span class="n">next_edges_</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">update_topological_nr</span><span class="p">(</span><span class="n">next_edge</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
</code></pre></div></div>

<p>Now, the forward pass of the function will execute, and after the execution  <code class="language-plaintext highlighter-rouge">set_history</code> will connect the output tensors to the <code class="language-plaintext highlighter-rouge">grad_fn</code> Node.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kr">inline</span> <span class="kt">void</span> <span class="nf">set_history</span><span class="p">(</span>
    <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">variable</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;&amp;</span> <span class="n">grad_fn</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">AT_ASSERT</span><span class="p">(</span><span class="n">grad_fn</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">variable</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span> <span class="p">{</span>
    <span class="c1">// If the codegen triggers this, you most likely want to add your newly added function</span>
    <span class="c1">// to the DONT_REQUIRE_DERIVATIVE list in tools/autograd/gen_variable_type.py</span>
    <span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">isDifferentiableType</span><span class="p">(</span><span class="n">variable</span><span class="p">.</span><span class="n">scalar_type</span><span class="p">()));</span>
    <span class="k">auto</span> <span class="n">output_nr</span> <span class="o">=</span>
        <span class="n">grad_fn</span><span class="o">-&gt;</span><span class="n">add_input_metadata</span><span class="p">(</span><span class="n">variable</span><span class="p">);</span>
    <span class="n">impl</span><span class="o">::</span><span class="n">set_gradient_edge</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="p">{</span><span class="n">grad_fn</span><span class="p">,</span> <span class="n">output_nr</span><span class="p">});</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="n">grad_fn</span><span class="o">-&gt;</span><span class="n">add_input_metadata</span><span class="p">(</span><span class="n">Node</span><span class="o">::</span><span class="n">undefined_input</span><span class="p">());</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://github.com/pytorch/pytorch/blob/e7cd59c7a061c78d8d0265e4308b5933e44f9176/torch/csrc/autograd/functions/utils.h#L58-L72"><code class="language-plaintext highlighter-rouge">set_history</code></a> calls <a href="https://github.com/pytorch/pytorch/blob/e7cd59c7a061c78d8d0265e4308b5933e44f9176/torch/csrc/autograd/variable.cpp#L242-L255"><code class="language-plaintext highlighter-rouge">set_gradient_edge</code></a>, which just copies the grad_fn and the <code class="language-plaintext highlighter-rouge">output_nr</code> to the <code class="language-plaintext highlighter-rouge">AutogradMeta</code> object that the tensor has.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="kt">void</span> <span class="nf">set_gradient_edge</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span> <span class="n">self</span><span class="p">,</span> <span class="n">Edge</span> <span class="n">edge</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span><span class="o">*</span> <span class="n">meta</span> <span class="o">=</span> <span class="n">materialize_autograd_meta</span><span class="p">(</span><span class="n">self</span><span class="p">);</span>
    <span class="n">meta</span><span class="o">-&gt;</span><span class="n">grad_fn_</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">edge</span><span class="p">.</span><span class="n">function</span><span class="p">);</span>
    <span class="n">meta</span><span class="o">-&gt;</span><span class="n">output_nr_</span> <span class="o">=</span> <span class="n">edge</span><span class="p">.</span><span class="n">input_nr</span><span class="p">;</span>
    <span class="c1">// For views, make sure this new grad_fn_ is not overwritten unless it is necessary</span>
    <span class="c1">// in the VariableHooks::grad_fn below.</span>
    <span class="c1">// This logic is only relevant for custom autograd Functions for which multiple</span>
    <span class="c1">// operations can happen on a given Tensor before its gradient edge is set when</span>
    <span class="c1">// exiting the custom Function.</span>
    <span class="k">auto</span> <span class="n">diff_view_meta</span> <span class="o">=</span> <span class="n">get_view_autograd_meta</span><span class="p">(</span><span class="n">self</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">diff_view_meta</span> <span class="o">&amp;&amp;</span> <span class="n">diff_view_meta</span><span class="o">-&gt;</span><span class="n">has_bw_view</span><span class="p">())</span> <span class="p">{</span>
      <span class="n">diff_view_meta</span><span class="o">-&gt;</span><span class="n">set_attr_version</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_version</span><span class="p">());</span>
    <span class="p">}</span>
  <span class="p">}</span>
</code></pre></div></div>

<p>This tensor now will be the input to another function and the above steps will be all repeated. Check the animation below to see how the graph is created.</p>

<p align="center">
<img src="https://pytorch.org/assets/images/computational_graph_creation.gif" width="100%" />
<br />
Figure 2: Animation that shows the graph creation
</p>

<h3 id="registering-python-functions-in-the-graph">Registering Python Functions in the graph</h3>

<p>We have seen how autograd creates the graph for the functions included in ATen. However, when we define our differentiable functions in Python, they are also included in the graph!</p>

<p>An autograd python defined function looks like the following:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">Function</span><span class="p">):</span>
     <span class="o">@</span><span class="nb">staticmethod</span>
     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
         <span class="n">result</span> <span class="o">=</span> <span class="n">i</span><span class="p">.</span><span class="n">exp</span><span class="p">()</span>
         <span class="n">ctx</span><span class="p">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
         <span class="k">return</span> <span class="n">result</span>

     <span class="o">@</span><span class="nb">staticmethod</span>
     <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
         <span class="n">result</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">saved_tensors</span>
         <span class="k">return</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="n">result</span>

<span class="c1"># Call the function
</span><span class="n">Exp</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
<span class="c1"># Outputs: tensor(1.6487, grad_fn=&lt;ExpBackward&gt;)
</span></code></pre></div></div>

<p>In the above snippet autograd detected our python function when creating the graph. All of this is possible thanks to the <a href="https://github.com/pytorch/pytorch/blob/release/1.9/torch/autograd/function.py#L106"><code class="language-plaintext highlighter-rouge">Function</code></a> class. Let’s take a look at what happens when we call <code class="language-plaintext highlighter-rouge">apply</code>.</p>

<p><code class="language-plaintext highlighter-rouge">apply</code> is defined in the <a href="https://github.com/pytorch/pytorch/blob/e7cd59c7a061c78d8d0265e4308b5933e44f9176/torch/csrc/autograd/python_function.cpp#L859-L908"><code class="language-plaintext highlighter-rouge">torch._C._FunctionBase</code></a> class, but this class is not present in the python source. <code class="language-plaintext highlighter-rouge">_FunctionBase</code> is defined in C++ by using the python C API to hook C functions together into a single python class. We are looking for a function named <a href="https://github.com/pytorch/pytorch/blob/e7cd59c7a061c78d8d0265e4308b5933e44f9176/torch/csrc/autograd/python_function.cpp#L577-L633"><code class="language-plaintext highlighter-rouge">THPFunction_apply</code></a>.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">PyObject</span> <span class="o">*</span><span class="nf">THPFunction_apply</span><span class="p">(</span><span class="n">PyObject</span> <span class="o">*</span><span class="n">cls</span><span class="p">,</span> <span class="n">PyObject</span> <span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
<span class="p">{</span>
  
  <span class="c1">// Generates the graph node</span>
  <span class="n">THPObjectPtr</span> <span class="n">backward_cls</span><span class="p">(</span><span class="n">PyObject_GetAttrString</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="s">"_backward_cls"</span><span class="p">));</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">backward_cls</span><span class="p">)</span> <span class="k">return</span> <span class="nb">nullptr</span><span class="p">;</span>
  <span class="n">THPObjectPtr</span> <span class="n">ctx_obj</span><span class="p">(</span><span class="n">PyObject_CallFunctionObjArgs</span><span class="p">(</span><span class="n">backward_cls</span><span class="p">,</span> <span class="nb">nullptr</span><span class="p">));</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">ctx_obj</span><span class="p">)</span> <span class="k">return</span> <span class="nb">nullptr</span><span class="p">;</span>
  <span class="n">THPFunction</span><span class="o">*</span> <span class="n">ctx</span> <span class="o">=</span> <span class="p">(</span><span class="n">THPFunction</span><span class="o">*</span><span class="p">)</span><span class="n">ctx_obj</span><span class="p">.</span><span class="n">get</span><span class="p">();</span>

  <span class="k">auto</span> <span class="n">cdata</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">PyNode</span><span class="o">&gt;</span><span class="p">(</span><span class="k">new</span> <span class="n">PyNode</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">ctx_obj</span><span class="p">)),</span> <span class="n">deleteNode</span><span class="p">);</span>
  <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">cdata</span> <span class="o">=</span> <span class="n">cdata</span><span class="p">;</span>

  <span class="c1">// Prepare inputs and allocate context (grad fn)</span>
  <span class="c1">// Unpack inputs will collect the edges</span>
  <span class="k">auto</span> <span class="n">info_pair</span> <span class="o">=</span> <span class="n">unpack_input</span><span class="o">&lt;</span><span class="nb">false</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span>
  <span class="n">UnpackedInput</span><span class="o">&amp;</span> <span class="n">unpacked_input</span> <span class="o">=</span> <span class="n">info_pair</span><span class="p">.</span><span class="n">first</span><span class="p">;</span>
  <span class="n">InputFlags</span><span class="o">&amp;</span> <span class="n">input_info</span> <span class="o">=</span> <span class="n">info_pair</span><span class="p">.</span><span class="n">second</span><span class="p">;</span>

   <span class="c1">// Initialize backward function (and ctx)</span>
  <span class="kt">bool</span> <span class="n">is_executable</span> <span class="o">=</span> <span class="n">input_info</span><span class="p">.</span><span class="n">is_executable</span><span class="p">;</span>
  <span class="n">cdata</span><span class="o">-&gt;</span><span class="n">set_next_edges</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">input_info</span><span class="p">.</span><span class="n">next_edges</span><span class="p">));</span>
  <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">needs_input_grad</span> <span class="o">=</span> <span class="n">input_info</span><span class="p">.</span><span class="n">needs_input_grad</span><span class="p">.</span><span class="n">release</span><span class="p">();</span>
  <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">is_variable_input</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">input_info</span><span class="p">.</span><span class="n">is_variable_input</span><span class="p">);</span>

  <span class="c1">// Prepend ctx to input_tuple, in preparation for static method call</span>
  <span class="k">auto</span> <span class="n">num_args</span> <span class="o">=</span> <span class="n">PyTuple_GET_SIZE</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span>
  <span class="n">THPObjectPtr</span> <span class="n">ctx_input_tuple</span><span class="p">(</span><span class="n">PyTuple_New</span><span class="p">(</span><span class="n">num_args</span> <span class="o">+</span> <span class="mi">1</span><span class="p">));</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">ctx_input_tuple</span><span class="p">)</span> <span class="k">return</span> <span class="nb">nullptr</span><span class="p">;</span>
  <span class="n">Py_INCREF</span><span class="p">(</span><span class="n">ctx</span><span class="p">);</span>
  <span class="n">PyTuple_SET_ITEM</span><span class="p">(</span><span class="n">ctx_input_tuple</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">PyObject</span><span class="o">*</span><span class="p">)</span><span class="n">ctx</span><span class="p">);</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_args</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">PyObject</span> <span class="o">*</span><span class="n">arg</span> <span class="o">=</span> <span class="n">PyTuple_GET_ITEM</span><span class="p">(</span><span class="n">unpacked_input</span><span class="p">.</span><span class="n">input_tuple</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="n">i</span><span class="p">);</span>
    <span class="n">Py_INCREF</span><span class="p">(</span><span class="n">arg</span><span class="p">);</span>
    <span class="n">PyTuple_SET_ITEM</span><span class="p">(</span><span class="n">ctx_input_tuple</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">arg</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="c1">// Call forward</span>
  <span class="n">THPObjectPtr</span> <span class="n">tensor_outputs</span><span class="p">;</span>
  <span class="p">{</span>
    <span class="n">AutoGradMode</span> <span class="n">grad_mode</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span>
    <span class="n">THPObjectPtr</span> <span class="n">forward_fn</span><span class="p">(</span><span class="n">PyObject_GetAttrString</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="s">"forward"</span><span class="p">));</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">forward_fn</span><span class="p">)</span> <span class="k">return</span> <span class="nb">nullptr</span><span class="p">;</span>
    <span class="n">tensor_outputs</span> <span class="o">=</span> <span class="n">PyObject_CallObject</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">,</span> <span class="n">ctx_input_tuple</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">tensor_outputs</span><span class="p">)</span> <span class="k">return</span> <span class="nb">nullptr</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="c1">// Here is where the outputs gets the tensors tracked</span>
  <span class="k">return</span> <span class="n">process_outputs</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">cdata</span><span class="p">,</span> <span class="n">ctx</span><span class="p">,</span> <span class="n">unpacked_input</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">tensor_outputs</span><span class="p">),</span>
                         <span class="n">is_executable</span><span class="p">,</span> <span class="n">node</span><span class="p">);</span>
  <span class="n">END_HANDLE_TH_ERRORS</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Although this code is hard to read at first due to all the python API calls, it essentially does the same thing as the auto-generated forward functions that we saw for ATen:</p>

<p>Create a <code class="language-plaintext highlighter-rouge">grad_fn</code> object.
Collect the edges to link the current <code class="language-plaintext highlighter-rouge">grad_fn</code> with the input tensors one.
Execute the function <code class="language-plaintext highlighter-rouge">forward</code>.
Assign the created <code class="language-plaintext highlighter-rouge">grad_fn</code> to the output tensors metadata.</p>

<p>The <code class="language-plaintext highlighter-rouge">grad_fn</code> object is created in:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1">// Generates the graph node</span>
  <span class="n">THPObjectPtr</span> <span class="nf">backward_cls</span><span class="p">(</span><span class="n">PyObject_GetAttrString</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="s">"_backward_cls"</span><span class="p">));</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">backward_cls</span><span class="p">)</span> <span class="k">return</span> <span class="nb">nullptr</span><span class="p">;</span>
  <span class="n">THPObjectPtr</span> <span class="nf">ctx_obj</span><span class="p">(</span><span class="n">PyObject_CallFunctionObjArgs</span><span class="p">(</span><span class="n">backward_cls</span><span class="p">,</span> <span class="nb">nullptr</span><span class="p">));</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">ctx_obj</span><span class="p">)</span> <span class="k">return</span> <span class="nb">nullptr</span><span class="p">;</span>
  <span class="n">THPFunction</span><span class="o">*</span> <span class="n">ctx</span> <span class="o">=</span> <span class="p">(</span><span class="n">THPFunction</span><span class="o">*</span><span class="p">)</span><span class="n">ctx_obj</span><span class="p">.</span><span class="n">get</span><span class="p">();</span>

  <span class="k">auto</span> <span class="n">cdata</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">PyNode</span><span class="o">&gt;</span><span class="p">(</span><span class="k">new</span> <span class="nf">PyNode</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">ctx_obj</span><span class="p">)),</span> <span class="n">deleteNode</span><span class="p">);</span>
  <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">cdata</span> <span class="o">=</span> <span class="n">cdata</span><span class="p">;</span>
</code></pre></div></div>

<p>Basically, it asks the python API to get a pointer to the Python object that can execute the user-written function. Then it wraps it into a <a href="https://github.com/pytorch/pytorch/blob/e7cd59c7a061c78d8d0265e4308b5933e44f9176/torch/csrc/autograd/python_function.h#L24-L58"><code class="language-plaintext highlighter-rouge">PyNode</code></a> object that is a specialized <code class="language-plaintext highlighter-rouge">Node</code> object that calls the python interpreter with the provided python function when <code class="language-plaintext highlighter-rouge">apply</code> is executed during the forward pass. Note that in the code <code class="language-plaintext highlighter-rouge">cdata</code> is the actual <code class="language-plaintext highlighter-rouge">Node</code> object that is part of the graph. <code class="language-plaintext highlighter-rouge">ctx</code> is the object that is passed to the python <code class="language-plaintext highlighter-rouge">forward</code>/<code class="language-plaintext highlighter-rouge">backward</code> functions and it is used to store autograd related information by both, the user’s function and PyTorch.</p>

<p>As in the regular C++ functions we also call <code class="language-plaintext highlighter-rouge">collect_next_edges</code> to track the inputs <code class="language-plaintext highlighter-rouge">grad_fn</code> objects, but this is done in <a href="https://github.com/pytorch/pytorch/blob/e7cd59c7a061c78d8d0265e4308b5933e44f9176/torch/csrc/autograd/python_function.cpp#L413-L448"><code class="language-plaintext highlighter-rouge">unpack_input</code></a>:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span><span class="o">&lt;</span><span class="kt">bool</span> <span class="n">enforce_variables</span><span class="p">&gt;</span>
<span class="n">std</span><span class="o">::</span><span class="n">pair</span><span class="o">&lt;</span><span class="n">UnpackedInput</span><span class="p">,</span> <span class="n">InputFlags</span><span class="o">&gt;</span> <span class="n">unpack_input</span><span class="p">(</span><span class="n">PyObject</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="p">{</span>
  <span class="p">...</span>
  <span class="n">flags</span><span class="p">.</span><span class="n">next_edges</span> <span class="o">=</span> <span class="p">(</span><span class="n">flags</span><span class="p">.</span><span class="n">is_executable</span> <span class="o">?</span> <span class="n">collect_next_edges</span><span class="p">(</span><span class="n">unpacked</span><span class="p">.</span><span class="n">input_vars</span><span class="p">)</span> <span class="o">:</span> <span class="n">edge_list</span><span class="p">());</span>
  <span class="k">return</span> <span class="n">std</span><span class="o">::</span><span class="n">make_pair</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">unpacked</span><span class="p">),</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">flags</span><span class="p">));</span>
<span class="p">}</span>
</code></pre></div></div>

<p>After this, the edges are assigned to the <code class="language-plaintext highlighter-rouge">grad_fn</code> by just doing <code class="language-plaintext highlighter-rouge">cdata-&gt;set_next_edges(std::move(input_info.next_edges));</code> and the forward function is called through the python interpreter C API.</p>

<p>Once the output tensors are returned from the forward pass, they are processed and converted to variables inside the <a href="https://github.com/pytorch/pytorch/blob/e7cd59c7a061c78d8d0265e4308b5933e44f9176/torch/csrc/autograd/python_function.cpp#L519-L562"><code class="language-plaintext highlighter-rouge">process_outputs</code></a> function.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">PyObject</span><span class="o">*</span> <span class="nf">process_outputs</span><span class="p">(</span><span class="n">PyObject</span> <span class="o">*</span><span class="n">op_obj</span><span class="p">,</span> <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">PyNode</span><span class="o">&gt;&amp;</span> <span class="n">cdata</span><span class="p">,</span>
                          <span class="n">THPFunction</span><span class="o">*</span> <span class="n">grad_fn</span><span class="p">,</span> <span class="k">const</span> <span class="n">UnpackedInput</span><span class="o">&amp;</span> <span class="n">unpacked</span><span class="p">,</span>
                          <span class="n">PyObject</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">THPObjectPtr</span><span class="o">&amp;&amp;</span> <span class="n">raw_output</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">is_executable</span><span class="p">,</span>
                          <span class="n">torch</span><span class="o">::</span><span class="n">jit</span><span class="o">::</span><span class="n">Node</span><span class="o">*</span> <span class="n">node</span><span class="p">)</span> <span class="p">{</span>
  <span class="p">...</span>
  <span class="n">_wrap_outputs</span><span class="p">(</span><span class="n">cdata</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">,</span> <span class="n">unpacked</span><span class="p">.</span><span class="n">input_vars</span><span class="p">,</span> <span class="n">raw_output</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">is_executable</span><span class="p">);</span>
  <span class="n">_trace_post_record</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">op_obj</span><span class="p">,</span> <span class="n">unpacked</span><span class="p">.</span><span class="n">input_vars</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">is_inplace</span><span class="p">,</span> <span class="n">unpack_output</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">is_executable</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">_save_variables</span><span class="p">(</span><span class="n">cdata</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">);</span>
  <span class="p">}</span> <span class="p">...</span>
  <span class="k">return</span> <span class="n">outputs</span><span class="p">.</span><span class="n">release</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Here, <a href="https://github.com/pytorch/pytorch/blob/e7cd59c7a061c78d8d0265e4308b5933e44f9176/torch/csrc/autograd/python_function.cpp#L302-L346"><code class="language-plaintext highlighter-rouge">_wrap_outputs</code></a> is in charge of setting the forward outputs <code class="language-plaintext highlighter-rouge">grad_fn</code> to the newly created one. For this, it calls another <code class="language-plaintext highlighter-rouge">_wrap_outputs</code> function defined in a different <a href="https://github.com/pytorch/pytorch/blob/e7cd59c7a061c78d8d0265e4308b5933e44f9176/torch/csrc/autograd/custom_function.cpp#L28-L105">file</a>, so the process here gets a little confusing.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="kt">void</span> <span class="nf">_wrap_outputs</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">PyNode</span><span class="o">&gt;&amp;</span> <span class="n">cdata</span><span class="p">,</span> <span class="n">THPFunction</span> <span class="o">*</span><span class="n">self</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">variable_list</span> <span class="o">&amp;</span><span class="n">input_vars</span><span class="p">,</span> <span class="n">PyObject</span> <span class="o">*</span><span class="n">raw_output</span><span class="p">,</span> <span class="n">PyObject</span> <span class="o">*</span><span class="n">outputs</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">is_executable</span><span class="p">)</span>
<span class="p">{</span>
  <span class="k">auto</span> <span class="n">cdata_if_executable</span> <span class="o">=</span> <span class="n">is_executable</span> <span class="o">?</span> <span class="n">cdata</span> <span class="o">:</span> <span class="nb">nullptr</span><span class="p">;</span>
 <span class="p">...</span>

  <span class="c1">// Wrap only the tensor outputs.</span>
  <span class="c1">// This calls csrc/autograd/custom_function.cpp</span>
  <span class="k">auto</span> <span class="n">wrapped_outputs</span> <span class="o">=</span> <span class="n">_wrap_outputs</span><span class="p">(</span><span class="n">input_vars</span><span class="p">,</span> <span class="n">non_differentiable</span><span class="p">,</span> <span class="n">dirty_inputs</span><span class="p">,</span> <span class="n">raw_output_vars</span><span class="p">,</span> <span class="n">cdata_if_executable</span><span class="p">);</span>
<span class="p">...</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The called <code class="language-plaintext highlighter-rouge">_wrap_outputs</code> is the one in charge of setting the autograd metadata in the output tensors:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">Variable</span><span class="o">&gt;&gt;</span> <span class="n">_wrap_outputs</span><span class="p">(</span><span class="k">const</span> <span class="n">variable_list</span> <span class="o">&amp;</span><span class="n">input_vars</span><span class="p">,</span>
  <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">unordered_set</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*&gt;</span> <span class="o">&amp;</span><span class="n">non_differentiable</span><span class="p">,</span>
  <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">unordered_set</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*&gt;</span> <span class="o">&amp;</span><span class="n">dirty_inputs</span><span class="p">,</span>
  <span class="k">const</span> <span class="n">at</span><span class="o">::</span><span class="n">ArrayRef</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">Variable</span><span class="o">&gt;&gt;</span> <span class="n">raw_outputs</span><span class="p">,</span>
  <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="o">&amp;</span><span class="n">cdata</span><span class="p">)</span> <span class="p">{</span>


  <span class="n">std</span><span class="o">::</span><span class="n">unordered_set</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*&gt;</span> <span class="n">inputs</span><span class="p">;</span>
  <span class="err">…</span>
  <span class="c1">// Sets the grad_fn and output_nr of an output Variable.</span>
  <span class="k">auto</span> <span class="n">set_history</span> <span class="o">=</span> <span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">Variable</span><span class="o">&amp;</span> <span class="n">var</span><span class="p">,</span> <span class="kt">uint32_t</span> <span class="n">output_nr</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">is_input</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">is_modified</span><span class="p">,</span>
                         <span class="kt">bool</span> <span class="n">is_differentiable</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Lots of checks</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">is_differentiable</span><span class="p">)</span> <span class="p">{</span>
     <span class="p">...</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">is_input</span><span class="p">)</span> <span class="p">{</span>
      <span class="c1">// An input has been returned, but it wasn't modified. Return it as a view</span>
      <span class="c1">// so that we can attach a new grad_fn to the Variable.</span>
      <span class="c1">// Run in no_grad mode to mimic the behavior of the forward.</span>
      <span class="p">{</span>
        <span class="n">AutoGradMode</span> <span class="n">grad_mode</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span>
        <span class="n">var</span> <span class="o">=</span> <span class="n">var</span><span class="p">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">var</span><span class="p">);</span>
      <span class="p">}</span>
      <span class="n">impl</span><span class="o">::</span><span class="n">set_gradient_edge</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="p">{</span><span class="n">cdata</span><span class="p">,</span> <span class="n">output_nr</span><span class="p">});</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">cdata</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">impl</span><span class="o">::</span><span class="n">set_gradient_edge</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="p">{</span><span class="n">cdata</span><span class="p">,</span> <span class="n">output_nr</span><span class="p">});</span>
    <span class="p">}</span>
  <span class="p">};</span>
</code></pre></div></div>

<p>And this is where <code class="language-plaintext highlighter-rouge">set_gradient_edge</code> was called and this is how a user-written python function gets included in the computational graph with its associated backward function!</p>

<h1 id="closing-remarks">Closing remarks</h1>

<p>This blog post is intended to be a code overview on how PyTorch constructs the actual computational graphs that we discussed in the previous post. The next entry will deal with how the autograd engine executes these graphs.</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="lf-grid">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org" class="footer-logo">
          <img src="/assets/images/logo-icon.svg" alt="PyTorch logo" width="40">
        </a>
      </div>

      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook"><path fill="currentColor" d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z"/></svg>	
        </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X"><path fill="currentColor" d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66"/></svg>
        </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube"><path fill="currentColor" d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z"/></svg>	
        </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn"><rect width="512" height="512" rx="0" fill="currentColor"/><circle fill="#000" cx="142" cy="138" r="37"/><path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198"/><path fill="#000" d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
        </a></li>
        <li><a href="https://social.lfx.dev/@pytorch" rel="me" target="_blank" title="PyTorch on Mastodon">
          <svg fill="currentColor" aria-label="Mastodon" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" xml:space="preserve"><path d="M21.327 8.566c0-4.339-2.843-5.61-2.843-5.61-1.433-.658-3.894-.935-6.451-.956h-.063c-2.557.021-5.016.298-6.45.956 0 0-2.843 1.272-2.843 5.61 0 .993-.019 2.181.012 3.441.103 4.243.778 8.425 4.701 9.463 1.809.479 3.362.579 4.612.51 2.268-.126 3.541-.809 3.541-.809l-.075-1.646s-1.621.511-3.441.449c-1.804-.062-3.707-.194-3.999-2.409a4.523 4.523 0 0 1-.04-.621s1.77.433 4.014.536c1.372.063 2.658-.08 3.965-.236 2.506-.299 4.688-1.843 4.962-3.254.434-2.223.398-5.424.398-5.424zm-3.353 5.59h-2.081V9.057c0-1.075-.452-1.62-1.357-1.62-1 0-1.501.647-1.501 1.927v2.791h-2.069V9.364c0-1.28-.501-1.927-1.502-1.927-.905 0-1.357.546-1.357 1.62v5.099H6.026V8.903c0-1.074.273-1.927.823-2.558.566-.631 1.307-.955 2.228-.955 1.065 0 1.872.409 2.405 1.228l.518.869.519-.869c.533-.819 1.34-1.228 2.405-1.228.92 0 1.662.324 2.228.955.549.631.822 1.484.822 2.558v5.253z"/></svg>
        </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/legal/policies/">Linux Foundation Policies</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">LF Projects, LLC Policies</a>. <a href="https://www.linuxfoundation.org/privacy">Privacy Policy</a> and <a href="https://www.linuxfoundation.org/terms">Terms of Use</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Learn</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/get-started">Get Started</a> 
          </li>
          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/#community-module">Community</a>
          </li>
          <li>
            <a href="https://discuss.pytorch.org">Forums</a>
          </li>
          <li>
            <a href="/resources">Developer Resources</a>
          </li>
          <li>
            <a href="/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Docs</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/docs">PyTorch</a>
          </li>
          <li>
            <a href="/pytorch-domains">PyTorch Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/blog">PyTorch Blog</a>
          </li>
          <li>
            <a href="/community-blog">Community Blog</a>
          </li>
          <li>
            <a href="/videos">Videos</a>
          </li>
          <li>
            <a href="/community-stories">Community Stories</a>
          </li>
          <li>
            <a href="/events">Events</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="/governing-board">Governing Board</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a href="/join">Become a Member</a>
        </li>
        <li class="resources-mobile-menu-title">
          <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub"><div id="topnav-gh-icon"></div></a>
        </li>

      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header, .announcement-header, .comm-stories-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

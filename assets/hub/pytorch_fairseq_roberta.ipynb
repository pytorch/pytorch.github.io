{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08afc08f",
   "metadata": {},
   "source": [
    "### This notebook is optionally accelerated with a GPU runtime.\n",
    "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "# RoBERTa\n",
    "\n",
    "*Author: Facebook AI (fairseq Team)*\n",
    "\n",
    "**A Robustly Optimized BERT Pretraining Approach**\n",
    "\n",
    "\n",
    "\n",
    "### Model Description\n",
    "\n",
    "Bidirectional Encoder Representations from Transformers, or [BERT][1], is a\n",
    "revolutionary self-supervised pretraining technique that learns to predict\n",
    "intentionally hidden (masked) sections of text. Crucially, the representations\n",
    "learned by BERT have been shown to generalize well to downstream tasks, and when\n",
    "BERT was first released in 2018 it achieved state-of-the-art results on many NLP\n",
    "benchmark datasets.\n",
    "\n",
    "[RoBERTa][2] builds on BERT's language masking strategy and modifies key\n",
    "hyperparameters in BERT, including removing BERT's next-sentence pretraining\n",
    "objective, and training with much larger mini-batches and learning rates.\n",
    "RoBERTa was also trained on an order of magnitude more data than BERT, for a\n",
    "longer amount of time. This allows RoBERTa representations to generalize even\n",
    "better to downstream tasks compared to BERT.\n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "We require a few additional Python dependencies for preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b439234",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install regex requests hydra-core omegaconf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32b1de",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "##### Load RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34466ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')\n",
    "roberta.eval()  # disable dropout (or leave in train mode to finetune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbda96d",
   "metadata": {},
   "source": [
    "##### Apply Byte-Pair Encoding (BPE) to input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7f80a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = roberta.encode('Hello world!')\n",
    "assert tokens.tolist() == [0, 31414, 232, 328, 2]\n",
    "assert roberta.decode(tokens) == 'Hello world!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6556b4d2",
   "metadata": {},
   "source": [
    "##### Extract features from RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b339c1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the last layer's features\n",
    "last_layer_features = roberta.extract_features(tokens)\n",
    "assert last_layer_features.size() == torch.Size([1, 5, 1024])\n",
    "\n",
    "# Extract all layer's features (layer 0 is the embedding layer)\n",
    "all_layers = roberta.extract_features(tokens, return_all_hiddens=True)\n",
    "assert len(all_layers) == 25\n",
    "assert torch.all(all_layers[-1] == last_layer_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3962a8",
   "metadata": {},
   "source": [
    "##### Use RoBERTa for sentence-pair classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3935e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download RoBERTa already finetuned for MNLI\n",
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')\n",
    "roberta.eval()  # disable dropout for evaluation\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Encode a pair of sentences and make a prediction\n",
    "    tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is not very optimized.')\n",
    "    prediction = roberta.predict('mnli', tokens).argmax().item()\n",
    "    assert prediction == 0  # contradiction\n",
    "\n",
    "    # Encode another pair of sentences\n",
    "    tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is based on BERT.')\n",
    "    prediction = roberta.predict('mnli', tokens).argmax().item()\n",
    "    assert prediction == 2  # entailment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d29eaa5",
   "metadata": {},
   "source": [
    "##### Register a new (randomly initialized) classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7456acb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta.register_classification_head('new_task', num_classes=3)\n",
    "logprobs = roberta.predict('new_task', tokens)  # tensor([[-1.1050, -1.0672, -1.1245]], grad_fn=<LogSoftmaxBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf7033",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding][1]\n",
    "- [RoBERTa: A Robustly Optimized BERT Pretraining Approach][2]\n",
    "\n",
    "\n",
    "[1]: https://arxiv.org/abs/1810.04805\n",
    "[2]: https://arxiv.org/abs/1907.11692"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

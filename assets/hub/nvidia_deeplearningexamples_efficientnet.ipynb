{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661b2bf7",
   "metadata": {},
   "source": [
    "### This notebook requires a GPU runtime to run.\n",
    "### Please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "# EfficientNet\n",
    "\n",
    "*Author: NVIDIA*\n",
    "\n",
    "**EfficientNets are a family of image classification models, which achieve state-of-the-art accuracy, being an order-of-magnitude smaller and faster. Trained with mixed precision using Tensor Cores.**\n",
    "\n",
    "<img src=\"https://pytorch.org/assets/images/classification.jpg\" alt=\"alt\" width=\"50%\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Model Description\n",
    "\n",
    "EfficientNet is an image classification model family. It was first described in [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946). This notebook allows you to load and test the EfficientNet-B0, EfficientNet-B4, EfficientNet-WideSE-B0 and, EfficientNet-WideSE-B4 models.\n",
    "\n",
    "EfficientNet-WideSE models use Squeeze-and-Excitation layers wider than original EfficientNet models, the width of SE module is proportional to the width of Depthwise Separable Convolutions instead of block width.\n",
    "\n",
    "WideSE models are slightly more accurate than original models.\n",
    "\n",
    "This model is trained with mixed precision using Tensor Cores on Volta and the NVIDIA Ampere GPU architectures. Therefore, researchers can get results over 2x faster than training without Tensor Cores, while experiencing the benefits of mixed precision training. This model is tested against each NGC monthly container release to ensure consistent accuracy and performance over time.\n",
    "\n",
    "We use [NHWC data layout](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html) when training using Mixed Precision.\n",
    "\n",
    "### Example\n",
    "\n",
    "In the example below we will use the pretrained ***EfficientNet*** model to perform inference on image and present the result.\n",
    "\n",
    "To run the example you need some extra python packages installed. These are needed for preprocessing images and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcf9d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install validators matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3381d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27944006",
   "metadata": {},
   "source": [
    "Load the model pretrained on ImageNet dataset.\n",
    "\n",
    "You can choose among the following models:\n",
    "\n",
    "| TorchHub entrypoint | Description |\n",
    "| :----- | :----- |\n",
    "| `nvidia_efficientnet_b0` | baseline EfficientNet |\n",
    "| `nvidia_efficientnet_b4` | scaled EfficientNet|\n",
    "| `nvidia_efficientnet_widese_b0` | model with Squeeze-and-Excitation layers wider than baseline EfficientNet model |\n",
    "| `nvidia_efficientnet_widese_b4` | model with Squeeze-and-Excitation layers wider than scaled EfficientNet model |\n",
    "\n",
    "There are also quantized version of the models, but they require nvidia container. See [quantized models](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets/efficientnet#quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d279b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\n",
    "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n",
    "\n",
    "efficientnet.eval().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa28230f",
   "metadata": {},
   "source": [
    "Prepare sample input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa251bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "uris = [\n",
    "    'http://images.cocodataset.org/test-stuff2017/000000024309.jpg',\n",
    "    'http://images.cocodataset.org/test-stuff2017/000000028117.jpg',\n",
    "    'http://images.cocodataset.org/test-stuff2017/000000006149.jpg',\n",
    "    'http://images.cocodataset.org/test-stuff2017/000000004954.jpg',\n",
    "]\n",
    "\n",
    "batch = torch.cat(\n",
    "    [utils.prepare_input_from_uri(uri) for uri in uris]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9b7863",
   "metadata": {},
   "source": [
    "Run inference. Use `pick_n_best(predictions=output, n=topN)` helper function to pick N most probable hypotheses according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168d6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = torch.nn.functional.softmax(efficientnet(batch), dim=1)\n",
    "    \n",
    "results = utils.pick_n_best(predictions=output, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813f923",
   "metadata": {},
   "source": [
    "Display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1014562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for uri, result in zip(uris, results):\n",
    "    img = Image.open(requests.get(uri, stream=True).raw)\n",
    "    img.thumbnail((256,256), Image.ANTIALIAS)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88505561",
   "metadata": {},
   "source": [
    "### Details\n",
    "For detailed information on model input and output, training recipies, inference and performance visit:\n",
    "[github](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets/efficientnet)\n",
    "and/or [NGC](https://ngc.nvidia.com/catalog/resources/nvidia:efficientnet_for_pytorch)\n",
    "\n",
    "### References\n",
    "\n",
    " - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)\n",
    " - [model on NGC](https://ngc.nvidia.com/catalog/resources/nvidia:efficientnet_for_pytorch)\n",
    " - [model on github](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets/efficientnet)\n",
    " - [pretrained model on NGC (efficientnet-b0)](https://ngc.nvidia.com/catalog/models/nvidia:efficientnet_b0_pyt_amp)\n",
    " - [pretrained model on NGC (efficientnet-b4)](https://ngc.nvidia.com/catalog/models/nvidia:efficientnet_b4_pyt_amp)\n",
    " - [pretrained model on NGC (efficientnet-widese-b0)](https://ngc.nvidia.com/catalog/models/nvidia:efficientnet_widese_b0_pyt_amp)\n",
    " - [pretrained model on NGC (efficientnet-widese-b4)](https://ngc.nvidia.com/catalog/models/nvidia:efficientnet_widese_b4_pyt_amp)\n",
    " - [pretrained, quantized model on NGC (efficientnet-widese-b0)](https://ngc.nvidia.com/catalog/models/nvidia:efficientnet_widese_b0_pyt_amp)\n",
    " - [pretrained, quantized model on NGC (efficientnet-widese-b4)](https://ngc.nvidia.com/catalog/models/nvidia:efficientnet_widese_b4_pyt_amp)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

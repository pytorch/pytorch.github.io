{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98244ed5",
   "metadata": {},
   "source": [
    "### This notebook is optionally accelerated with a GPU runtime.\n",
    "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "# YOLOP\n",
    "\n",
    "*Author: Hust Visual Learning Team*\n",
    "\n",
    "**YOLOP pretrained on the BDD100K dataset**\n",
    "\n",
    "## Before You Start\n",
    "To install YOLOP dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0aaeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install -qr https://github.com/hustvl/YOLOP/blob/main/requirements.txt  # install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e3712d",
   "metadata": {},
   "source": [
    "## YOLOP: You Only Look Once for Panoptic driving Perception\n",
    "\n",
    "### Model Description\n",
    "\n",
    "<img width=\"800\" alt=\"YOLOP Model\" src=\"https://github.com/hustvl/YOLOP/raw/main/pictures/yolop.png\">\n",
    "&nbsp;\n",
    "\n",
    "- YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the **BDD100K** dataset.\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "#### Traffic Object Detection Result\n",
    "\n",
    "| Model          | Recall(%) | mAP50(%) | Speed(fps) |\n",
    "| -------------- | --------- | -------- | ---------- |\n",
    "| `Multinet`     | 81.3      | 60.2     | 8.6        |\n",
    "| `DLT-Net`      | 89.4      | 68.4     | 9.3        |\n",
    "| `Faster R-CNN` | 77.2      | 55.6     | 5.3        |\n",
    "| `YOLOv5s`      | 86.8      | 77.2     | 82         |\n",
    "| `YOLOP(ours)`  | 89.2      | 76.5     | 41         |\n",
    "\n",
    "#### Drivable Area Segmentation Result\n",
    "\n",
    "| Model         | mIOU(%) | Speed(fps) |\n",
    "| ------------- | ------- | ---------- |\n",
    "| `Multinet`    | 71.6    | 8.6        |\n",
    "| `DLT-Net`     | 71.3    | 9.3        |\n",
    "| `PSPNet`      | 89.6    | 11.1       |\n",
    "| `YOLOP(ours)` | 91.5    | 41         |\n",
    "\n",
    "#### Lane Detection Result\n",
    "\n",
    "| Model         | mIOU(%) | IOU(%) |\n",
    "| ------------- | ------- | ------ |\n",
    "| `ENet`        | 34.12   | 14.64  |\n",
    "| `SCNN`        | 35.79   | 15.84  |\n",
    "| `ENet-SAD`    | 36.56   | 16.02  |\n",
    "| `YOLOP(ours)` | 70.50   | 26.20  |\n",
    "\n",
    "#### Ablation Studies 1: End-to-end v.s. Step-by-step\n",
    "\n",
    "| Training_method | Recall(%) | AP(%) | mIoU(%) | Accuracy(%) | IoU(%) |\n",
    "| --------------- | --------- | ----- | ------- | ----------- | ------ |\n",
    "| `ES-W`          | 87.0      | 75.3  | 90.4    | 66.8        | 26.2   |\n",
    "| `ED-W`          | 87.3      | 76.0  | 91.6    | 71.2        | 26.1   |\n",
    "| `ES-D-W`        | 87.0      | 75.1  | 91.7    | 68.6        | 27.0   |\n",
    "| `ED-S-W`        | 87.5      | 76.1  | 91.6    | 68.0        | 26.8   |\n",
    "| `End-to-end`    | 89.2      | 76.5  | 91.5    | 70.5        | 26.2   |\n",
    "\n",
    "#### Ablation Studies 2: Multi-task v.s. Single task\n",
    "\n",
    "| Training_method | Recall(%) | AP(%) | mIoU(%) | Accuracy(%) | IoU(%) | Speed(ms/frame) |\n",
    "| --------------- | --------- | ----- | ------- | ----------- | ------ | --------------- |\n",
    "| `Det(only)`     | 88.2      | 76.9  | -       | -           | -      | 15.7            |\n",
    "| `Da-Seg(only)`  | -         | -     | 92.0    | -           | -      | 14.8            |\n",
    "| `Ll-Seg(only)`  | -         | -     | -       | 79.6        | 27.9   | 14.8            |\n",
    "| `Multitask`     | 89.2      | 76.5  | 91.5    | 70.5        | 26.2   | 24.4            |\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "- In table 4, E, D, S and W refer to Encoder, Detect head, two Segment heads and whole network. So the Algorithm (First, we only train Encoder and Detect head. Then we freeze the Encoder and Detect head as well as train two Segmentation heads. Finally, the entire network is trained jointly for all three tasks.) can be marked as ED-S-W, and the same for others.\n",
    "\n",
    "### Visualization\n",
    "\n",
    "#### Traffic Object Detection Result\n",
    "\n",
    "<img width=\"800\" alt=\"Traffic Object Detection Result\" src=\"https://github.com/hustvl/YOLOP/raw/main/pictures/detect.png\">\n",
    "&nbsp;\n",
    "\n",
    "#### Drivable Area Segmentation Result\n",
    "\n",
    "<img width=\"800\" alt=\"Drivable Area Segmentation Result\" src=\"https://github.com/hustvl/YOLOP/raw/main/pictures/da.png\">\n",
    "&nbsp;\n",
    "\n",
    "#### Lane Detection Result\n",
    "\n",
    "<img width=\"800\" alt=\"Lane Detection Result\" src=\"https://github.com/hustvl/YOLOP/raw/main/pictures/ll.png\">\n",
    "&nbsp;\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "- The visualization of lane detection result has been post processed by quadratic fitting.\n",
    "\n",
    "### Deployment\n",
    "\n",
    "Our model can reason in real-time on **Jetson Tx2**, with **Zed Camera** to capture image. We use **TensorRT** tool for speeding up. We provide code for deployment and reasoning of model in [github code](https://github.com/hustvl/YOLOP/tree/main/toolkits/deploy).\n",
    "\n",
    "\n",
    "### Load From PyTorch Hub\n",
    "This example loads the pretrained **YOLOP** model and passes an image for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce846e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# load model\n",
    "model = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\n",
    "\n",
    "#inference\n",
    "img = torch.randn(1,3,640,640)\n",
    "det_out, da_seg_out,ll_seg_out = model(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4d4def",
   "metadata": {},
   "source": [
    "### Citation\n",
    "\n",
    "See for more detail in [github code](https://github.com/hustvl/YOLOP) and [arxiv paper](https://arxiv.org/abs/2108.11250).\n",
    "\n",
    "If you find our paper and code useful for your research, please consider giving a star and citation:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

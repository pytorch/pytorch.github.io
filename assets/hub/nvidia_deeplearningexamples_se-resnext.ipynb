{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30023af5",
   "metadata": {},
   "source": [
    "### This notebook requires a GPU runtime to run.\n",
    "### Please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "# SE-ResNeXt101\n",
    "\n",
    "*Author: NVIDIA*\n",
    "\n",
    "**ResNeXt with Squeeze-and-Excitation module added, trained with mixed precision using Tensor Cores.**\n",
    "\n",
    "_ | _\n",
    "- | -\n",
    "![alt](https://pytorch.org/assets/images/SEArch.png) | ![alt](https://pytorch.org/assets/images/classification.jpg)\n",
    "\n",
    "\n",
    "\n",
    "### Model Description\n",
    "\n",
    "The ***SE-ResNeXt101-32x4d*** is a [ResNeXt101-32x4d](https://arxiv.org/pdf/1611.05431.pdf)\n",
    "model with added Squeeze-and-Excitation module introduced\n",
    "in the [Squeeze-and-Excitation Networks](https://arxiv.org/pdf/1709.01507.pdf) paper.\n",
    "\n",
    "This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures. Therefore, researchers can get results 3x faster than training without Tensor Cores, while experiencing the benefits of mixed precision training. This model is tested against each NGC monthly container release to ensure consistent accuracy and performance over time.\n",
    "\n",
    "We use [NHWC data layout](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html) when training using Mixed Precision.\n",
    "\n",
    "#### Model architecture\n",
    "\n",
    "![SEArch](https://pytorch.org/assets/images/SEArch.png)\n",
    "\n",
    "_Image source: [Squeeze-and-Excitation Networks](https://arxiv.org/pdf/1709.01507.pdf)_\n",
    "\n",
    "Image shows the architecture of SE block and where is it placed in ResNet bottleneck block.\n",
    "\n",
    "\n",
    "Note that the SE-ResNeXt101-32x4d model can be deployed for inference on the [NVIDIA Triton Inference Server](https://github.com/triton-inference-server/server) using TorchScript, ONNX Runtime or TensorRT as an execution backend. For details check [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/resources/se_resnext_for_triton_from_pytorch).\n",
    "\n",
    "### Example\n",
    "\n",
    "In the example below we will use the pretrained ***SE-ResNeXt101-32x4d*** model to perform inference on images and present the result.\n",
    "\n",
    "To run the example you need some extra python packages installed. These are needed for preprocessing images and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install validators matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961e1a62",
   "metadata": {},
   "source": [
    "Load the model pretrained on ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a5e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "resneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d')\n",
    "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n",
    "\n",
    "resneXt.eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1339f1ce",
   "metadata": {},
   "source": [
    "Prepare sample input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7637bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "uris = [\n",
    "    'http://images.cocodataset.org/test-stuff2017/000000024309.jpg',\n",
    "    'http://images.cocodataset.org/test-stuff2017/000000028117.jpg',\n",
    "    'http://images.cocodataset.org/test-stuff2017/000000006149.jpg',\n",
    "    'http://images.cocodataset.org/test-stuff2017/000000004954.jpg',\n",
    "]\n",
    "\n",
    "\n",
    "batch = torch.cat(\n",
    "    [utils.prepare_input_from_uri(uri) for uri in uris]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b94b64b",
   "metadata": {},
   "source": [
    "Run inference. Use `pick_n_best(predictions=output, n=topN)` helper function to pick N most probable hypotheses according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c197d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = torch.nn.functional.softmax(resneXt(batch), dim=1)\n",
    "    \n",
    "results = utils.pick_n_best(predictions=output, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4c35ea",
   "metadata": {},
   "source": [
    "Display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for uri, result in zip(uris, results):\n",
    "    img = Image.open(requests.get(uri, stream=True).raw)\n",
    "    img.thumbnail((256,256), Image.ANTIALIAS)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1d0f92",
   "metadata": {},
   "source": [
    "### Details\n",
    "For detailed information on model input and output, training recipies, inference and performance visit:\n",
    "[github](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets/se-resnext101-32x4d)\n",
    "and/or [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/resources/se_resnext_for_pytorch).\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    " - [Squeeze-and-Excitation Networks](https://arxiv.org/pdf/1709.01507.pdf)\n",
    " - [model on github](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets/se-resnext101-32x4d)\n",
    " - [model on NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/resources/se_resnext_for_pytorch)\n",
    " - [pretrained model on NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/models/seresnext101_32x4d_pyt_amp)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b195a467",
   "metadata": {},
   "source": [
    "### This notebook is optionally accelerated with a GPU runtime.\n",
    "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "# Open-Unmix\n",
    "\n",
    "*Author: Inria*\n",
    "\n",
    "**Reference implementation for music source separation**\n",
    "\n",
    "<img src=\"https://pytorch.org/assets/images/sigsep_umx-diagram.png\" alt=\"alt\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b399f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# assuming you have a PyTorch >=1.6.0 installed\n",
    "pip install -q torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c3746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# loading umxhq four target separator\n",
    "separator = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq')\n",
    "\n",
    "# generate random audio\n",
    "# ... with shape (nb_samples, nb_channels, nb_timesteps)\n",
    "# ... and with the same sample rate as that of the separator\n",
    "audio = torch.rand((1, 2, 100000))\n",
    "original_sample_rate = separator.sample_rate\n",
    "\n",
    "# make sure to resample the audio to models' sample rate, separator.sample_rate, if the two are different\n",
    "# resampler = torchaudio.transforms.Resample(original_sample_rate, separator.sample_rate)\n",
    "# audio = resampler(audio)\n",
    "\n",
    "estimates = separator(audio)\n",
    "# estimates.shape = (1, 4, 2, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b504d338",
   "metadata": {},
   "source": [
    "### Model Description\n",
    "\n",
    "__Open-Unmix__ provides ready-to-use models that allow users to separate pop music into four stems: __vocals__, __drums__, __bass__ and the remaining __other__ instruments. The models were pre-trained on the freely available [MUSDB18](https://sigsep.github.io/datasets/musdb.html) dataset.\n",
    "\n",
    "Each target model is based on a three-layer bidirectional deep LSTM. The model learns to predict the magnitude spectrogram of a target source, like vocals, from the magnitude spectrogram of a mixture input. Internally, the prediction is obtained by applying a mask on the input. The model is optimized in the magnitude domain using mean squared error.\n",
    "\n",
    "A `Separator` meta-model (as shown in the code example above) puts together multiple _Open-unmix_ spectrogram models for each desired target, and combines their output through a multichannel generalized Wiener filter, before application of inverse STFTs using `torchaudio`.\n",
    "The filtering is differentiable (but parameter-free) version of [norbert](https://github.com/sigsep/norbert).\n",
    "\n",
    "### Pre-trained `Separator` models\n",
    "\n",
    "* __`umxhq` (default)__  trained on [MUSDB18-HQ](https://sigsep.github.io/datasets/musdb.html#uncompressed-wav) which comprises the same tracks as in MUSDB18 but un-compressed which yield in a full bandwidth of 22050 Hz.\n",
    "\n",
    "* __`umx`__ is trained on the regular [MUSDB18](https://sigsep.github.io/datasets/musdb.html#compressed-stems) which is bandwidth limited to 16 kHz due to AAC compression. This model should be used for comparison with other (older) methods for evaluation in [SiSEC18](sisec18.unmix.app).\n",
    "\n",
    "Furthermore, we provide a model for speech enhancement trained by [Sony Corporation](link)\n",
    "\n",
    "* __`umxse`__ speech enhancement model is trained on the 28-speaker version of the [Voicebank+DEMAND corpus](https://datashare.is.ed.ac.uk/handle/10283/1942?show=full).\n",
    "\n",
    "All three models are also available as spectrogram (core) models, which take magnitude spectrogram inputs and ouput separated spectrograms.\n",
    "These models can be loaded using `umxhq_spec`, `umx_spec` and `umxse_spec`.\n",
    "\n",
    "### Details\n",
    "\n",
    "For additional examples, documentation and usage examples,  please visit this [the github repo](https://github.com/sigsep/open-unmix-pytorch).\n",
    "\n",
    "Furthermore, the models and all utility function to preprocess, read and save audio stems, are available in a python package that can be installed via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c4b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install openunmix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b435c1aa",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [Open-Unmix - A Reference Implementation for Music Source Separation](https://doi.org/10.21105/joss.01667)\n",
    "- [SigSep - Open Ressources for Music Separation](https://sigsep.github.io/)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

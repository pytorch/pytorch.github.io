{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dfcd836",
   "metadata": {},
   "source": [
    "### This notebook requires a GPU runtime to run.\n",
    "### Please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "# FastPitch 2\n",
    "\n",
    "*Author: NVIDIA*\n",
    "\n",
    "**The FastPitch model for generating mel spectrograms from text**\n",
    "\n",
    "<img src=\"https://pytorch.org/assets/images/fastpitch_model.png\" alt=\"alt\" width=\"50%\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Model Description\n",
    "\n",
    "This notebook demonstrates a PyTorch implementation of the FastPitch model described in the [FastPitch](https://arxiv.org/abs/2006.06873) paper.\n",
    "The FastPitch model generates mel-spectrograms and predicts a pitch contour from raw input text. In version 1.1, it does not need any pre-trained aligning model to bootstrap from. To get the audio waveform we need a second model that will produce it from the generated mel-spectrogram. In this notebook we use HiFi-GAN model for that second step.\n",
    "\n",
    "The FastPitch model is based on the [FastSpeech](https://arxiv.org/abs/1905.09263) model. The main differences between FastPitch vs FastSpeech are as follows:\n",
    "* no dependence on external aligner (Transformer TTS, Tacotron 2); in version 1.1, FastPitch aligns audio to transcriptions by itself as in [One TTS Alignment To Rule Them All](https://arxiv.org/abs/2108.10447),\n",
    "* FastPitch explicitly learns to predict the pitch contour,\n",
    "* pitch conditioning removes harsh sounding artifacts and provides faster convergence,\n",
    "* no need for distilling mel-spectrograms with a teacher model,\n",
    "* capabilities to train a multi-speaker model.\n",
    "\n",
    "\n",
    "#### Model architecture\n",
    "\n",
    "![FastPitch Architecture](https://raw.githubusercontent.com/NVIDIA/DeepLearningExamples/master/PyTorch/SpeechSynthesis/FastPitch/img/fastpitch_model.png)\n",
    "\n",
    "### Example\n",
    "In the example below:\n",
    "\n",
    "- pretrained FastPitch and HiFiGAN models are loaded from torch.hub\n",
    "- given tensor representation of an input text (\"Say this smoothly to prove you are not a robot.\"), FastPitch generates mel spectrogram\n",
    "- HiFiGAN generates sound given the mel spectrogram\n",
    "- the output sound is saved in an 'audio.wav' file\n",
    "\n",
    "To run the example you need some extra python packages installed. These are needed for preprocessing of text and audio, as well as for display and input/output handling. Finally, for better performance of FastPitch model, we download the CMU pronounciation dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dc2067",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "apt-get update\n",
    "apt-get install -y libsndfile1 wget\n",
    "pip install numpy scipy librosa unidecode inflect librosa matplotlib==3.6.3\n",
    "wget https://raw.githubusercontent.com/NVIDIA/NeMo/263a30be71e859cee330e5925332009da3e5efbc/scripts/tts_dataset_files/heteronyms-052722 -qO heteronyms\n",
    "wget https://raw.githubusercontent.com/NVIDIA/NeMo/263a30be71e859cee330e5925332009da3e5efbc/scripts/tts_dataset_files/cmudict-0.7b_nv22.08 -qO cmudict-0.7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389320f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d20e66b",
   "metadata": {},
   "source": [
    "Download and setup FastPitch generator model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a087b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastpitch, generator_train_setup = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_fastpitch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1525071",
   "metadata": {},
   "source": [
    "Download and setup vocoder and denoiser models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9192f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hifigan, vocoder_train_setup, denoiser = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_hifigan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb0200",
   "metadata": {},
   "source": [
    "Verify that generator and vocoder models agree on input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_SPECIFIC_ARGS = [\n",
    "    'sampling_rate', 'hop_length', 'win_length', 'p_arpabet', 'text_cleaners',\n",
    "    'symbol_set', 'max_wav_value', 'prepend_space_to_text',\n",
    "    'append_space_to_text']\n",
    "\n",
    "for k in CHECKPOINT_SPECIFIC_ARGS:\n",
    "\n",
    "    v1 = generator_train_setup.get(k, None)\n",
    "    v2 = vocoder_train_setup.get(k, None)\n",
    "\n",
    "    assert v1 is None or v2 is None or v1 == v2, \\\n",
    "        f'{k} mismatch in spectrogram generator and vocoder'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710f29a2",
   "metadata": {},
   "source": [
    "Put all models on available device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastpitch.to(device)\n",
    "hifigan.to(device)\n",
    "denoiser.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca0834",
   "metadata": {},
   "source": [
    "Load text processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d4293",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_textprocessing_utils', cmudict_path=\"cmudict-0.7b\", heteronyms_path=\"heteronyms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd30e6c",
   "metadata": {},
   "source": [
    "Set the text to be synthetized, prepare input and set additional generation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c8835",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Say this smoothly, to prove you are not a robot.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641732e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = tp.prepare_input_sequence([text], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30875d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kw = {'pace': 1.0,\n",
    "          'speaker': 0,\n",
    "          'pitch_tgt': None,\n",
    "          'pitch_transform': None}\n",
    "denoising_strength = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626b8f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batches:\n",
    "    with torch.no_grad():\n",
    "        mel, mel_lens, *_ = fastpitch(batch['text'].to(device), **gen_kw)\n",
    "        audios = hifigan(mel).float()\n",
    "        audios = denoiser(audios.squeeze(1), denoising_strength)\n",
    "        audios = audios.squeeze(1) * vocoder_train_setup['max_wav_value']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed62ac8",
   "metadata": {},
   "source": [
    "Plot the intermediate spectorgram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9216744",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,12))\n",
    "res_mel = mel[0].detach().cpu().numpy()\n",
    "plt.imshow(res_mel, origin='lower')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('frequency')\n",
    "_=plt.title('Spectrogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08063107",
   "metadata": {},
   "source": [
    "Syntesize audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99120e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_numpy = audios[0].cpu().numpy()\n",
    "Audio(audio_numpy, rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feccadfa",
   "metadata": {},
   "source": [
    "Write audio to wav file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7311fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import write\n",
    "write(\"audio.wav\", vocoder_train_setup['sampling_rate'], audio_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e204b6b",
   "metadata": {},
   "source": [
    "### Details\n",
    "For detailed information on model input and output, training recipies, inference and performance visit: [github](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/HiFiGAN) and/or [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/dle/resources/fastpitch_pyt)\n",
    "\n",
    "### References\n",
    "\n",
    " - [FastPitch paper](https://arxiv.org/abs/2006.06873)\n",
    " - [FastPitch on NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/dle/resources/fastpitch_pyt)\n",
    " - [HiFi-GAN on NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/dle/resources/hifigan_pyt)\n",
    " - [FastPitch and HiFi-GAN on github](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/HiFiGAN)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
